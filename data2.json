[{"repo": "gongdelu/E-commerce-Customer-service-robot", "language": "Jupyter Notebook", "readme_contents": "# E-commerce-Customer-service-robot\n\u7535\u5546\u591a\u8f6e\u5bf9\u8bdd\u5ba2\u670d\u673a\u5668\u4eba\n\n\u6570\u636e\u96c6\uff1a18\u5e74\u516c\u5f00\u7684\u6dd8\u5b9d\u5ba2\u670d\u5bf9\u8bdd\u6570\u636e https://github.com/cooelf/DeepUtteranceAggregation\n\n\u5b9e\u73b0\u7c7b\u522b\uff1a\u751f\u6210\u5f0f\u6a21\u578b\n\n\u6a21\u578b\u67b6\u6784\uff1aSeq2Seq\u6a21\u578b\uff0c\u7f16\u7801\u5668\u4f7f\u7528\u591a\u5c42\u53cc\u5411GRU,\u89e3\u7801\u5668\u4f7f\u7528\u591a\u5c42\u5355\u5411GRU\u53ca\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bad\u7ec3\u6a21\u5f0f\u4f7f\u7528\u4e86teacher forcing\u548c\u68af\u5ea6\u4fee\u526a\u6280\u5de7\uff0c\u6d4b\u8bd5\u6a21\u5f0f\u5219\u4f7f\u7528\u96c6\u675f\u641c\u7d22\u6280\u5de7\u6765\u751f\u6210\u56de\u7b54\n\n\u4f9d\u8d56\uff1a\n  \n  torch 1.0.1\n  \n  jieba 0.38\n  \n  numpy 1.15.4\n"}, {"repo": "Azure/fta-customerfacingapps", "language": "C#", "readme_contents": "# FastTrack for Azure\n\nSee our [FastTrack for Azure landing page](https://azure.microsoft.com/programs/azure-fasttrack/partners) for more information.\n\n# Cloud Native Apps Scenarios\n\n* [E-Commerce](ecommerce/)\n* [Containerize an E-Commerce Site](containerizeecommercesite/)\n* [Event Driven Applications](event-driven-apps/)\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all others rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n"}, {"repo": "ELMAHDI-AR/CustomerAuthentication-With-ASP.NET-MVC", "language": "PowerShell", "readme_contents": ""}, {"repo": "kpei/Customer-Analytics", "language": "Jupyter Notebook", "readme_contents": "# Customer Analytics iPython Notebook\nSee http://kpei.me/blog/?p=921\n"}, {"repo": "Sylius/CustomerOrderCancellationPlugin", "language": "PHP", "readme_contents": "> :warning: **BEWARE!**\n> This repository has been deprecated and will not be maintained or evolved by the Sylius Team. You can still use it with compatible Sylius versions, but at your own risk, as no bugs will be fixed on it.\n\n<p align=\"center\">\n    <a href=\"https://sylius.com\" target=\"_blank\">\n        <img src=\"https://demo.sylius.com/assets/shop/img/logo.png\" />\n    </a>\n</p>\n\n<h1 align=\"center\">Customer Order Cancellation Plugin</h1>\n\n<p align=\"center\"><a href=\"https://sylius.com/plugins/\" target=\"_blank\"><img src=\"https://sylius.com/assets/badge-official-sylius-plugin.png\" width=\"200\"></a></p>\n\n<p align=\"center\">This plugin allows customers to cancel the placed order before it is processed.</p>\n\n![Screenshot showing the customer's orders page with cancel buttons](docs/screenshot.png)\n\n## Business Value\n\nSo far, once a Customer changed their mind about already placed Order, it was up to the Administrator to cancel the order.\nHowever, we have asked ourselves a question - why can't Customer cancel the order when it is yet to be paid? Here comes\nCustomer Order Cancellation Plugin that allows canceling the unpaid order straight from the order history view.\n\n## Installation\n\n#### Beware!\n\n> This installation instruction assumes that you're using Symfony Flex. If you don't, take a look at the\n[legacy installation instruction](docs/legacy_installation.md). However, we strongly encourage you to use\nSymfony Flex, it's much quicker! :)\n\nTo install plugin, just require it with composer:\n\n```bash\ncomposer require sylius/customer-order-cancellation-plugin\n```\n\n> Remember to allow community recipes with `composer config extra.symfony.allow-contrib true` or during plugin installation process\n\n## Extension points\n\nCustomer Order Cancellation plugin uses `Order` entity derived from SyliusCoreBundle as well as its already defined states.\n\nDefault plugin implementation assumes that an Order can be canceled by a Customer when its payment state is \n`awaiting_payment` and shipment state equals `ready`. This conditions can be easily changed by creating a custom\nimplementation of `CustomerOrderCancellationCheckerInterface` or decorating the existing one.\n\n## Security issues\n\nIf you think that you have found a security issue, please do not use the issue tracker and do not post it publicly. \nInstead, all security issues must be sent to `security@sylius.com`.\n"}, {"repo": "optiflow/rfm-customer-segmentation", "language": "Jupyter Notebook", "readme_contents": "# Recency, Frequency, and Monetary (RFM) Customer Segmentation Using K-Means\n- Article explaining details of approaches is published on [Medium](https://towardsdatascience.com/the-most-important-data-science-tool-for-market-and-customer-segmentation-c9709ca0b64a).\n\n## Dataset\n- The dataset is obtained from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/online+retail)\n\n## License\nThe contents of this repository are covered under the [MIT License](https://opensource.org/licenses/MIT).\n"}, {"repo": "watson-developer-cloud/social-customer-care", "language": "CSS", "readme_contents": "# Social Customer Care [![Build Status](https://travis-ci.org/watson-developer-cloud/social-customer-care.svg?branch=master)](https://travis-ci.org/watson-developer-cloud/social-customer-care)\n\n## DEPRECATED: this repo is no longer actively maintained. It can still be used as reference, but may contain outdated or unpatched code.\n\n## Getting Started\n\n### Training Watson\n\n1. The Natural Language Classifier requires training prior to using the application. The training data is provided in `data/classifier-training-data.csv`. Run the following command:\n\n```sh\nnpm run train\n```\n\n1. Sign up at [dev-twitter](http://apps.twitter.com) for application credentials. Create a new application with the `Create new app` button and fill out the required form.\n\n1. Update `env.properties` with the credentials for your Twitter application.\n\n```s\nTWITTER_CONSUMER_KEY=REPLACE WITH YOUR CONSUMER KEY\nTWITTER_CONSUMER_SECRET=REPLACE WITH YOUR CONSUMER SECRET\nTWITTER_ACCESS_TOKEN_KEY=REPLACE WITH YOUR ACCESS TOKEN KEY\nTWITTER_ACCESS_TOKEN_SECRET=REPLACE WITH YOUR ACCESS TOKEN SECRET\n```\n\n### Run locally as Node.js application\n\n```sh\nnpm install\nnpm run build\nnpm start\n```\n\n### Run locally in Docker container\n\n```sh\nbx plugin install dev -r Bluemix\nbx dev run\n```\n\n### Deploy to Bluemix as CloudFoundry application\n\n```sh\nbx cf push\n```\n\n### Directory structure\n\n```none\n.\n\u251c\u2500\u2500 app.js                                   // express routes\n\u251c\u2500\u2500 GulpFile.js\n\u251c\u2500\u2500 credentials.json                         // Watson service credentials\n\u251c\u2500\u2500 env.properties                           // Application properties\n\u251c\u2500\u2500 server.js\n\u251c\u2500\u2500 socket.js                                // Socket.io\n\u251c\u2500\u2500 training/                                // Training files\n\u251c\u2500\u2500 ui                                       // front end source filessrc files for the UI\n\u251c\u2500\u2500 util\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 process-personality-profile.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 process-tweet.js\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 twitter-helper.js\n\u2514\u2500\u2500 views                                    // Views\n```\n\n## About this pattern\n\nFor a given input, a trained Natural Language Classifier responds with a list of intent classes and a confidence score. A frequent use case is to use the user intent and confidence scores to determine how best to assist a customer. This pattern can be used to send customers to an appropriate agent, to respond to commonly asked questions, and to begin the appropriate dialog conversation as is done with the [Conversational Agent](https://github.com/watson-developer-cloud/conversational-agent-application-starter-kit) Starter Kit. This Starter Kit demonstrates the basic approach of how the Natural Language Classifier can be easily used to improve customer support.\n\nTo demonstrate this approach, the Starter Kit uses a live stream of the tweets being sent to [Twitter's @Support](https://twitter.com/Support) account. These tweets are sent to a trained Natural Language Classifier. The resulting intents and confidence scores are used to delegate the tweets or provide a potential response.\n\nThe results of Tone Analyzer, Natural Language Understanding, and Personality Insights services are used to provide quick insight into the customer. Tone Analyzer is used to determine the current mood of the user based on their tweet. Natural Language Understanding and Personality Insights are ran on each the customer's past tweets. The entity and keyword extraction of Natural Language Understanding provides a quick snapshot of the topics that the customer typically tweets about and Personality Insights determines provides an estimate of the customer's personality. This information can be used to provide an agent with quick insight into their customer or even to help send a customer to the most appropriate agent (ex. avoid sending irate customers to a new agent).\n\n**Note**: For the purposes of this starter kit, random bodies of text are substituted for the customer's past tweets. This is done to avoid the issues involved with retrieving past at scale. For your application, past tweets can be easily retrieved by using the [IBM Insights for Twitter](https://console.ng.bluemix.net/docs/services/Twitter/index.html#twitter) Service available on Bluemix.\n\n## Adapting/Extending the Starter Kit\n\nThis Starter Kit works off of Twitter data. However, the concepts used here are platform independent and can be applied wherever you already do customer support. This includes email, sms, Facebook, and chat/messaging apps.\n\nThe following are a basic set of instructions for how to adapt the Starter Kit to your own use case.\n\n1. The Twitter feed can easily be changed by modifying the `TWITTER_TOPIC` variable in your `.env.js` file.\n1. The Natural Language Classifier Service needs a new ground truth for your new feed. The best approach is to collect real user tweets. An easy solution is to use the [Insights for Twitter](https://console.ng.bluemix.net/docs/#services/Twitter/index.html) service available on Bluemix. Using this service, you can retrieve historical tweets from the Twitter Decahose (a 10% random sample of Tweets). Another alternative is to modify the starter kit to save incoming tweets and run the application locally. The number of tweets required will depend upon the complexity of the feed. In most cases, around 300 tweets is enough to receive respectable performance for a proof of concept, demo, or testing.\n1. Create the ground truth for the Natural Language Classifier by classifying the tweets that you collected in the previous step. Further details on this process can be found in the link below.\n1. Decide how the application will handle each use case. The approach taken by the Starter Kit is to provide a faq style response or to delegate the customer to an appropriate agent. This can be customized by altering the `data/default-responses.json` file. Be sure to include an entry for each intent in your classifier ground truth.\n1. Use the personality insights profile to drive your customer engagement strategy. The Starter Kit simply displays a few highlights from the customer's personality but much more is possible. Some examples of how to apply the service can be found in the Personality Insight [documentation](https://www.ibm.com/watson/developercloud/doc/personality-insights/basics.shtml#overviewApply).\n\n## Reference information\n\nThe following links provide more information about the Natural Language Classifier, Tone Analyzer, Natural Language Understanding, and Personality Insights services.\n\n### Natural Language Classifier\n\n* [API documentation](http://www.ibm.com/watson/developercloud/doc/natural-language-classifier/index.html): Get an in-depth knowledge of the Natural Language Classifier service\n* [API reference](http://www.ibm.com/watson/developercloud/natural-language-classifier/api/v1/): SDK code examples and reference\n* [API Explorer](https://watson-api-explorer.mybluemix.net/apis/natural-language-classifier-v1): Try out the API\n* [Creating your own classifier](http://www.ibm.com/watson/developercloud/doc/natural-language-classifier/getting-started.html): How to use the API to create and use your own classifier\n\n### Natural Language Understanding\n\n* [API documentation](http://www.alchemyapi.com/api): Get an in-depth understanding of the AlchemyAPI services\n* [AlchemyData News reference](http://docs.alchemyapi.com/): API and query gallery\n\n### Personality Insights\n\n* [API documentation](http://www.ibm.com/watson/developercloud/doc/personality-insights/): Get an in-depth understanding of the Personality Insights services\n* [API reference](http://www.ibm.com/watson/developercloud/personality-insights/api/v3/): SDK code examples and reference\n* [API explorerer](https://watson-api-explorer.mybluemix.net/apis/personality-insights-v2): Try out the REST API\n\n## License\n\nApache-2.0.\n\n[natural-language-understanding]: http://www.ibm.com/watson/developercloud/natural-language-understanding.html\n[natural-language-classifier]: http://www.ibm.com/watson/developercloud/nl-classifier.html\n[personality-insights]: http://www.ibm.com/watson/developercloud/personality-insights.html\n[dev-twitter]: http://apps.twitter.com\n"}, {"repo": "Sylius/CustomerReorderPlugin", "language": "PHP", "readme_contents": "> :warning: **BEWARE!**\n> This repository has been deprecated and will not be maintained or evolved by the Sylius Team. You can still use it with compatible Sylius versions, but at your own risk, as no bugs will be fixed on it.\n\n<p align=\"center\">\n    <a href=\"https://sylius.com\" target=\"_blank\">\n        <img src=\"https://demo.sylius.com/assets/shop/img/logo.png\" />\n    </a>\n</p>\n\n<h1 align=\"center\">Customer Reorder Plugin</h1>\n\n<p align=\"center\"><a href=\"https://sylius.com/plugins/\" target=\"_blank\"><img src=\"https://sylius.com/assets/badge-official-sylius-plugin.png\" width=\"200\"></a></p>\n\n<p align=\"center\">This plugin allows customers to reorder a previously placed order.</p>\n\n![Screenshot showing the customer's orders page with reorder buttons](docs/screenshot.png)\n\n## Business value\n\nThe plugin allows Customer to reorder any Order that has already been placed. Once a Reorder button is clicked, a new cart \nfilled with items taken from a previously placed order is created. If for some reason Reorder can't be fulfilled completely,\nthe Customer is informed about every circumstance that have affected the Order (i. e. promotion being no longer available\nor differences in item's prices).\n\nOnce the Reorder process is completed, the newly created Order is listed in the history just like any other Orders.\n\n## Installation\n\n#### Beware!\n\n> This installation instruction assumes that you're using Symfony Flex. If you don't, take a look at the\n[legacy installation instruction](docs/legacy_installation.md). However, we strongly encourage you to use\nSymfony Flex, it's much quicker! :)\n\nTo install plugin, just require it with composer:\n\n```bash\ncomposer require sylius/customer-reorder-plugin\n```\n\n> Remember to allow community recipes with `composer config extra.symfony.allow-contrib true` or during plugin installation process\n\n## Extension points\n\nCustomer Reorder plugin is based on two processes:\n\n* reorder processing\n* reorder eligibility checking\n\nThey are both based on Symfony's compiler passes and configured in `services.xml` file.\n\nReorderProcessing and EligibilityChecking are independent processes - once a Reorder\nis created using Processors (services tagged as `sylius_customer_reorder_plugin.reorder_processor`), the created\nentity is passed to Eligibility Checkers (services tagged as `sylius_customer_reorder_plugin.eligibility_checker`).\n\nHence, both processes can be extended separately by adding services that implement `ReorderEligibilityChecker`\nand are tagged as `sylius_customer_reorder_plugin.eligibility_checker` or implement `ReorderProcessor` and are tagged as\n`sylius_customer_reorder_plugin.reorder_processor`.\n\nBoth `Reorder` button layout and action performed on clicking it are defined in\n`reorder.html.twig` template which is declared in `config.yml` file.\n\nWhat's more, since Order is a Resource, major part of its configuration is placed\nin `*.yml` files. Without using the plugin, Order had `Show` and `Pay` actions.\nAdding `Reorder` action required extending order-related behaviours in `config.yml` file.\n\nYou can read much more about Resources here:\n<http://docs.sylius.com/en/1.2/components_and_bundles/bundles/SyliusResourceBundle/index.html> \n\n## Security issues\n\nIf you think that you have found a security issue, please do not use the issue tracker and do not post it publicly. \nInstead, all security issues must be sent to `security@sylius.com`.\n"}, {"repo": "xamarin/app-customers", "language": "C#", "readme_contents": "# Sample Deprecated & Replaced\n\nThis sample has officially been deprecated and replaced with the \"[My Contacts](https://github.com/xamarin/app-contacts)\" sample application that showcases Xamarin.Forms and an ASP.NET Core backend.\n\nThis repo is left here for historical purposes, back links, and is read-only.\n\n# app-customers\n<img src=\"https://raw.githubusercontent.com/xamarin/app-customers/master/Screenshots/Customers_Screens.jpg\" />\n\nA simple Xamarin.Forms demo app with three primary screens:\n\n* a list screen\n* a read-only detail screen\n* an editable detail screen\n\nIncludes integrations such as:\n\n* getting directions\n* making calls\n* sending text messages\n* email composition\n\n##Build Status\n[![Build Status](https://www.bitrise.io/app/7210fb7015b6b6b6.svg?token=y5K_xmtDXKdEzUprMqbWTg&branch=master)](https://www.bitrise.io/app/7210fb7015b6b6b6)\n\n## Google Maps API key (Android)\nFor Android, you'll need to obtain a Google Maps API key:\nhttps://developer.xamarin.com/guides/android/platform_features/maps_and_location/maps/obtaining_a_google_maps_api_key/\n\nInsert it in `~/Droid/Properties/AndroidManifest.xml`:\n\n    <application android:label=\"Customers\" android:theme=\"@style/CustomersTheme\">\\\n      ...\n      <meta-data android:name=\"com.google.android.geo.API_KEY\" android:value=\"[YOUR API KEY HERE]\" />\n      ...\n    </application>\n\n## Screens\n<img src=\"https://raw.githubusercontent.com/xamarin/app-customers/master/Screenshots/Customers_ListPage.png\" width=\"600\" />\n<img src=\"https://raw.githubusercontent.com/xamarin/app-customers/master/Screenshots/Customers_DetailPage.png\" width=\"600\" />\n<img src=\"https://raw.githubusercontent.com/xamarin/app-customers/master/Screenshots/Customers_EditPage.png\" width=\"600\" />\n<img src=\"https://raw.githubusercontent.com/xamarin/app-customers/master/Screenshots/Customers_GetDirections.png\" width=\"600\" />\n\n"}, {"repo": "Prakhar-FF13/Customer-Analytics", "language": "Jupyter Notebook", "readme_contents": "## Business Problem :\n\n#### Problem statement: The goal is to come up with a solution for the given questions:\n\n1. Can we categorize the customers in a particular segment based on their buying patterns? (Customer Segmentation)\n\n2. Can we predict which kind of items they will buy in future based on their segmentation? (Prediction)\n\n** Input: ** We will be using e-commerce data that contains the list of purchases in 1 year for 4,000 customers.\n\n** Output: ** The first goal is that we need to categorize our consumer base into appropriate customer segments. The second goal is we need to predict the purchases for the current year and the next year based on the customers' first purchase.\n\n<hr>\n\n## ML Problem Mapping.\n\n1. Given a dataset of transanctions (Online Retail dataset from UCI Machine Learning repository) get the segments i.e clusters/segments. (Find common patterns and group them)\n2. Predict what to display to what group of users\n\n<hr>\n\n## Additional Info.\n\n#### What is Customer Segmentation ?\n\nCustomer segmentation is a process where we divide the consumer base of the company into subgroups. We need to generate the subgroups by using some specific characteristics so that the company sells more products with less marketing expenditure. Before moving forward, we need to understand the basics, for example, what do I mean by customer base? What do I mean by segment? How do we generate the consumer subgroup? What are the characteristics that we consider while we are segmenting the consumers? Let's answers these questions one by one.\n\nBasically, the consumer base of any company consists of two types of consumers:\n\n1. Existing consumers\n\n2. Potential consumers\n\nGenerally, we need to categorize our consumer base into subgroups. These subgroups are called segments. We need to create the groups in such a way that each subgroup of customers has some shared characteristics. Example ->\n\nSuppose a company is selling baby products. Then, it needs to come up with a consumer segment (consumer subgroup) that includes the consumers who want to buy the baby products. We can build the first segment (subgroup) with the help of a simple criterion. We will include consumers who have one baby in their family and bought a baby product in the last month. Now, the company launches a baby product that is too costly or premium. In that case, we can further divide the first subgroup into monthly income and socio-economic status. Based on these new criteria, we can generate the second subgroup of consumers. The company will target the consumers of the second subgroup for the costly and premium products, and for general products, the company will target consumers who are part of the first subgroup.\n\nWhen we have different segments, we can design a customized marketing strategy as well as customized products that suit the customer of the particular segment. This segment-wise marketing will help the company sell more products with lower marketing expenses. Thus, the company will make more profit. This is the main reason why companies use customer segmentation analysis nowadays. Customer segmentation is used among other domain such as the retail domain, finance domain, and in customer relationship management (CRM)-based products. \n\n#### How companies are making marketing strategies based on the customer segmentation analysis?\n\nCompanies are using the STP approach to make the marketing strategy firm.\n\n#### What is STP ?\n\nSTP stands for Segmentation-Targeting-Positioning. In this approach, there are three stages. The points that we handle in each stage are explained as follows:\n\n1. Segmentation: In this stage, we create segments of our customer base using their profile characteristics as well as consider features provided in the preceding figure. Once the segmentation is firm, we move on to the next stage.\n\n2. Targeting: In this stage, marketing teams evaluate segments and try to understand which kind of product is suited to which particular segment(s). The team performs this exercise for each segment, and finally, the team designs customized products that will attract the customers of one or many segments. They will also select which product should be offered to which segment.\n\n3. Positioning: This is the last stage of the STP process. In this stage, companies study the market opportunity and what their product is offering to the customer. The marketing team should come up with a unique selling proposition. Here, the team also tries to understand how a particular segment perceives the products, brand, or service. This is a way for companies to determine how to best position their offering. The marketing and product teams of companies create a value proposition that clearly explains how their offering is better than any other competitors. Lastly, the companies start their campaign representing this value proposition in such a way that the consumer base will be happy about what they are getting.\n\n<hr>"}, {"repo": "gmasse/ovh-ipxe-customer-script", "language": null, "readme_contents": "# iPXE customer script (BETA)\n\nBooting your server with your own iPXE script allows cool things like:\n - running diskless bare-metal system like CoreOS, SmartOS, ...\n - recovering with your own Rescue system or bare-metal restore tools (like Acronis and Idera)\n - launching Standard installer like ESXi, [Linux](examples/linux.md), Solaris, ...\n\nHow?\nUsing directly the RESTful API [EU](https://api.ovh.com/)|[CA](https://ca.api.ovh.com) or the API console [EU](https://api.ovh.com/console/)|[CA](https://ca.api.ovh.com/console/).\n\nIt's also working for Kimsufi [EU](https://eu.api.kimsufi.com)|[CA](https://ca.api.kimsufi.com) and SoYouStart [EU](https://eu.api.soyoustart.com)|[CA](https://ca.api.soyoustart.com).\n\n\n#### 1. Upload your custom iPXE script\nCoreOS example (https://coreos.com/docs/running-coreos/bare-metal/booting-with-ipxe/)\n\n```http\nPOST /1.0/me/ipxeScript HTTP/1.1\n\n{\n  \"description\": \"CoreOS stable\",\n  \"name\": \"coreos\",\n  \"script\": \"#!ipxe\\n\\nset base-url http://stable.release.core-os.net/amd64-usr/current\\nkernel ${base-url}/coreos_production_pxe.vmlinuz sshkey=\\\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAYQC2PxAKTLdczK9+RNsGGPsz0eC2pBlydBEcrbI7LSfiN7Bo5hQQVjki+Xpnp8EEYKpzu6eakL8MJj3E28wT/vNklT1KyMZrXnVhtsmOtBKKG/++odpaavdW2/AU0l7RZiE= coreos pxe demo\\\"\\ninitrd ${base-url}/coreos_production_pxe_image.cpio.gz\\nboot\"\n}\n```\n```http\nHTTP/1.1 200 OK\n```\n\n#### 2. Request your custom bootId\n\n```http\nGET /1.0/dedicated/server/ns320309.ip-46-105-117.eu/boot?bootType=ipxeCustomerScript HTTP/1.1\n```\n```http\nHTTP/1.1 200 OK\n\n[\n  38\n]\n```\n\n#### 3. Configure your server to boot on your iPXE script\n\n```http\nPUT /1.0/dedicated/server/ns320309.ip-46-105-117.eu HTTP/1.1\n\n{\n  \"bootId\": 38\n}\n```\n```http\nHTTP/1.1 200 OK\n```\n\n#### 4. Reboot your server!\n"}, {"repo": "navdeep-G/customer-churn", "language": "Jupyter Notebook", "readme_contents": "# Customer Churn Analysis\n\n## Overview\n  - This analysis is a set IPython Notebook's containing Churn Analysis for a Telco dataset \n\n"}, {"repo": "php-cuong/magento2-customer-avatar", "language": "PHP", "readme_contents": "# Magento 2 Customer Avatar\nThis is an awesome module, it allows the customers the opportunity to personalize their account by uploading an avatar.\n\nPlease donate if you enjoy my extension.\n\n[![](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=CR756BABGNDC4)\n\n## See the video How I can make this extension here\nhttps://www.youtube.com/watch?v=akcaOBrLTzM&list=PL98CDCbI3TNvPczWSOnpaMoyxVISLVzYQ\n\n## The features of this extension:\n### Frontend:\n- The customer can upload a new avatar.\n- The avatar can be displayed in the header of the website.\n- The avatar can be displayed in the reviews list.\n\n### Backend:\n- Display the avatar of the customer in the customer's grid of Magento Admin.\n- Upload a new avatar or delete an avatar of the customer.\n\n## Introduction installation:\n\n### 1 - Using Composer\n\n```\ncomposer require php-cuong/magento2-customer-avatar:dev-master\n\n```\n\n### 2- Enable the Customer Avatar extension\n * php bin/magento setup:upgrade\n * php bin/magento setup:static-content:deploy\n * php bin/magento indexer:reindex\n * php bin/magento cache:flush\n\n### 3 - See results\n#### Frontend\nLog into your customer account, go to Edit Account Information\n\n##### The avatar in the header\n\n![ScreenShot](https://raw.githubusercontent.com/php-cuong/magento2-customer-avatar/master/Snapshot/header-avatar.png)\n\n##### The avatar in the edit account information\n\n![ScreenShot](https://raw.githubusercontent.com/php-cuong/magento2-customer-avatar/master/Snapshot/upload-new-avatar.png)\n\n##### The avatar in the reviews list\n\n![ScreenShot](https://raw.githubusercontent.com/php-cuong/magento2-customer-avatar/master/Snapshot/customer-review.png)\n\n#### Backend\nLog into your Magento admin, go to Customers -> All Customers\n\n##### The avatar in the customer's grid of Magento Admin\n\n![ScreenShot](https://raw.githubusercontent.com/php-cuong/magento2-customer-avatar/master/Snapshot/avatar-in-customer-grid.png)\n\n##### Upload a new avatar or delete an avatar of the customer\n\n![ScreenShot](https://raw.githubusercontent.com/php-cuong/magento2-customer-avatar/master/Snapshot/upload-delete-an-avatar.png)\n\n## Donations\nPlease donate if you enjoy my extension.\n\n[![](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=CR756BABGNDC4)\n\n\n"}, {"repo": "printu/customerio", "language": "PHP", "readme_contents": "# Customer.io API Client\n\nPHP bindings for the Customer.io API.\n\n[API Documentation](https://customer.io/docs/api/)\n\n[![Build Status](https://travis-ci.org/printu/customerio.svg?branch=master)](https://travis-ci.org/printu/customerio)\n[![Code Climate](https://codeclimate.com/github/printu/customerio/badges/gpa.svg)](https://codeclimate.com/github/printu/customerio)\n[![Test Coverage](https://codeclimate.com/github/printu/customerio/badges/coverage.svg)](https://codeclimate.com/github/printu/customerio/coverage)\n\nThere are two primary API hosts available for to integrate with:\n\n**Behavioral Tracking**\n\nhttps://track.customer.io/api/v1/ \\\nBehavioral Tracking API is used to identify and track customer data with Customer.io.\n\n**API**\n\nhttps://api.customer.io/v1/api/ \\\nAPI allows you to read data from your Customer.io account for use in custom workflows in your backend system or for reporting purposes.\n\n**API (BETA)**\n\nhttps://beta-api.customer.io/v1/api/ \\\nBeta API features should be used with caution, and should be expected to change with little notice. Please refer to [this](https://customer.io/docs/api/#apitrackintroduction) documentation for available endpoints.\n## Installation\n\nThe API client can be installed via [Composer](https://github.com/composer/composer).\n\nIn your composer.json file:\n\n```json\n{\n    \"require\": {\n        \"printu/customerio\": \"~3.0\"\n    }\n}\n```\n\nOnce the composer.json file is created you can run `composer install` for the initial package install and `composer update` to update to the latest version of the API client.\n\nThe client uses [Guzzle](http://docs.guzzlephp.org/en/stable/).\n\n## Basic Usage\n\nRemember to include the Composer autoloader in your application:\n\n```php\n<?php\nrequire_once 'vendor/autoload.php';\n\n// Application code...\n?>\n```\n\nConfigure your access credentials when creating a client:\n\n```php\n<?php\nuse Customerio\\Client;\n\n$client = new Client('YOUR_API_KEY', 'YOUR_SITE_ID');\n\n/*\n * To authenticate, provide your key as a Bearer token in a HTTP Authorization header.\n * You can create and manage your API keys by visiting your App API Keys page directly or by clicking the Integrations\n *  link in the left-hand menu of your Customer.io account and choosing Customer.io API > Manage API Credentials > App API Keys.\n */\n$client->setAppAPIKey('APP_KEY');\n\n?>\n```\n\nChange region to EU\n\n```php\n<?php\nuse Customerio\\Client;\n\n$client = new Client('YOUR_API_KEY', 'YOUR_SITE_ID', ['region' => 'eu']);\n\n?>\n```\n\n### Local Testing\n\nRun `phpunit` from the project root to start all tests.\n\n### Examples\n\n#### Customers\n\n```php\n<?php\n// Create customer\ntry {\n    $client->customers->add(\n        [\n            'id' => 1,\n            'email' => 'user@example.com',\n            'plan' => 'free',\n            'created_at' => time()\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error\n}\n\n// Get customer\ntry {\n    $client->customers->get(\n        [\n            'email' => 'user@example.com',        \n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error\n}\n\n// Update customer\ntry {\n    $client->customers->update(\n        [\n            'id' => 1,\n            'email' => 'user@example.com',\n            'plan' => 'premium'\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error   \n}\n\n// Delete customer\ntry {\n    $client->customers->delete(\n        [\n            'id' => 1,\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error   \n}\n```\n\n#### Events\n\n```php\n<?php\n// Add customer event\ntry {\n    $client->customers->event(\n        [\n            'id' => 1,\n            'name' => 'test-event',\n            'data' => [\n                'event-metadata-1' => 'test',\n                'event-metadata-2' => 'test-2'\n            ]\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error\n}\n\n// Add anonymous event\ntry {\n    $client->events->anonymous(\n        [\n            'name' => 'invite-friend',\n            'data' => [\n                'recipient' => 'invitee@example.com'\n            ]\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error\n}\n```\n\nAnonymous event [example](http://customer.io/docs/invitation-emails.html) usage.\n\n#### Segments\n```php\n<?php\n// Get segment data\ntry {\n    $client->segments->get(\n        [\n            'id' => 1\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error\n}\n```\n\nCheck for other available methods [here](https://customer.io/docs/api/#apibeta-apisegmentssegments_list)\n\n#### PageView\n\n```php\n<?php\n// Add page view\ntry {\n    $result = $client->page->view(\n        [\n            'id' => 1,\n            'url' => 'http://example.com/login',\n            'data' => [\n                'referrer' => 'http://example.com'\n            ]\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error\n}\n```\n\n#### Campaigns\n\n```php\n<?php\n// Get campaigns data\ntry {\n    $client->campaigns->get(\n        [\n            'id' => 1\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error\n}\n```\n\nCheck for other available methods [here](https://customer.io/docs/api/#apibeta-apicampaignscampaigns_get)\n\n```php\n<?php\n// Trigger broadcast campaign\ntry {\n    $result = $client->campaigns->trigger(\n        [\n            'id' => 1,\n            'data' => [\n                'headline' => 'Roadrunner spotted in Albuquerque!',\n                'date' => 'January 24, 2018', \n                'text' => 'We\\'ve received reports of a roadrunner in your immediate area! Head to your dashboard to view more information!' \n            ],\n            'recipients' => [\n                'segments' => [\n                    'id' => 1\n                ]\n            ]\n        ]\n    );\n} catch (\\GuzzleHttp\\Exception\\GuzzleException $e) {\n    // Handle the error\n}\n```\n\nSee [here](https://learn.customer.io/documentation/api-triggered-data-format.html) for more examples of API Triggered Broadcasts\n\n## License\n\nMIT license. See the [LICENSE](LICENSE) file for more details."}, {"repo": "f5devcentral/f5-digital-customer-engagement-center", "language": "HCL", "readme_contents": "# F5 Digital Customer Engagement Center Repository\nF5 Digital Engagement Center Code Repository\n\n[Project Documentation in ReadTheDocs](https://f5-digital-customer-engagement-center.readthedocs.io/en/latest/index.html)\n## Overview\nThis project will be utilized to demo and provide reusable configurations for F5 Digital Engagement Center labs and roadshows\n\n## Getting Started\nThis repository and its examples are meant to support a bring your own credentials approach.\nthe credentials can be obtained through an F5 UDF course deployment, or your own cloud credentials.\n\n## Installation\n\nThis project can be run with or without the provided container for development or [**devcontainer.**](https://code.visualstudio.com/docs/remote/containers)\n\n### Devcontainer TLDR\n  - Requirements:\n      - [vscode](https://code.visualstudio.com/)\n      - [ms-vscode-remote.vscode-remote-extensionpack](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack)\n      - [docker](https://www.docker.com/)\n      - [wsl2](https://docs.microsoft.com/en-us/windows/wsl/) on windows workstations\n  - Running:\n    - open vscode command pallet\n      - >Remote-Containers: Rebuild and Reopen in Container\n## Usage\n\nThe project is meant to deliver a framework for creation solutions using common or customized modules.\n\n\n### Solutions\n  Each solution will:\n  - Explain which infrastructure it requires\n      - readme.md\n  - Provide step by step instructions to deploy the required infrastructure and configure it mirroring the scripts\n    - formatting for readthedocs\n  - Contain scripts to:\n    - Deploy the required infrastructure\n      - setup.sh\n    - Create the required configuration\n      - demo.sh\n          - demo script is meant to be a wrapper for the scripts you want to use to configure your deployed solution infrastructure.\n          the most common scenario of using ansible through a docker container, is permitted by the privileged devcontainer.\n    - Destroy the required infrastructure\n      - cleanup.sh\n  - Accept variables\n    - auto.tfvars.example\n      an example of available variables for the infrastructure\n  - Export variables\n    - Solutions should be addressable as terraform modules, this requires they output relevant connection information in a standardized way.\n  - Be responsible for any IAM objects needed\n  - Create and manage any security groups/firewall rules needed by the solution.\n  - Include defaults not present in the auto.tvfars when using terraform\n  - Provide an auto.tfvars example when using terraform\n\na sample solution template is available in **~/solutions/solution_template**\n\n---\n### Modules\nEach module will:\n- Attempt to use community or vendor supported modules first\n- Export consistent outputs following project naming [standards](contributing.md)\n- Include standalone examples\n- Work with existing networks\n---\n### Devcontainer\nThe container will:\n- provide current releases of:\n  - Terraform\n  - Ansible\n  - pre-commit\n  - aws-cli\n  - azure-cli\n  - gcloud-cli\n\n---\n## Development\n\nchecking for secrets as well as linting is performed by git pre-commit with the module requirements handled in the devcontainer.\n\ntesting pre-commit hooks:\n  ```bash\n  # test pre commit manually\n  pre-commit run -a -v\n  ```\nbuilding documentation:\n  ```bash\n  mkdir -p docs_local\n  #local\n  sphinx-build -b html $PWD docs_local/\n  #preview local\n  # non wsl normal file path volume\n  docker run -it --rm -d -p 80:80 --name docs -v $PWD/docs_local:/usr/share/nginx/html nginx\n  # wsl requires volume path to original dockerhost filesystem\n  docker run -it --rm -d -p 80:80 --name docs -v /c/github/f5-digital-customer-engagement-center/docs_local:/usr/share/nginx/html nginx\n  ```\n---\n## Troubleshooting\nmodule not pulling in changes:\n  - force module update\n    ```bash\n    terraform get -update\n    ```\n---\n## Support\nFor support, please open a GitHub issue.  Note, the code in this repository is community supported and is not supported by F5 Networks.  For a complete list of supported projects please reference [SUPPORT.md](SUPPORT.md).\n\n---\n## Community Code of Conduct\nPlease refer to the [F5 DevCentral Community Code of Conduct](code_of_conduct.md).\n\n---\n## License\n[Apache License 2.0](LICENSE)\n\n---\n## Copyright\nCopyright 2014-2020 F5 Networks Inc.\n\n---\n### F5 Networks Contributor License Agreement\n\nBefore you start contributing to any project sponsored by F5 Networks, Inc. (F5) on GitHub, you will need to sign a Contributor License Agreement (CLA).\n\nIf you are signing as an individual, we recommend that you talk to your employer (if applicable) before signing the CLA since some employment agreements may have restrictions on your contributions to other projects.\nOtherwise by submitting a CLA you represent that you are legally entitled to grant the licenses recited therein.\n\nIf your employer has rights to intellectual property that you create, such as your contributions, you represent that you have received permission to make contributions on behalf of that employer, that your employer has waived such rights for your contributions, or that your employer has executed a separate CLA with F5.\n\nIf you are signing on behalf of a company, you represent that you are legally entitled to grant the license recited therein.\nYou represent further that each employee of the entity that submits contributions is authorized to submit such contributions on behalf of the entity pursuant to the CLA.\n"}, {"repo": "pabitralenka/Customer-Feedback-Analysis", "language": "Python", "readme_contents": "# Customer Feedback Analysis, IJCNLP'17\n* **Our goal is to determine what class(es) the customer feedback sentences should be annotated with five-plus-one-classes       categorization (comment, request, bug, complaint, meaningless and undetermined) as in four languages i.e. English,           French, Japanese and Spanish**.\n* This is one of the shared tasks of [**IJCNLP - 2017**](http://ijcnlp2017.org/site/page.aspx?pid=901&sid=1133&lang=en). For more details about the task, please visit [here.](https://sites.google.com/view/customer-feedback-analysis/)\n***\n\n### Citing the paper\n\nIf you are using this code for any sort of research, please cite our paper \n* [**IITP at IJCNLP-2017 Task 4: Auto Analysis of Customer Feedback using CNN and GRU Network.**](http://aclweb.org/anthology/I17-4031)\n***\n\n### Dataset \n#### Training Data samples for CNN (training.tsv) from different languages used\ntag  | consumer_complaint_narrative\n------------- | -------------\ncomment  | Rooms and sitting area was always immaculate.\nrequest  | :) Deber\u00edan abrir vacantes para beta-testers :)\nmeaningless | il beug tou le temp\ncomplaint  | \u30b7\u30e3\u30f3\u30d7\u30fc\u304c\u6ce1\u7acb\u305f\u306a\u3044\n\n#### Test Data samples for CNN (test.tsv) from different languages used\nid  | consumer_complaint_narrative\n------------- | -------------\nen-test-0002  | You can't go wrong!!!\nes-test-0004  | La habitaci\u00f3n s\u00faper grande! muy c\u00f3moda..\nfr-test-0006  | La salle de bains est splendide.\njp-test-0016  | \u65e5\u3005\u306e\u5fd9\u3057\u3055\u3092\u5fd8\u308c\u3066\u3001\u5a18\u304c\u512a\u3057\u304f\u3055\u308c\u308b\u3068\u512a\u3057\u304f\u306a\u308c\u308b\u306d\n\n#### Training Data samples for CNN + RNN (training.tsv) from different languages used\nCategory  | Descript\n------------- | -------------\ncomment  | Rooms and sitting area was always immaculate.\nrequest  | :) Deber\u00edan abrir vacantes para beta-testers :)\nmeaningless  | il beug tou le temp\ncomplaint  | \u30b7\u30e3\u30f3\u30d7\u30fc\u304c\u6ce1\u7acb\u305f\u306a\u3044\n\n#### Test Data samples for CNN + RNN (test.tsv) from different languages used\nid  | Descript\n------------- | -------------\nen-test-0002  | You can't go wrong!!!\nes-test-0004  | La habitaci\u00f3n s\u00faper grande! muy c\u00f3moda..\nfr-test-0006  | La salle de bains est splendide.\njp-test-0016  | \u65e5\u3005\u306e\u5fd9\u3057\u3055\u3092\u5fd8\u308c\u3066\u3001\u5a18\u304c\u512a\u3057\u304f\u3055\u308c\u308b\u3068\u512a\u3057\u304f\u306a\u308c\u308b\u306d\n***\n\n### Running the Code\n#### For CNN\n##### Train \n* Command : **`python3 train.py training.tsv parameters.json`**\n* A directory will be created during training, and the best model will be saved in this directory.\n\n##### Test\n* Provide the model directory (created when running `train.py`) and test data to `predict.py`\n* Command : **`python3 predict.py trained_model_1505467324/ test.tsv`**\n***\n\n#### For CNN + RNN\n##### Train \n* Command : **`python3 train.py training.tsv training_config.json`**\n* A directory will be created during training, and the best model will be saved in this directory.\n\n##### Test\n* Provide the model directory (created when running `train.py`) and test data to `predict.py`\n* Command : **`python3 predict.py trained_results_1505468375/ test.tsv`**\n***\n\n### Reporting Doubts and Errors\n* For any queries, please drop me an email at pabitra.lenka18@gmail.com.\n* Please refer to the publication for detailed results and model performances.\n\n### Credits\n* I would like to thank [Jie Zhang](https://github.com/jiegzhan) and [Denny Britz](https://github.com/dennybritz) for sharing their code.\n* We have used their code and modified according to our need by incorporating pre-trained `Word2Vec` embedding.\n* [Deepak Gupta](https://github.com/deepak1357) has also contributed to this code repository. \n"}, {"repo": "2427595858/customer-maven", "language": "Java", "readme_contents": "# customer-maven\n\n## \u9879\u76ee\u4ecb\u7ecd\n\u4f7f\u7528ssm+maven\u7f16\u5199\u7684\u5ba2\u6237\u4fe1\u606f\u7ba1\u7406\u7cfb\u7edf\n- \u4f7f\u7528\u6846\u67b6\uff1aspring4+mybatis+springMVC\n- \u6570\u636e\u5e93\uff1amysql5.7\n- \u9879\u76ee\u7ba1\u7406\u5de5\u5177\uff1amaven\n- \u7f16\u8bd1\u5668\uff1aintellij idea\n\n\u6ce8\uff1a\u6b64\u9879\u76ee\u662f\u57fa\u4e8e\u4e4b\u524d\u7684[Servlet+JSP+SQL Server+Layui+intellij idea\u7f16\u5199\u7684\u5ba2\u6237\u4fe1\u606f\u7ba1\u7406\u7cfb\u7edf](https://github.com/2427595858/Customer-Information-Management)\u9879\u76ee\u8fdb\u884c\u6539\u9020\u3002\n\n## \u5b9e\u73b0\u529f\u80fd\n- \u5ba2\u6237\u4fe1\u606f\u7684\u589e\u5220\u67e5\u6539\uff0c\u4ee5\u53ca\u6a21\u7cca\u67e5\u8be2\u529f\u80fd\n- \u5206\u9875\u663e\u793a\u5ba2\u6237\u4fe1\u606f\n\n## \u6548\u679c\u9884\u89c8\n\n1. \u5ba2\u6237\u5217\u8868\u9875\n\n![\u5ba2\u6237\u5217\u8868\u9875](https://img-blog.csdn.net/20180521164218822)\n\n2. \u6dfb\u52a0\u5ba2\u6237\n\n![\u6dfb\u52a0\u5ba2\u6237](https://img-blog.csdn.net/20180521164239826)\n\n3. \u9ad8\u7ea7\u641c\u7d22\n\n![\u9ad8\u7ea7\u641c\u7d22](https://img-blog.csdn.net/20180521164303811)\n\n4. \u5f00\u53d1\u65e5\u5fd7\n\n![\u5f00\u53d1\u65e5\u5fd7](https://img-blog.csdn.net/20180521164324852)\n\n\u76f8\u5173\u6559\u7a0b\u8bf7\u5230[\u6211\u7684\u535a\u5ba2](https://blog.csdn.net/silversoldier/article/details/80387067#t17)\u67e5\u770b\u3002\n\n\u5982\u679c\u89c9\u5f97\u4e0d\u9519\u7684\u8bdd\uff0c\u8bf7\u7ed9\u4e2astar\u5427\u3002\n\n\n\n\n"}, {"repo": "kiwicommerce/magento2-login-as-customer", "language": "PHP", "readme_contents": "# We're not maintaining this extension, if you need any support please contact us at hello@kiwicommerce.co.uk\n\n# Magento 2 - Login as Customer by [KiwiCommerce](https://kiwicommerce.co.uk/)\n- Admin can login as customers to trace any process in which the customer is facing the issue.\n- Admin don\u2019t need the password of the customer and the Authentication will not be change.\n- Admin can login as customer from the admin panel in just one click and is able to process on the storefront as a customer and is  redirected to My Account\u2019s page.\n- This Extension also has security measures to hide login options for different admin users.\n- This Extension also has options from where admin can enable and disable the extension.\n\n## Installation\n1. Composer Installation\n      - Navigate to your Magento root folder<br />\n            `cd path_to_the_magento_root_directory`\n      - Then run the following command<br />\n            `composer require kiwicommerce/module-login-as-customer`\n      - Make sure that composer finished the installation without errors.\n\n 2. Command Line Installation\n      - Backup your web directory and database\n      - Download the latest Login as Customer installation package kiwicommerce-login-as-customer-vvvv.zip from [here](https://github.com/kiwicommerce/magento2-login-as-customer/releases)\n      - Navigate to your Magento root folder<br />\n          `cd path_to_the_magento_root_directory`<br />\n      - Upload contents of the Login as Customer installation package to your Magento root directory\n      - Then run the following command<br />\n          `php bin/magento module:enable KiwiCommerce_LoginAsCustomer`<br />\n   \n- After install the extension, run the following command\n```\nphp bin/magento setup:upgrade\nphp bin/magento setup:di:compile\nphp bin/magento setup:static-content:deploy\nphp bin/magento cache:flush\n```\n- Log out from the backend and login again.\n          \nFind More details on [KiwiCommerce](https://kiwicommerce.co.uk/extensions/magento2-login-as-customer)\n\n## Where will it appear in Admin Panel\n### Customers Grid page\nAdmin can see the login as customer button on customer Grid page and Edit page.\n\n<img src=\"https://kiwicommerce.co.uk/wp-content/uploads/2018/06/login-as-customer-customer-grid.jpg\"/><br/>\n\n### Orders Grid Page\nAdmin can see the login as customer button on Order Grid page and View page.\n\n<img src=\"https://kiwicommerce.co.uk/wp-content/uploads/2018/06/login-as-customer-order-grid.jpg\"/><br/>\n\n### Login as Customer Log \nAdmin can also track records of how many times an admin user logged in as a customer along with the login time and IP address. Not only this it also offers filter facility for each and every login.\n\n<img src=\"https://kiwicommerce.co.uk/wp-content/uploads/2018/06/login-as-customer-login-log.jpg\"/> <br/>\n\n## Configuration\nUser can control or set where \u201cLogin as customer\u201d link will be displayed using the Setting section that is given below.\n\n<img src=\"https://kiwicommerce.co.uk/wp-content/uploads/2018/06/login-as-customer-system-configuration.jpg\" /> <br/>\n\n## Need Additional Features?\nFeel free to get in touch with us at https://kiwicommerce.co.uk/get-in-touch/\n\n## Other KiwiCommerce Extensions\n* [Magento 2 Cron Scheduler](https://kiwicommerce.co.uk/extensions/magento2-cron-scheduler/)\n* [Magento 2 Inventory Log](https://kiwicommerce.co.uk/extensions/magento2-inventory-log/)\n* [Magento 2 Enhanced SMTP](https://kiwicommerce.co.uk/extensions/magento2-enhanced-smtp/)\n* [Magento 2 Admin Activity](https://kiwicommerce.co.uk/extensions/magento2-admin-activity/)\n* [Magento 2 Customer Password](https://github.com/kiwicommerce/magento2-customer-password/)\n\n## Contribution\nWell unfortunately there is no formal way to contribute, we would encourage you to feel free and contribute by:\n \n  - Creating bug reports, issues or feature requests on <a target=\"_blank\" href=\"https://github.com/kiwicommerce/magento2-login-as-customer/issues\">Github</a>\n  - Submitting pull requests for improvements.\n    \nWe love answering questions or doubts simply ask us in issue section. We're looking forward to hearing from you!\n \n  - Follow us <a href=\"https://twitter.com/KiwiCommerce\">@KiwiCommerce</a>\n  - <a href=\"mailto:support@kiwicommerce.co.uk\">Email Us</a>\n  - Have a look at our <a href=\"https://kiwicommerce.co.uk/docs/login-as-customer/\">documentation</a>\n"}, {"repo": "klein0r/magento-customer-helper", "language": "PHP", "readme_contents": "Magento Help Customers Extension\n======================\n\nSends an email after a given time range to customers which failed to login.\n\n1. A customer tries to logon several times with a wrong password\n2. Failed tries will be logged into the database\n3. If the logon is successful, the fail entry will be deleted from the database\n4. A cron job checks for customers which failed to login every 10 minutes\n5. All customers are informed by mail that they failed to logon\n6. The customer uses a link in the mail to reset their password\n7. Everyone is happy :)\n\nThis extension should stop frustration of customers if they fail to logon because of forgotton passwords.\n\nBy Matthias Kleine (http://mkleine.de)\n"}, {"repo": "mapr-demos/customer360", "language": "Jupyter Notebook", "readme_contents": "BY USING THIS SOFTWARE, YOU EXPRESSLY ACCEPT AND AGREE TO THE TERMS OF THE AGREEMENT CONTAINED IN THIS GITHUB REPOSITORY.  See the file EULA.md for details.\n\n#  Customer 360 Powered by MapR&trade;\n\nThe <a href=\"https://mapr.com/products/mapr-converged-data-platform/\">MapR Converged Data Platform</a> is uniquely suited to run \nCustomer 360 applications because it provides the foundations for\n\n1. Cloud Scale Storage\n2. Schemaless Data Integration\n3. Machine Learning\n\nCustomer 360 applications require the ability to access data lakes containing structured and unstructured data, integrate data sets, and run operational and analytical workloads simultaneously. MapR enables applications to glean customer intelligence through machine learning that relates to customer personality, sentiment, propensity to buy, and likelihood to churn. Check out the <a href=\"https://mapr.com/solutions/quickstart/customer-360-knowing-your-customer-is-step-one/\">Customer 360 Quick Start Solution</a> to learn more about MapR's products and solutions for Customer 360 applications.\n\n\n<img src=\"https://github.com/mapr-demos/customer360/blob/master/images/dataflow.gif\" width=\"70%\">\n\n## Overview\n\nThis demo application focuses on showing how the following three tenants to customer 360 applications can be achieved on MapR:\n\n1. Big Data storage of structured and semi-structured data in files, tables, and streams\n2. SQL-based data integration of disperate datasets\n3. Predictive analytics through machine learning insights\n\nThere are a lot of different ways to demo MapR. Some people like to use data visualizations to convey the value of MapR's technology. Other people like go deep into APIs for more developer oriented conversations. In this demo, we try to accomodate both approaches. For the graphical approach, this demo application runs in a stand-alone web server that shows interactive data visualizations (shown below). For the deep technical dive, we provide Jupyter Notebooks to show code and MapR's APIs.\n\n<img src=\"https://github.com/mapr-demos/customer360/blob/master/images/screenshot.png\" width=\"70%\">\n\n\n# Demo Script\n\n## How does \"convergence\" make Customer Intelligence better?\n\nCompany's have all kinds of information about their customers. That information is stored in many different formats and in many different ways. It may be schemaless or schemaful. It may be in database tables, it may be in files, or it may be in streams like web clicks or social media activity. In all these cases, MapR provides the platform you need to ingest and store the data. MapR enables you to conveniently ingest data by mounting the cluster with NFS. Furthermore, storing massive and unbounded data streams are no problem for MapR's **cloud scale data storage**.\n\nBut we don't want to just store data, we want to be able to access all this information in order to analyze customer behavior and product usage. So **Data Integration** is also a critical part for Customer 360. MapR does data integration with Apache Drill - the industry's best SQL engine for Hadoop. \n\nBut its not enough to simply consolidate datasets. We want to do more than just SQL. We want to harness the power of **Machine Learning** to not only better understand customer characteristics like sentiment, propensity to buy, and likelihood to churn but also to improve fraud detection and targeted marketing.\n\nCloud Scale Storage, Data Integration, and Machine Learning are essential to achieving the most value out of a Customer 360 application.  Any one of those would be a challenge by itself, but with the MapR Converged Data Platform you get them all in a package that is faster/better/cheaper than anything else.\n\n## Setting the Scene\n\nYou're a customer support representative. You're about to answer a call from Erika Gallardo. Before you talk to her try to answer these questions: \n\n- Is she already navigating the web site? \n- What should you keep in mind to upsell? \n- What do you think she'll be asking about?\n\n## What's behind the scenes?\n\n- **Customer Directory** is populated by a SQL query which accesses data in MapR-DB, MySQL, and JSON files. Thanks to Apache Drill, these SQL queries can be executed without specifying a schema.\n- **Machine Learning** is used to predict spend rate and lifetime value by linear regressions. It's also used to create the Heatmap that identifies upsell/cross-sell opportunities for customer groups segemented by K-Means.\n- **Real-Time Streaming** is used to show clickstreams for each user. This data helps customer service reps know how customers are using our bank's web site during support calls. Clickstream data is also useful for tracking the long-term customer journey.\n- **Survey Feedback** is derived from surveys in which only a fraction of customers responded, but which we can extrapolate to identify patterns with other customers in the same group.\n- **Customer Image** is an example of binary data saved as jpg files which can be used for fraud detection by camera surveillance in banks.\n\n\n# Get Community Support!\n\nVisit the [MapR Community](https://community.mapr.com/) pages where you can post questions and discuss your use case.\n\n\n\n\n\n"}, {"repo": "SonarSoftwareInc/customer_portal", "language": "JavaScript", "readme_contents": "[![Customer Portal](https://i.imgur.com/AMoOuyg.png)](https://github.com/SonarSoftwareInc/customer_portal)\n\n# Sonar Customer Portal\n\nThis is a prebuilt and self-hosted customer portal for [Sonar](https://sonar.software).\n\n## Quick start\n\nThese instructions will get you set up and running with SSL through [LetsEncrypt](https://letsencrypt.org) as well as automatic updates provided by [Watchtower](https://github.com/v2tec/watchtower).\n\n**_If you are a current Sonar customer, and you need assistance with any part of this process, please don't hesitate to reach out to support@sonar.software for help. We are more than happy to help you get your portal setup!_**\n\nYou'll need a machine running Ubuntu 16 or 18 x64. **Please note that the customer portal will not work Ubuntu 19, as Docker is currently unsupported.** The installation script currently assumes a Debian-based distro. We recommend a minimum of 2 vCPUs, at least 2GB of RAM, and at least 25 GB of storage.\n\nIt will need a public facing IP address and a valid domain name pointing to it (e.g. portal.myisp.com).\n\n## Getting started\n\nSSH into your VM.\n\nInstall required packages:\n`sudo apt-get -y update && sudo apt-get -y upgrade && sudo apt-get -y install git unzip`\n\nClone the repository:\n`git clone https://github.com/SonarSoftwareInc/customer_portal.git`\n\nNow change directory into the repository that we've just cloned:\n`cd customer_portal`\n\nRun the install script:\n`sudo ./install.sh | tee customerportal-install.log`\n\nFollow the instructions as prompted by the installation script.\nYou can view the installation log by running\n`cat customerportal-install.log`\n\n### Define a Role\n\n#### SonarV1 Role Instructions ####\n1. Go to System > Roles\n1. Create a new role called Customer Portal\n1. Assign the following permissions:\n   * Accounts: Read, Create, Update, Delete\n   * Financial: Read\n   * Ticketing: Read, Create, Update, Delete\n   * Ticket Super User: Enabled\n\n#### SonarV2 Role Instructions ####\n\nThe Sonar version 2 instructions have been moved to the [Sonar Knowledge Base](https://docs.sonar.expert/baseline-config/customer-portal-configuration-checklist#api_user_permissions).\n\n### Define a Username\n\nThe API username and API password you are prompted for are credentials for your Sonar instance. You should create a dedicated user to utilize for the customer portal - **do not use your admin username/password!** Create the user as instructed above and assign to this \"Customer Portal\" role.\n\n**Please be aware that at this time, you cannot use a `^` or `\\` character in the API password. This is a known issue that may be addressed in the future.**\n\n## Post-Installation Setup\n\nAfter the setup process is complete, your instance should be up and running. You can navigate to the settings URL (which is `/settings` on the domain you setup, e.g. `https://portal.myisp.com/settings`) and use the settings key that should have been generated for you with the installation script.\n\n## Common tasks\n\nStarting the customer portal:\n`sudo docker-compose start`\n\nStopping the customer portal:\n`sudo docker-compose stop`\n\nViewing the logs:\n`sudo docker-compose logs`\n\n## Commands you can use after setup\n\nFrom the `customer_portal` directory, you can execute `sudo docker-compose exec app /bin/bash` to access the docker container that the portal is running in. After doing this, you can execute the commands below.\n\n* `php artisan sonar:settingskey` will generate a new key for the `/settings` page if you forget the one you had.\n* `php artisan sonar:test:smtp {email}` will test your email configuration. Replace `{email}` with your email address, and the portal will attempt to send you a test email.\n* `php artisan sonar:test:paypal` will test your PayPal configuration, if it is enabled.\n\n## Upgrading\n\nUpgrades for the customer portal are done automatically and require no interaction on your part! The customer portal will automatically check for updates every 5 minutes and update itself. For further customization such as setting an update window or configuring email and/or Slack notifications, please see https://github.com/v2tec/watchtower\n\n## Troubleshooting\n\nIf you get the following error during setup:\n\n```\n[/var/www/html/storage]:rw': invalid mount config for type \"volume\": invalid mount path: '[/var/www/html/storage]' mount path must be absolute\n```\n\nTry removing the created storage volume by executing `sudo docker volume rm customer_portal_storage` and rerunning the installation script.\n\n## Customizing the portal\n\nThis portal is built using [Laravel](https://laravel.com/). You are welcome to fork and modify this repository for your own needs! Do not attempt to customize the files inside the existing Docker container, as they will be automatically overwritten during upgrade. If you need help customizing this portal beyond what is currently available, we recommend [Solutions4Ebiz](https://www.solutions4ebiz.com/) as an experienced third party developer.\n"}, {"repo": "watson-developer-cloud/customer-engagement-bot", "language": "JavaScript", "readme_contents": "<h1 align=\"center\" style=\"border-bottom: none;\">\ud83d\ude80 Customer Engagement Bot</h1>\n<h3 align=\"center\">DEPRECATED: this repo is no longer actively maintained. It can still be used as reference, but may contain outdated or unpatched code.</h3>\n<p align=\"center\">\n  <a href=\"http://travis-ci.org/watson-developer-cloud/customer-engagement-bot\">\n    <img alt=\"Travis\" src=\"https://travis-ci.org/watson-developer-cloud/customer-engagement-bot.svg?branch=master\">\n  </a>\n  <a href=\"#badge\">\n    <img alt=\"semantic-release\" src=\"https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--release-e10079.svg\">\n  </a>\n</p>\n</p>\n\n\n![Demo GIF](readme_images/ce-demo.gif?raw=true)\n\nFor more information on the Watson Assistant (Conversation) service, see the [detailed documentation](https://console.bluemix.net/docs/services/conversation/index.html#about).\nFor more information on the Tone Analyzer Service, see the [detailed documentation](https://console.bluemix.net/docs/services/tone-analyzer/index.html#about).\n\n## Deploying the application\n\nIf you want to experiment with the application or use it as a basis for building your own application, you need to deploy it in your own environment. You can then explore the files, make changes, and see how those changes affect the running application. After making modifications, you can deploy your modified version of the application to IBM Cloud.\n\n## Prerequisites\n\n1. Sign up for an [IBM Cloud account](https://console.bluemix.net/registration/).\n1. Download the [IBM Cloud CLI](https://console.bluemix.net/docs/cli/index.html#overview).\n1. Create an instance of the Watson Assistant service and get your credentials:\n    - Go to the [Watson Assistant](https://console.bluemix.net/catalog/services/conversation) page in the IBM Cloud Catalog.\n    - Log in to your IBM Cloud account.\n    - Click **Create**.\n    - Click **Show** to view the service credentials.\n    - Copy the `apikey` value\n    - Copy the `url` value.\n1. Create an instance of the Tone Analyzer service and get your credentials:\n    - Go to the [Tone Analyzer](https://console.bluemix.net/catalog/services/tone-analyzer) page in the IBM Cloud Catalog.\n    - Log in to your IBM Cloud account.\n    - Click **Create**.\n    - Click **Show** to view the service credentials.\n    - Copy the `apikey` value\n    - Copy the `url` value.\n\n## Configuring the application\n\n1. In your IBM Cloud console, open the Watson Assistant service instance\n\n1.  Click on the  **Launch tool** button to launch into the Watson Assistant tooling.  \n\n    ![Watson Assistant Launch Tool](readme_images/WA_LaunchTool.png)\n\n\n1.  This is the Watson Assistant tooling where you can create assistants, skills and and setup different chatbots applications. We'll be importing a pre-built skill. **Click on 'Skills'** on the top left, and then on the **Create new** button. \n\n    ![Watson Assistant New Skill](readme_images/WA_CreateNewSkill.png)\n\n1.  **Click on 'Import Skill'** and then on the **Choose JSON File** button. \n\n    ![Watson Assistant Import Skill](readme_images/WA_ImportSkill.png)\n\n1.  Find the workspace JSON file `training/ce-workspace.json` from this repository on your local machine and **Click the 'Import'** button (make sure the **Everything** radio button is selected to import intents, entities and dialog).  \n\n    ![Watson Assistant Import Skill Complete](readme_images/WA_ImportSkillFinish.png)\n\n1.  You will be redirected into a page with four tabs, Intents, Entities, Dialog, and Content Catalog. For the purposes of this lab, the skill is fairly complete.\n\n\n1.  To interact with the correct skill, you will  need the unique identifier for your skill.  You can find the workspace ID from the Watson Assistant tooling. From the main Skills page, **Click on the three stacked dots** on the top right of the skill you created/imported. Then **click on the 'View API Details'** option in the menu.\n\n    ![Workspace ID](readme_images/WA_WorkspaceID.png)  \n\n    Copy the **Workspace ID** value from this page.\n\n1. In the application folder, copy the *.env.example* file and create a file called *.env*\n\n    ```\n    cp .env.example .env\n    ```\n\n1. Open the *.env* file and add the service credentials that you obtained for both Watson Assistant and Tone Analyzer in the previous step.\n\n1. Add the `WORKSPACE_ID` to the previous properties\n\n1. Your `.env` file  should looks like:\n\n    ```\n    # Environment variables\n    WORKSPACE_ID=1c464fa0-2b2f-4464-b2fb-af0ffebc3aab\n    ASSISTANT_IAM_APIKEY=_5iLGHasd86t9NddddrbJPOFDdxrixnOJYvAATKi1\n    ASSISTANT_URL=https://gateway-syd.watsonplatform.net/assistant/api\n\n    TONE_ANALYZER_IAM_APIKEY=UdHqOFLzoOCFD2M50AbsasdYhOnLV6sd_C3ua5zah\n    TONE_ANALYZER_URL=https://gateway-syd.watsonplatform.net/tone-analyzer/api\n    ```\n\n## Running locally\n\n1. Install the dependencies\n\n    ```\n    npm install\n    ```\n\n1. Run the application\n\n    ```\n    npm start\n    ```\n\n1. View the application in a browser at `localhost:3000`\n\n## Deploying to IBM Cloud as a Cloud Foundry Application\n\n1. Login to IBM Cloud with the [IBM Cloud CLI](https://console.bluemix.net/docs/cli/index.html#overview)\n\n    ```\n    ibmcloud login\n    ```\n\n1. Target a Cloud Foundry organization and space.\n\n    ```\n    ibmcloud target --cf\n    ```\n\n1. Edit the *manifest.yml* file. Change the **name** field to something unique.  \n  For example, `- name: my-app-name`.\n1. Deploy the application\n\n    ```\n    ibmcloud app push\n    ```\n\n1. View the application online at the app URL.  \nFor example: https://my-app-name.mybluemix.net\n\n# What to do next\n\nAfter you have the application installed and running, experiment with it to see how it responds to your input.\n\n## Modifying the application\n\nAfter you have the application deployed and running, you can explore the source files and make changes. Try the following:\n\n   * Modify the `.js` files to change the application logic.\n\n   * Modify the `.html` file to change the appearance of the application page.\n\n   * Use the Watson Assistant tool to train the service for new intents, or to modify the dialog flow. For more information, see the [Watson Assistant service documentation](https://www.ibm.com/watson/services/conversation/).\n\n# What does the Customer Engagement Bot application do?\n\nThe application interface is designed for chatting with a customer engagement bot. Based on a previous laptop purchase, the bot asks how the experience has been and responds accordingly if given a negative or positive response.\n\nThe chat interface is in the left panel of the UI, and the JSON response object returned by the Watson Assistant service in the right panel. Your input is run against a small set of sample data trained with the following intents:\n\n    yes\n    no\n    refund: get a refund on current laptop\n    restart: restart the conversation at any point\n    tradeIn: replace current laptop with another one\n    thanks\n    greeting\n\nThe dialog is also trained on two types of entities:\n\n    design\n    size\n    weight\n\nThese intents and entities help the bot understand variations your input.\n\nBelow you can find some sample interactions:\n\n![Alt text](readme_images/examples.jpg?raw=true)\n\nIn order to integrate the Tone Analyzer with the Watson Assistant service (formerly Conversation), the following approach was taken:\n   * Intercept the user's message. Before sending it to the Watson Assistant service, invoke the Tone Analyzer Service. See the call to `toneDetection.invokeToneAsync` in the `invokeToneConversation` function in [app.js](./app.js).\n   * Parse the JSON response object from the Tone Analyzer Service, and add appropriate variables to the context object of the JSON payload to be sent to the Watson Assistant service. See the `updateUserTone` function in [tone_detection.js](./addons/tone_detection.js).\n   * Send the user input, along with the updated context object in the payload to the Watson Assistant service. See the call to `conversation.message` in the `invokeToneConversation` function in [app.js](./app.js).\n\n\nYou can see the JSON response object from the Watson Assistant service in the right hand panel.\n\n![Alt text](readme_images/tone_context.jpg?raw=true)\n\nIn the conversation template, alternative bot responses were encoded based on the user's emotional tone. For example:\n\n![Alt text](readme_images/rule.png?raw=true)\n\n\n## License\n\nThis sample code is licensed under Apache 2.0.  \nFull license text is available in [LICENSE](LICENSE).\n\n## Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md).\n\n## Open Source @ IBM\n\nFind more open source projects on the\n[IBM Github Page](http://ibm.github.io/).\n"}, {"repo": "seifip/starbucks-customer-segmentation", "language": "Jupyter Notebook", "readme_contents": "_Udacity Data Science Nanodegree project by [Philip Seifi](https://www.seifi.co/)._\n\n_You can find the accompanying blog post on [Medium](https://medium.com/@seifip/starbucks-offers-advanced-customer-segmentation-with-python-737f22e245a4)._\n\n# Starbucks offers: Advanced customer segmentation with\u00a0Python\n\nA small startup can afford to target users based on broad-stroke rules and rough demographics.\n\nOnce a company grows to the size of Starbucks, with millions of daily customers, and $1.6B in credit stored on loyalty cards, they have got to graduate to a more sophisticated method to target their marketing.\n\nOne such approach, cluster analysis, uses mathematical models to discover groups of similar customers based on variations in their demographics, purchasing habits, and other characteristics.\n\nBelow, I will explore a customer transaction and marketing offer dataset graciously provided by Starbucks.\n\nI will then use Principal Component Analysis (PCA) and the k-means unsupervised Machine Learning algorithm to group these customers into clusters that can be used to automate an effective outreach campaign.\n\n## One month of transaction data\nThe dataset includes one month of simulated customer data, including their purchasing habits, and interactions with promotional offers.\n\nEach person in the simulation has hidden traits that influence their purchasing patterns, and that are associated with their observable traits.\nThe dataset consists of three separate JSON files:\n\n1. **Customer profiles\u200a** - \u200atheir age, gender, income, and date of becoming a member.\n\n2. **Portfolio\u200a** - \u200aOffers sent during the 30-day test period, via web, email, mobile or social media channels, or a combination thereof. The offers have varying levels of difficulty (minimum spend) and reward, and fall into one of three categories: Discount, Buy-one-get-one (BOGO), Informational\n\n3. **Transcript\u200a** - \u200aA list of offer interactions (receive/view/complete), and all other transactions during the test period.\n\n## Data wrangling\nBefore I could visualize and model the data, I've had to do some preprocessing both outside, and in Python.\n\nAmong others, I have:\n\n1. Removed empty lines in transcript.json using search \\n\\n & replace \\n in VSCode.\n2. Imputed empty income values with the mean, and added a separate column that tracks missing income values with 1s\n3. Engineered a new column tracking the year when the user became a member\n4. One-hot-encoded channels using the MultiLabelBinarizer\n5. One-hot-encoded offer types, genders, years joined and event types using get_dummies\n6. Dropped outliers (people with age > 99)\n7. Engineered first receipt, view and completion time columns (a customer can receive and interact with the same offer multiple times)\n8. Dropped misattributions (completion without view, completion before view, or view before receipt)\n9. Added RFM Recency and Frequency scores.\n10. Engineered view and conversion rate columns for each offer type\n11. Merged all data into one dataframe grouped by customers, including means and sums for all available data, as well as additional columns for the average number of exposures per offer-type.\n\n## Customer clusters identified\n### Segment 1\nCustomers in this segment receive regular BOGO offers, and practically no discount offers. These BOGO offers involve more valuable rewards than for customers in other segments.\n\nTheir frequency and average order value are not unusual, which suggests these customers are conditioned to BOGOs, and we might have to continue sending them regular offers to keep their patronage.\nBOGOs convert really well with customers in this segment, so this is a great lever in times when we need to quickly generate additional sales.\n\n### Segment 2\nCustomers in this segment receive a higher than average number of offers, and convert really well for both BOGOs and discounts. Demographically, a higher than average share of these customers selected their gender as \"Other\".\n\nTheir average order value is not unusual, in line with the average, but their frequency is above average, probably as a result of the regular offers they receive and act on.\n\nThis is another segment we can target to quickly generate additional sales.\n\n### Segment 3\nCustomers in this segment receive no BOGO offers. They do get occasional discount offers, on which they convert about average, as well as slightly more informational messages than other customers.\n\nThese customers have about average frequency and average order value, and would likely continue to frequent Starbucks even if we stopped sending them offers.\n\n### Segment 4\nCustomers in this segment receive regular offers, which they open, but never convert.\n\nDemographically, they are predominantly male, and lower than average income. They also visit Starbucks less frequently, and make smaller average purchases.\n\nGiven the low LTV and low conversion rates for this group, we would be best to avoid targeting them in our marketing.\n\n## Acknowledgements\nDataset by [Starbucks](https://www.starbucks.com/).\n"}, {"repo": "pusher-community/customer-support-chat-example", "language": "JavaScript", "readme_contents": "# Customer support chat example using Chatkit\nHow to create a customer support chat system using Pusher Chatkit\n\n[View tutorial](https://pusher.com/tutorials/chat-widget-javascript)\n\n![](https://www.dropbox.com/s/8unv1hu1t3k34ti/Creating-a-customer-support-chat-widget-using-JavaScript-and-Chatkit.gif?raw=1)\n\n## Installation\n* [Create a Chatkit instance](https://pusher.com/chatkit)\n* Once you have created a Chatkit instance, go to the **Inspector** tab and create a new user called `chatkit-dashboard`\n* Clone or download the repository.\n* `cd` into the repository (`cd customer-support-chat-example`)\n* Copy the `config.example.js` file to `config.js` and replace the placeholder keys with your Chatkit application's keys.\n* Remember to udpate the `PUSHER_INSTANCE_LOCATOR` variables in [`assets/admin.js`](https://github.com/neoighodaro/customer-support-chat-example/blob/master/assets/admin.js) and [`assets/chat.js`](https://github.com/neoighodaro/customer-support-chat-example/blob/master/assets/chat.js)\n* Run `npm install` to install dependencies then `node index.js` to run the server\n* Open http://localhost:3000 and http://localhost:3000/admin on your browser\n\n\n"}, {"repo": "nenes25/prestashop_customerautogroups", "language": "PHP", "readme_contents": "# prestashop_customerautogroups\r\n\r\nPrestashop module which automaticaly add customer in groups depending params after registration.\r\nThe rules can be combined, and can be sorted by priority.\r\n\r\nFor example you can use it for the folowing cases :\r\n - all user from a specifiq country / city / or even street get to a particular group\r\n - all men in a specifiq group ...\r\n\r\nYou can imagine all the possibilites with all customer's and customer's address attributes.  \r\nOnce the module is intalled, a new link \"Customer Auto Groups\" appears in customer menu in the administration.  \r\nThere you can see and manage your rules.\r\n\r\n<img src=\"http://www.h-hennes.fr/blog/wp-content/uploads/2015/12/autogrouplist.jpg\" alt=\"autogroups List\" />\r\n\r\n\r\n# (FR )prestashop_customerautogroups\r\nModule Prestashop qui permets d'ajouter automatiquement les clients dans un groupe sp\u00e9cifique, en fonction de ses param\u00e8tres.  \r\nCe module est compatible \u00e0 partir de Prestashop 1.5 ( depuis la version 0.1.1 )  \r\nVous pouvez consulter l'article sur mon blog:  \r\n<a href=\"http://www.h-hennes.fr/blog/2015/11/10/prestashop-ajouter-automatiquement-les-clients-a-un-groupe-lors-de-linscription/\" target=\"blank\">http://www.h-hennes.fr/blog/2015/11/10/prestashop-ajouter-automatiquement-les-clients-a-un-groupe-lors-de-linscription/</a>\r\n\r\n"}, {"repo": "fateh491989/customersuppert_admin", "language": "Dart", "readme_contents": "# customersuppert_admin\nNOTE- THERE MUST BE ONE ADMIN ELSE CUSTOMER APP WILL GIVE YOU ERROR\n![cacustomer](https://user-images.githubusercontent.com/16588377/73612945-9a4a3200-4616-11ea-9f75-c016cc7f87c4.png)\n![ca](https://user-images.githubusercontent.com/16588377/73612953-afbf5c00-4616-11ea-8939-51225003e8aa.png)\n\n\nA new Flutter application.\n\n## Getting Started\n\nThis project is a starting point for a Flutter application.\n\nA few resources to get you started if this is your first Flutter project:\n\n- [Lab: Write your first Flutter app](https://flutter.dev/docs/get-started/codelab)\n- [Cookbook: Useful Flutter samples](https://flutter.dev/docs/cookbook)\n\nFor help getting started with Flutter, view our\n[online documentation](https://flutter.dev/docs), which offers tutorials,\nsamples, guidance on mobile development, and a full API reference.\n"}, {"repo": "Customerly/Customerly-iOS-SDK", "language": "Swift", "readme_contents": "<p align=\"left\">\n<a href=\"http://www.customerly.io\">\n  <img src=\"https://avatars1.githubusercontent.com/u/23583405?s=200&v=4\" height=\"100\" alt=\"Live Chat ios SDK Help Desk\"></a>\n</p>\n\n<h1>Live Chat iOS SDK from Customerly</h1>\n<h2> The Best-in-Class Live Chat for your mobile apps. Integrate painlessly the Live Chat for your customer support inside any iOS App with <a href=\"http://www.customerly.io/go/live-chat?utm_source=github&utm_medium=readme&utm_campaign=iossdk\">Customerly Live Chat </a> SDK </h2>\n\n  [![Language](https://img.shields.io/badge/Swift-5-orange.svg)]()\n  [![Language](https://img.shields.io/badge/Objective--C-compatible-blue.svg)]()\n  [![License](https://img.shields.io/badge/license-Apache%20License%202.0-red.svg)]()\n  \n**Customerly** is the most complete <strong>Live Chat</strong> solution with Help Desk for your mobile apps. Help them where they are with the customer support widget. Easy to integrate Live Chat, once integrated you can track user data and gather user feedback. \n\nRun Surveys directly into your mobile apps and get the responses in one place. \n\nThe Customerly Live Chat iOS SDK is really simple to integrate in your apps, and allow your users to contact you via chat.\n\n<p align=\"center\">\n  <img src=\"https://github.com/customerly/customerly.github.io/blob/master/ios/resources/chat-preview.png?raw=true\" width=500 alt=\"Live Chat Help Desk ios SDK \"/>\n</p>\n\n## Features\n- [x] Support via live chat in real time\n- [x] Track your users\n- [x] Set attributes\n- [x] Set company attributes\n- [x] Track events\n- [x] Run Surveys\n- [x] English, Spanish & Italian localizations\n- [x] Objective-C compatibility\n- [x] Many more is coming....\n\n## Requirements\n\n- iOS 10.0+\n- Xcode 10.2.1+\n- Swift 5 or Objective-C\n\n## CocoaPods\n\nTo use the Customerly SDK we recommend to use Cocoapods 1.7.0 or later\n\n[CocoaPods](http://cocoapods.org) is a dependency manager for Cocoa projects. You can install it with the following command:\n\n```bash\n$ gem install cocoapods\n```\n\nTo integrate the Customerly SDK into your Xcode project using CocoaPods, specify it in your `Podfile`:\n\n\n```ruby\nsource 'https://github.com/CocoaPods/Specs.git'\nplatform :ios, '10.0'\nuse_frameworks!\n\npod 'CustomerlySDK'\n```\n\nThen, run the following command:\n\n```bash\n$ pod install\n```\n\n## Usage\nIf you are setting up a new project, you need to install the SDK. You may have already completed this as part of creating a Customerly account. We recommend using CocoaPods 1.7.0 or later to install the SDK.\n\nFirst of all, if you don't have an Xcode project yet, create one, then install the SDK following the paragraph `Cocoapods`.\n\n**1)** Import the Customerly iOS SDK module in your UIApplicationDelegate subclass:\n\n```\nimport CustomerlySDK\n```\n**2)** Configure a Customerly iOS SDK shared instance, in your App Delegate, inside **application:didFinishLaunchingWithOptions:** method:\n\n```\nCustomerly.sharedInstance.configure(appId: \"YOUR_CUSTOMERLY_APP_ID\")\n```\nalso add inside **applicationDidBecomeActive:**\n\n```\nCustomerly.sharedInstance.activateApp()\n```\n\nIf you want to enable the logging in console, you can set verboseLogging variable to true. By default verbose logging is disabled.\n\n```\nCustomerly.sharedInstance.verboseLogging = true\n```\n\n**3)** From iOS 10, you'll need to make sure that you add `NSPhotoLibraryUsageDescription` & `NSCameraUsageDescription` to your Info.plist so that your users have the ability to upload photos in Customerly's chat. Furthermore remember to set the `NSAppTransportSecurity` to `NSAllowsArbitraryLoads`.\n\n**If in doubt, you can look at the examples in the demo application.**\n\n\n### User registration\nYou can register logged in users of your app into Customerly calling the method `registerUser:`. You\u2019ll also need to register your user anywhere they log in.\n\nExample:\n\n```\nCustomerly.sharedInstance.registerUser(email: \"axlrose@example.com\", user_id: \"123ABC\", name: \"Axl Rose\")\n```\n\nor using a closure\n\n```\nCustomerly.sharedInstance.registerUser(email: emailTextField.text!, user_id: userIdTextField.text, name: nameTextField.text, success: { \n                //Success\n            }, failure: { \n                //Failure\n            })\n```\n\nYou can also logout users:\n\n```\nCustomerly.sharedInstance.logoutUser()\n```\n\nIn this method, *user_id*, *name*, *attributes*, *company*, *success* and *failure* are optionals.\n\nIf you don't have a login method inside your apps don't worry, users can use the chat using their emails.\n\n### Chat\nYou can open the support view controller calling the method `openSupport:`\n\n```\nCustomerly.sharedInstance.openSupport(from: self)\n```\nwhere **self** is your current view controller.\n\n### Surveys (nothing to do)\n\nWith the Customerly SDK you can deliver surveys directly into your app app without any lines of code.\n\nThey will be automatically displayed to your user as soon as possible.\n\nRemember that you can get updates about new surveys available using the `update:` method.\n\n### Attributes\nInside attributes you can add every custom data you prefer to track.\n\n```swift\n// Eg. This attribute define what kind of pricing plan the user has purchased \nCustomerly.sharedInstance.setAttributes(attributes: [\"pricing_plan_type\" : \"basic\"])\n```\n\n### Company\nYou can also set company data by submitting an attribute map, like:\n\n```\nCustomerly.sharedInstance.setCompany(company: [\"company_id\": \"123\", \"name\": \"My Company\", \"plan\": 3])\n```\n\nWhen you set a company, \"company_id\" and \"name\" are required fields for adding or modifying a company.\n\n### Events\nSend to Customerly every event you want to segment users better\n\n```\n// Eg. This send an event that track a potential purchase\nCustomerly.sharedInstance.trackEvent(event: \"added_to_cart\")\n```\n\n### Extra\n\nIf you want to get a generic update, call `update:`\n\n```\nCustomerly.sharedInstance.update(success: { \n            //Update success\n        }) { \n            //Update failure\n        }\n\n```\n\n## Contributing\n\n- If you **need help** or you'd like to **ask a general question**, open an issue or contact our support on [Customerly.io](https://www.customerly.io)\n- If you **found a bug**, open an issue.\n- If you **have a feature request**, open an issue.\n- If you **want to contribute**, submit a pull request.\n\n\n## License\nCustomerly iOS SDK is available under the Apache License 2.0. See the LICENSE file for more info.\n"}, {"repo": "Chale-project/wechat-minapp-scs-customer", "language": "Vue", "readme_contents": "# scs\n\n> \u667a\u6167\u4fbf\u5229\u5e97\u5c0f\u7a0b\u5e8f\u9879\u76ee\u4ecb\u7ecd\n\n## Build Setup\n\n``` bash\n# install dependencies\nnpm install\n\n# serve with hot reload at localhost:8080\nnpm run dev\n\n# build for production with minification\nnpm run build\n\n# build for production and view the bundle analyzer report\nnpm run build --report\n```\n\nFor detailed explanation on how things work, checkout the [guide](http://vuejs-templates.github.io/webpack/) and [docs for vue-loader](http://vuejs.github.io/vue-loader).\n"}, {"repo": "liu513632815/WebCustomerService", "language": "Java", "readme_contents": "# WebCustomerService\nweb\u7248\u5728\u7ebf\u5ba2\u670d\u804a\u5929\u7cfb\u7edf  \n  1.\u652f\u6301\u81ea\u5b9a\u5236\u5e7f\u544a\u63a8\u9001\u548c\u81ea\u52a8\u7b54\u590d\uff0c\u8fdb\u5165\u76d1\u63a7\u9875\u9762\u5e95\u90e8\u6309\u94ae\u81ea\u5b9a\u5236\u6d88\u606f  \n  2.\u5f00\u7bb1\u5373\u7528\uff0c\u76f4\u63a5\u542f\u52a8\u9879\u76ee\u901a\u8fc7IP:port/index\u8fdb\u5165\u76d1\u63a7\u9875\u9762\uff0cIP:port/customer\u6ce8\u518c\u5e76\u8fdb\u5165\u5ba2\u670d\u9875\u9762\uff0cIP:port/consumer\u6ce8\u518c\u5e76\u8fdb\u5165\u5ba2\u6237\u9875\u9762\uff0c\u65e0\u4fb5\u5165\u968f\u65f6\u5d4c\u5165\u5176\u4ed6  \u4efb\u4f55B/S\u7cfb\u7edf\u3002  \n  3.\u53ef\u540c\u65f6\u591a\u5ba2\u670d\u591a\u5ba2\u6237\u5bf9\u63a5\uff0c\u7cfb\u7edf\u81ea\u52a8\u4e3a\u5ba2\u6237\u8f6e\u8be2\u5206\u914d\u5ba2\u670d\u3002\u5f53\u53ea\u6709\u4e00\u4e2a\u5ba2\u670d\u65f6\uff0c\u6240\u6709\u5ba2\u6237\u90fd\u5c06\u5bf9\u63a5\u5230\u8be5\u5ba2\u670d\n\n# \u76d1\u63a7\u9875\u9762\n![image](https://github.com/liu513632815/WebCustomerService/blob/master/img/index.png)\n\n# \u5ba2\u670d\u9875\u9762\n![image](https://github.com/liu513632815/WebCustomerService/blob/master/img/customer.png)\n\n# \u5ba2\u6237\u9875\u9762\n![image](https://github.com/liu513632815/WebCustomerService/blob/master/img/consumer.png)\n\n"}, {"repo": "chawucirencc/Predicting-whether-a-telecommunications-company-is-losing-customers", "language": "Python", "readme_contents": "# Predicting-whether-a-telecommunications-company-is-losing-customers\n\u5bf9\u7535\u4fe1\u516c\u53f8\u7684\u6570\u636e\u505a\u51fa\u5206\u6790\uff0c\u5efa\u7acb\u9884\u6d4b\u672a\u77e5\u5ba2\u6237\u662f\u5426\u4e3a\u8981\u6d41\u5931\u7684\u5ba2\u6237\u3002\n"}, {"repo": "KolatimiDave/Expresso-Customer-Churn-Prediction", "language": "Jupyter Notebook", "readme_contents": "# Expresso-Customer-Churn-Prediction\nThis repository explains how to predict customer churn. An Hackathon Organized by Data Science Nigeria(DSN-AI) to help Expresso predict customer Churn. <b>My 2nd place solution </b>, log_loss of 0.246675 on [Zindi](https://zindi.africa/hackathons/dsn-pre-bootcamp-hackathon-expresso-churn-prediction-challenge/leaderboard) where the competition was hosted. I've also added a section in the notebook to get a score of  0.246643, which could be the 'unofficial' <b> 1st place solution </b>.\n\n### About [Expresso](http://www.expressotelecom.sn/):\nExpresso is an African telecommunications company that provides customers with airtime and mobile data bundles. The objective of this challenge is to develop a machine learning model to predict the likelihood of each Expresso customer \u201cchurning,\u201d i.e. becoming inactive and not making any transactions for 90 days\n\n#### My Approach \n* Handled Missing Values\n* Preprocessed Catgegorical variables\n* Clustering\n* Feature Creation\n* KFold Validation\n* Model Blending\n#### Improvements that can be made\n* Feature Selection\n* Handling missing data more efficiently\n* Hyper-parameter tuning\n#### Requirements\n- pip install requirements.txt\n### Leaderboard Scores\n- Catboost - 0.2466929\n- Xgboost - 0.2469854\n- Xgboost and Catboost Blended - 0.246643\n\nIf you have any questions, comments or concerns, feel free to reach me on [linkedin](https://www.linkedin.com/in/olukolatimi-david-19a841187/)\n"}, {"repo": "pyxploiter/customer-service-chatbot", "language": "Python", "readme_contents": "# customer-service-ai-chatbot\nIt is an artificially intelligent chatbot which can interact with customers with voice.\n\n## Some of libraries are outdated!!! If you want to contribute to this project, contact me at:\nhttps://www.linkedin.com/in/muhammad-asad-ali-35464613a/\n"}, {"repo": "jeremyjordan/customer-segmentation", "language": "Jupyter Notebook", "readme_contents": "## Overview\n\nThis is a more reader-friendly version of a project I worked on as part of my Udacity Machine Learning Nanodegree. The original project is available [here](https://github.com/jeremyjordan/machine-learning/tree/master/projects/customer_segments).\n\nIn this project, we'll look at purchasing data for clients of a wholesale distributor and attempt to use purchasing behavior to identify a way to predict what group each client belongs to. The characteristics of the grouping is unknown, we simply want to see if we can put similar observations (in this case, clients) into the same group. We'll then compare these predicted groupings to the true channels (Hotel/Restaurant/Cafe and Retail) each client belongs to.\n\nMany companies today collect vast amounts of data on customers and clientele, and have a strong desire to understand the meaningful relationships hidden in their customer base. Being equipped with this information can assist a company engineer future products and services that best satisfy the demands or needs of their customers.\n\n## Data\n\nThe customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal. More information can be found on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers).\n\n**Features**\n1) `Fresh`: annual spending (m.u.) on fresh products (Continuous);\n2) `Milk`: annual spending (m.u.) on milk products (Continuous);\n3) `Grocery`: annual spending (m.u.) on grocery products (Continuous);\n4) `Frozen`: annual spending (m.u.) on frozen products (Continuous);\n5) `Detergents_Paper`: annual spending (m.u.) on detergents and paper products (Continuous);\n6) `Delicatessen`: annual spending (m.u.) on and delicatessen products (Continuous);\n7) `Channel`: {Hotel/Restaurant/Cafe - 1, Retail - 2} (Nominal)\n8) `Region`: {Lisnon - 1, Oporto - 2, or Other - 3} (Nominal)\n\nNote: (m.u.) is shorthand for *monetary units*.\n\nFor this project we'll attempt to **predict** which channel each client belongs to, so we won't look at this column until after we've segmented the data using clustering techniques.\n\n### Instructions\n\nThis project requires Python 2.7. All of the libraries used in this project are included in the [Anaconda](http://continuum.io/downloads) distribution of Python, it is highly suggested that you use Anaconda to manage packages and environments for data science with Python.\n\nWhen you're ready, fire up the notebook.\n\n```bash\njupyter notebook customer_segments.ipynb\n```\n"}, {"repo": "mageplaza/magento-2-login-as-customer", "language": "PHP", "readme_contents": "\r\n# Magento 2 Login As Customer extension FREE\r\n\r\n[Magento 2 Login as Customer by Mageplaza](https://www.mageplaza.com/magento-2-login-as-customer/) supports the store admin to login to customers' account and automatically save that login data. In many cases, customers face some difficulty or inconvenience in My Account page, or right in the checkout step. Login as Customer module will be a great solution for such these circumstances. The store can have a quick view after access as a customer, as a result, this can save a significant amount of time for both administrator and customers.\r\n\r\n\r\n\r\n## 1. Documentation\r\n\r\n- Installation guide: https://www.mageplaza.com/install-magento-2-extension/\r\n- User guide: https://docs.mageplaza.com/login-as-customer/index.html\r\n- Introduction page: http://www.mageplaza.com/magento-2-login-as-customer/\r\n- Contribute on Github: https://github.com/mageplaza/magento-2-login-as-customer\r\n- Get Support: https://github.com/mageplaza/magento-2-login-as-customer/issues\r\n\r\n\r\n## 2. Login as Customer FAQs\r\n\r\n**Q: I got error: Mageplaza_Core has been already defined**\r\n\r\nA: Read solution: https://github.com/mageplaza/module-core/issues/3\r\n\r\n**Q: I am a store administrator, how can I access a customer\u2019s account?**\r\n\r\nA: You can quickly login customer\u2019s account to get information from Order View and Customer View right on the admin backend. \r\n\r\n**Q: I am a store owner, my store site has many admin accounts. How can I manage customers' login history?**\r\n\r\nA: Magento 2 Login as Customer extension allows store owners to view access history. On the backend, please navigate `Report > Customer > Login as Customer Logs`. \r\n\r\n**Q: Is it possible to export logs data to save daily?**\r\n\r\nA:  Yes, it is totally possible. From the report logs, you can hit the Export button and select the exported CSV or XML file. \r\n\r\n\r\n## 3. How to install Magento 2 Login as Customer extension\r\n- Install via composer (recommend)\r\n- Run the following command in Magento 2 root folder:\r\n\r\nWith Marketing Automation (recommend):\r\n```\r\ncomposer require mageplaza/module-login-as-customer mageplaza/module-smtp\r\nphp bin/magento setup:upgrade\r\nphp bin/magento setup:static-content:deploy\r\n```\r\n\r\nWithout Marketing Automation:\r\n```\r\ncomposer require mageplaza/module-login-as-customer\r\nphp bin/magento setup:upgrade\r\nphp bin/magento setup:static-content:deploy\r\n```\r\n\r\n## 4. Login as Customer highlight features\r\n\r\n\r\n### Instantly access to any customer account \r\n\r\nOne of the most noticeable features of **M2 Login as Customer extension** is that the store admin can log in a customer\u2019s account to take some necessary actions.\r\n\r\n![Magento 2  Instantly access to any customer account ](https://i.imgur.com/wjxAovr.png)\r\n\r\n#### Easy access from convenient places \r\n\r\nOn the Magento 2 backend, the admin can quickly access the account of any customer by selecting between **Order View** and **Customer View** options. Right here, there are two important places, in which all information of customers and orders is continuously updated. Hence, **Login as Customer** link is placed here, to support admins in accessing quickly and conveniently, without any difficulty. \r\n\r\n#### Only one click to access all information needed\r\n\r\nOn the pages mentioned, with only one click on the **Login as Customer** button, the admin will be redirected instantly to customers\u2019 account with full actual information such as my dashboard, order information, account information, etc. With **Magento 2 Login as Customer module**, it is not only easy to access but is also possible to edit or make any updates in the blink of an eye.\r\n\r\n\r\n\r\n### Record login history\r\n\r\n![Magento 2 Login as Customer module](https://i.imgur.com/Uvg5qTA.png)\r\n\r\n#### Continually record every login attempt\r\n\r\nAny access to customers' account can be recorded exactly in **Login as Customer Logs section on the backend**. There is a report here, which provides all access information, including date and time, customer account, admin account.\r\n \r\n#### Manage and track customer/order information easily \r\n\r\nIn case a store has multiple admin accounts, the access report mentioned above is potential for the store owner. It effectively supports managing and tracking every single admins' login as customer attempt. Furthermore, deleting or editing any data is not allowed on this report. As a result, it is definite that the admin will always be updated about any changes related to customers, and is able to avoid unexpected ill-intentioned access as well. \r\n\r\n\r\n\r\n### Export login logs function\r\n\r\n**Mageplaza Login as Customer module for Magento 2** also support the export function. On the report log, the administrator can easily export login data so that they can save those data on the daily or monthly basis with ease. By this way, it is easy to store the login records, then use for login information management or any other specific purposes. \r\n\r\nThe most common-used exported file formats are CSV and Excel XML.  \r\n\r\n![Mageplaza Login as Customer module for Magento 2](https://i.imgur.com/nlgTMbm.png)\r\n\r\n\r\n## 5. Full Magento 2 Login as Customer Features\r\n\r\n### For admins\r\n- Enable/disable the extension \r\n- Login as customer from Order View \r\n- Login as customer from Customer View\r\n- Easily view and track recorded login access \r\n- Possible to export login data to CSV or Excel XML \r\n\r\n### For customers\r\n- Quickly get support from the store\u2019s admins in editing order information or customer information  \r\n- Save time and effort to edit information, accordingly, have a better shopping experience\r\n\r\n\r\n\r\n## 6. How to configure Magento 2 Login as Customer\r\n\r\n### 6.1.Configuration\r\n\r\nFrom **Admin panel**, go to `Stores > Configuration > Mageplaza > Login as Customer` \r\n\r\n![How to configure Login as Customer extension](https://i.imgur.com/s7jE6Zr.png)\r\n\r\n- Select **Enable = Yes**:\r\n  - Activate the module. Then, admin can log in customer\u2019s account from the admin backend. \r\n  - `Login as Customer` button will be displayed on Customer View and Order View sections from admin backend.\r\n  \r\n- **Customer Edit**\r\n  \r\n![How to configure Magento 2 Login as Customer](https://i.imgur.com/YQni7Dz.png)\r\n\r\n- **Order View**\r\n\r\n![How to configure Login as Customer module](https://i.imgur.com/pdS3kop.png)\r\n\r\n\r\n\r\n### 6.2. Login as Customer Logs\r\n\r\nFrom **Admin panel**, select `Report > Customer > Login as Customer Logs`\r\n\r\n- Any access from any admin account will be recorded\r\n- Not allowed to delete or edit any information on this log history\r\n- Admin can export any log to CSV or XML files\r\n\r\n![Magento 2 Login as Customer Logs](https://i.imgur.com/CoeCxzo.png)\r\n\r\n**People also search:**\r\n- magento 2 login as customer\r\n- login as customer magento 2\r\n- magento 2 login as customer extension\r\n- mageplaza login as customer\r\n- mageplaza login as customer magento 2\r\n- magento 2 login to your customer account\r\n- login as customer magento 2\r\n\r\n\r\n**Other free extension on Github**\r\n- [Magento 2 SEO](https://github.com/mageplaza/magento-2-seo)\r\n- [magento 2 Google Maps](https://github.com/mageplaza/magento-2-google-maps)\r\n- [Magento 2 Delete Orders](https://github.com/mageplaza/magento-2-delete-orders)\r\n- [Magento 2 GDPR Free](https://github.com/mageplaza/magento-2-gdpr)\r\n- [Magento 2 blog](https://github.com/mageplaza/magento-2-blog)\r\n- [Magento 2 Same Order Number](https://github.com/mageplaza/magento-2-same-order-number)\r\n- [Magento 2 Layered Navigation](https://github.com/mageplaza/magento-2-ajax-layered-navigation)\r\n- [Magento 2 security](https://github.com/mageplaza/magento-2-security)\r\n\r\n**Get more [Magento 2 extension on Marketplace](https://marketplace.magento.com/partner/Mageplaza):**\r\n- [Magento 2 Configurable Product Preselect](https://marketplace.magento.com/mageplaza-module-configurable-product-grid-view.html)\r\n- [Magento 2 Payment Restriction](https://marketplace.magento.com/mageplaza-module-payment-restriction.html)\r\n- [Magento 2 Frequently Bought Together](https://marketplace.magento.com/mageplaza-module-frequently-bought-together.html)\r\n- [Magento 2 Shipping Rules](https://marketplace.magento.com/mageplaza-module-shipping-rules.html)\r\n- [Magento 2 Abandoned Cart Email](https://marketplace.magento.com/mageplaza-module-abandoned-cart-email.html)\r\n- [Magento 2 SMTP](https://marketplace.magento.com/mageplaza-module-smtp.html)\r\n- [Magento 2 Shipping Restrictions](https://marketplace.magento.com/mageplaza-module-shipping-restriction.html)\r\n- [Magento 2 Multiple Coupons](https://marketplace.magento.com/mageplaza-module-multiple-coupons.html)\r\n- [Magento 2 Order Attributes](https://marketplace.magento.com/mageplaza-module-order-attributes.html)\r\n- [Magento 2 Barcode](https://marketplace.magento.com/mageplaza-module-barcode.html)\r\n"}, {"repo": "makeyourcloud/myc-vtiger-customer-portal", "language": "JavaScript", "readme_contents": "MYC Vtiger Customer Portal\n============================================\nAlternative vTiger Customer Portal - Bootstrap 3 Based - Modular Code\n\nPlease refer to our forum for any support question: http://forum.makeyourcloud.com  \nFor more informations, Tutorials, F.A.Q, Themes and Plugins you can visit our website: http://makeyourcloud.com  \n\nUPDATED VERSION 0.8.1 - MINOR BUG FIXES\n\nUPDATED VERSION 0.8 - NEW FEATURES & BUG FIXES:\n\n\n[FIXED] Problems reported in the setup process for some servers \n\n[FIXED] Language issues\n\n[FIXED] Minor bugs\n\n[FIXED] Minor pdf download problems, ENHANCED PDFMaker compatibility\n\n[NEW FEATURE] Totally RENEWED ADMIN LAYOUT, categorized options to simplify the configuration process\n\n[NEW FEATURE] Integrated MYC WBSERVICES CLIENT library\n\n[NEW FEATURE] [LANGUAGE] FRENCH LANGUAGE pack integrated (provided by Chris)\n\n[NEW FEATURE] PLUGINS structure to allow easy extension of the portal functionalities\n\n[NEW FEATURE] PLUGINS STORE and THEMES STORE section.\n\n[NEW FEATURE] APPEARANCE SETTINGS section.\n\n[NEW FEATURE] DATE FORMAT option in Appearance settings menu to allow you specify how to show dates to your customers\n\n[NEW FEATURE] PORTAL TITLE option in Appearance settings menu to allow you specify the name to show as the page title \n\n[NEW FEATURE] [PLUGIN] FREE plugin module to manage EVENTS, that allow you to choose WHICH FIELDS to show for the events LIST VIEW SPECIFYING the display ORDER, also provide an EVENTS CALENDAR view.\n\n[NEW FEATURE] [PLUGIN] FREE plugin to allow you CUSTOMIZE the portal MENU SORTING.\n\nNOTES:\nSome plugins like Event Plugin and Inventory Lines requires valid vtiger api credentials to work, this is required to enable the Standard Webservices Api related plugins but not indispensable to make the customer portal working, if you don\u2019t want to insert, or you insert incorrect credentials all the plugins that requires this connection will be disabled but the portal will still work as usual.\nThe same concept is applied to the google map api key, if you don\u2019t provide an api key the map in the event plugin detail view will be disabled and the address text will be displayed instead.\n\nUPDATE INSTRUCTIONS:\nThis update make some fundamental changes to the base code of the precedent version, our advice is to make a new fresh installation with the new files then apply your own customization ( also download an the updated version of our themes if you use it and upload the updated theme zip using the new theme manager section ).\n\n\n\n\n\n\nVERSION 0.5 - NEW FEATURES & BUX FIXES:\n\n[FIX] - Fixed a problem with the default index module permission, now it follow the order as you set in you CRM in the section customer portal settings\n\n[FIX] - Fixed problems with the translation engine\n\n[FIX] - Optimized attachment upload process\n\n[FIX] - Other general bus fixes\n\n[FIX] - Config files structure reimplemented\n\n[FIX] - Page titles enhanced ( now dynamically generated )\n\n[NEW FEATURE] - Implemented native compatibility with PDFMaker, you don\u2019t need to apply any patch ( on the portal side )\n\n[NEW FEATURE] - Added a New Dashboard view containing the most important modules summary info \n\n[NEW FEATURE] - Vtwsclib libraries integrated ( vtiger webservices )\n\n[NEW FEATURE] - Fullcalendar javascript libraries integrated (javascript graphic calendar)\n\n[NEW FEATURE] - Google maps API integration ( To show the Event location on map )\n\n[NEW FEATURE] - Added module Events with calendar ( vtiger api credentials required )\n\n[NEW FEATURE] - Added jstz library to help provide support for dynamic event time changing depending on browser timezone\n\n[NEW FEATURE] - Added a new visual configuration tool to help you to set-up and customize your portal\n\n[NEW FEATURE] - Added possibility to upload and set a portal logo from the configuration tool\n\n[NEW FEATURE] - Theme manager with new theme store integrated in the configuration tool that will help you to upload your custom theme zip, set your default theme and discover new themes directly from the store.\n\n\n\n\n\nVERSION 0.1 - PORTAL FEATURES:\n\n-Modular MVC Pattern and Object Oriented Code\n\n-Easily extensible and customizable\n\n-Bootstrap 3 Based layouts and themes\n\n-Support for multiple themes and instant theme preview\n\n-Easy setup: just 3 step and you are under production!\n\n-Easy themes customization without confusion!\n\n-Mobile Ready and 100% Responsive.\n\n-Compatible with vTiger 5.x and 6.x\n\n\n\nCopyright 2014 - 2015 Proseguo s.l. - MakeYourCloud"}, {"repo": "sisl/CustomerSim", "language": "Python", "readme_contents": "# CustomerSim\n\nThis repository contains code for `Customer simulation for direct marketing experiments` paper, [IEEE DSAA 2016](http://ieeexplore.ieee.org/document/7796934/). Authors: Yegor Tkachenko, Mykel J. Kochenderfer, Krzysztof Kluza.\n\n# Software\n\nPython is our main language (with some preprocessing done in R).\n\nYou will need the following Python packages\n\n+ libpgm - for learning Bayesian networks (install from `https://github.com/CyberPoint/libpgm`)\n+ Keras and Theano - for deep learning (make sure you have the cutting edge theano via `pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git`)\n+ scikit-learn - for random forests\n\n+ NumPy\n+ SciPy\n+ Matplotlib\n+ pickle\n+ networkx\n+ json\n+ pandas\n+ Cython\n+ H5PY\n\nR prerequisites include\n\n+ plyr\n+ zipcode\n\n# Data\n\n+ KDD1998: `https://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html` (accessed on 2015-07-04)\n+ Valued Shoppers: `https://www.kaggle.com/c/acquire-valued-shoppers-challenge` (accessed on 2015-10-07)\n\n# Getting started\n\n+ `src` folder contains the code to replicate results of the paper\n+ the folder should be placed within a parent folder that contains unzipped data, downloaded from the links above\n+ then run `bash train_and_validate_all.sh` - it will take a couple of hours to finish (to preprocess the data, to train and validate the models)\n+ all results will be stored in the `results` folder\n\nOnce the code has run, follow code in `simulate_kdd98.py` and `simulate_vs.py` to perform simulation based on KDD1998 and Kaggle Valued Shoppers data sets (respectively). You can also check out files in `src` folder for details on validation procedure.\n\nNote, this code contains a correction and uses &tau;=0.275 in KDD98 simulation, instead of the &tau;=0.25 in the paper, leading to improved results in `KDD1998 simulation, 18 times step` section of Table II of the paper.\n\n# Required directory structure\n\n+ customersim\n\t+ train_and_validate_all.sh\n\t+ simulate_kdd98.py\n\t+ simulate_vs.py\n\t+ kdd98_data\n\t\t+ cup98LRN.txt\n\t\t+ cup98VAL.txt\n\t+ kaggle_valued_shoppers\n\t\t+ offers\n\t\t+ testHistory\n\t\t+ trainHistory\n\t\t+ transactions\n\t+ src\n\t\t+ net_designs.py\n\t\t+ shared_functions.py\n\t\t+ kdd98_preprocess.R\n\t\t+ kdd98_initial_snapshot.py\n\t\t+ kdd98_propagate_classifier.py\n\t\t+ kdd98_propagate_regressor.py\n\t\t+ kdd98_simulate.py\n\t\t+ vs_preprocess_category.py\n\t\t+ vs_propagate_regressor_category.py\n\t\t+ vs_simulate.py\n\n# Technical note - recency metric\n\nWe take use of \"transaction recency\" and \"interaction recency\" metrics in both simulators, where by recency we mean the \"number of periods elapsed since the last transaction\" (or since the last interaction). This definition runs into a problem when we ask what should the value of these metrics be before the customer has transacted or has been interacted with. For this reason, it may be desirable to focus on analysis of \"repeat\" transactions only (effectively discarding the information about the preceding period), and so transaction recency has a clear interpretation. This approach, however, also runs into some problems. \n\nFor example, if we are tracking, in parallel, customer transactions and marketing interactions with the customer, we may be in a situation where the customer has transacted several times, but we have not interacted with him once. At that point in time transaction recency metric would have a clear interpretation, but it would still be not clear what value to set the interaction recency metric to. And focusing only on repeat interactions, as often done in the literature, would lead us to potentially discard a lot of otherwise useful data.\n\nAn alternative approach, which we take in this work, is to redefine \"transaction recency\" not only as \"time elapsed since the transaction\", but also, if no transaction has yet occurred, as \"time elapsed since the beginning of observation of the group of customers\". The advantage of this approach is that it removes the need to recluse oneself to analysis of repeat events only and data discarding, alleviates the need to resolve conflicts between different metrics tracked in parallel, and also significantly simplifies the computation. \n\nThe disadvantage is that transaction recency metric acquires different meanings for customers with frequency of transaction >0 and for customers with frequency of transaction = 0, which may lead to noise if we try to use the simulator to learn customer lifetime value in these parts of state space. However, while the learning may be slightly trickier, given that both frequency and recency of transactions are included in the customer state description, a good function approximator can learn  variation in customer value between those different states. (The above discussion applies to interaction recency too).\n\n\n# License\n\nThe software is distributed under the Apache License 2.0\n"}, {"repo": "ameerbadri/amazon-alexa-twilio-customer-service", "language": "Python", "readme_contents": "# Amazon Alexa and Twilio Powered Customer Service\nImagine a scenario where you can quickly request a phone callback from a customer service agent without searching for or dialing a company\u2019s toll-free phone support line.\n\nToday, such streamlined customer journeys are easy to create when we consider augmenting the emerging world of voice assistant driven customer interactions with existing contact centre investments.  One such technology is Amazon Alexa, it provides the programming blocks to implement voice based interactions with a customer through the Amazon Echo speaker.  While Twilio provides the voice communication layer between the customer and your backend contact centre infrastructure.\n\n# A possible customer journey using Amazon Echo:\n* Customer initiates the callback service skill by saying: \u201cAlexa ask Twilio customer callback\u201d\n* Alexa interacts and collects the context and reason for callback request from the customer.\n* Then Alexa custom skills (via the AWS lambda function) invokes the Twilio server side Python SDK.\n* Twilio Python SDK using the Twilio REST API follows one of the three possible flows to connect to the call centre:\n\t* For PSTN connectivity to the contact centre, Twilio will dial the contact centre and customer\u2019s phone number and bridge the two party.\n\t* For SIP connectivity to the contact centre, Twilio will connect to  the contact centre's (PBX/SBC) over SIP 2.0 and customer\u2019s phone number and bridge the two party.\n\t* For connection to a Twilio based contact centre, your web application  will first create a task for the agent (using TaskRouter).  When the next available agent accepts the task, Twilio will initiate a call from the agent desktop (using WebRTC) to the customer\u2019s phone (via global carrier network) and bridge the two party.\n\nTo learn how to build your own Amazon Echo/Alexa custom skills and integrate into your existing contact centre using Twilio's APIs, please follow this post for step by step instructions.\n\nLet's get started... \n\n# Pre-requisites  \n* Amazon Echo (you can buy here: [Amazon UK](https://www.amazon.co.uk/dp/B01GAGVIE4), [Amazon US](https://amzn.com/B00X4WHP5E))\n* Knowledge of [Alexa Custom Skills](https://developer.amazon.com/edw/home.html#/skills/list)\n* Twilio Account [Sign up here](https://www.twilio.com/try-twilio)\n* Knowledge of [Twilio voice API](https://www.twilio.com/voice/api)\n\nLevel: Intermediate, Advanced\n\n# High level Architecture:\n![](Signal_London_2016_Building_A_Twilio_Powered_Contact_Center.001.jpeg)\n\n![](Signal_London_2016_Building_A_Twilio_Powered_Contact_Center.002.jpeg)\n\n\n# Architectural Components Setup:\n  a) Alexa Custom Skills Configuration:\n  The Alexa custom skills provides speech to text functionality, intent recognition and extraction of attributes from spoken phrases.  These instructions assume you\u2019re familiar with developing Alexa custom skills. \n\n# Skill Information setup:\n![](alexa_interaction_model_1.png)\n\n# Interaction Model:\n  Next, we need to setup and train Alexa with our interaction model.  Your setup will look like this:\nIntent Schema:\n![](alexa_interaction_model_2.png)\n\nCustom Slots:\n![](alexa_interaction_model_3.png)\n\nSample Utterances:\n![](alexa_interaction_model_4.png)\n\t\nNote: The source model can be found on this github repository \n\nb) AWS Lambda function to interact with Alexa skills: \nThe AWS lambda function works in conjunction with Alexa custom skills to provide the business logic needed for the conversation between the user and Alexa.  It also gathers the required attributes and eventually passes it to Twilio REST API to make the outbound call to the contact centre and customer.\nYou\u2019ll need to configure the following:\nconfig.py: Replace the value for Twilio Account Sid and Auth Token with your own credentials.\n![](config_setup.png)\n\nLambda_function.py: Replace the following values.\n![](lambda_function_change_1.png)\n\n![](lambda_function_change_2.png)\n\nOnce you have made the above changes, you\u2019ll need to zip all the files in the \u201clambda function\u201d folder.  On OS X, the system will create Archive.zip file.  You\u2019ll upload this ZIP file to AWS as your lambda function.\n\nNote: Makes sure you only zip the contents (files and sub-folders)  of the \u201clambda function\u201d folder and not the folder itself.\nThe source code for the Lambda function can be found on this github repository \n\nNext, you\u2019ll need to enable this custom skill in your Alexa app connected to your Amazon Echo.\nFinally, invoke the Alexa customer callback skill by saying the following to your Amazon Echo:\n# \u201cAlexa open twilio customer callback\u201d\n\n# Summary\nIt\u2019s important to serve customers over newly emerging channels for better customer experience and to differentiate your brand.  Integrating Amazon Echo and Alexa Skills as newer channels into your existing contact center infrastructure is pretty easy.  Twilio provides the critical communications glue between these consumer devices and your backend contact centre infrastructure.\n\nI hope you found this post helpful.  I look forward to hearing about your use cases.\n"}, {"repo": "bplank/ijcnlp2017-customer-feedback", "language": "Python", "readme_contents": "# ijcnlp2017-customer-feedback\n\nThe code for the system described in Plank (2017).\n\nSimple system (SVM) that won (ranked 1st out of 12 teams) in the IJCNLP 2017\nshared task 4 on Customer Feedback Analysis:\nhttps://sites.google.com/view/customer-feedback-analysis/\n\n```\n@InProceedings{plank:2017:IJCNLP,\n  author    = {Plank, Barbara},\n  title     = {All-In-1: Short Text Classification with One Model for All Languages},\n  booktitle = {Proceedings of the International Joint Conference on Natural Language Processing (Shared Task 4)},\n  month     = {December},\n  year      = {2017},\n  address   = {Taipei, Taiwan},\n  publisher = {Association for Computational Linguistics},\n}\n```\n"}, {"repo": "likeastore/ngCustomerVoice", "language": "CSS", "readme_contents": "# ngCustomerVoice\n\nSmall [Angular.js](http://angularjs.org) provider for sending feedback, Q&A and support for your application. It can be used as replacement for [Intercom](https://www.intercom.io) or [Uservoice](https://www.uservoice.com/) messaging plugins.\n\n[![](https://farm8.staticflickr.com/7346/14003765983_137b61a269_c.jpg)](http://likeastore.github.io/ngCustomerVoice)\n\n### [Demo](http://likeastore.github.io/ngCustomerVoice)\n\n## Install\n\nngCustomerVoice is a simple angular provider and directive based on [ngDialog](http://likeastore.github.io/ngDialog/) popups. So for proper work you can install it through [Bower](http://bower.io/) package manager (recommended):\n\n```bash\nbower install ngCustomerVoice\n```\n\nAnd don't forget to include all necessary javascript and css files to your application:\n\n- ``ngDialog.css``\n- ``ngCustomerVoice.css``\n- ``angular.js``\n- ``ngDialog.js``\n- ``ngCustomerVoice.js``\n\n## Usage\n\nTo start using plugin you just need to insert **directive** into your template:\n\n```html\n<div ng-customer-voice=\"'your.user@email.com'\"></div>\n```\n\nYou are able to insert some identifier of a user (email, username, id, etc.) but it's optional, this can be handled on your server instead.\n\nAnd inside your app configuration  use ``ngCustomerVoiceProvider`` to specify which API url will be requested:\n\n```js\nvar app = angular.module('YourApp', ['ngDialog', 'ngCustomerVoice']);\napp.config(['ngCustomerVoiceProvider', function (ngCustomerVoiceProvider) {\n\tngCustomerVoiceProvider.apiUrl('/your/email/server');\n}]);\n```\n\nThat's it!\n\n## License\n\nMIT Licensed\n\nCopyright (c) 2014, Likeastore.com info@likeastore.com\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"}, {"repo": "vercel/customer-support-examples", "language": "JavaScript", "readme_contents": "<div align=\"center\">\n<h1>Real-time Customer Support Widget Examples</h1>\n<a href=\"https://zeit.co/blog/real-time-customer-support\">Read the blog post</a>\n<span>&nbsp;&nbsp;&nbsp;&nbsp;</span>\n<a href=\"https://customer-support.now.sh\">Checkout the demo</a>\n</div>\n\n## Intro\n\nThis repository demonstrates integrating different customer messaging platforms to a [Next.js](https://nextjs.org) application with [React Hooks](https://reactjs.org/docs/hooks-intro.html). We make sure of different fallbacks to make the experience accessible to different types of users.\n\nThe chat platforms included here are  [Intercom](https://www.intercom.com/), [Zendesk](https://www.zendesk.com/), [Drift](https://www.drift.com), [Chatlio](https://chatlio.com/), and [Crisp](https://crisp.chat).\n\n## Develop Locally\n\nOnce you have [Now CLI](https://zeit.co/download) or [Now Desktop](https://zeit.co/download) installed, clone this repo, and run `now dev` to start your journey. \n\nTo start testing, run `yarn test`.\n\n## Deploy to Now\n\nInstall [Now CLI](https://zeit.co/download) or [Now Desktop](https://zeit.co/download), specify secrets with Now CLI, and run `now` to deploy your application to [Now](https://zeit.co/now).\n\nFor information on environment variables and secrets, check out our [documentation](https://zeit.co/docs/v2/deployments/environment-variables-and-secrets/).\n\n## Credits\n\n* [react-intercom](https://github.com/nhagen/react-intercom)\n* [react-driftjs](https://github.com/chardmd/react-drift)\n\n\n## About\n\nOur mission at ZEIT is to make the cloud accessible to everyone. We do that by creating products that improve developer experience, provisioning infrastructure that is globally available, and by teaching the developer community about serverless-related technology. We made VRS to showcase that it\u2019s possible to create a fully functional, high-performance e-commerce store without requiring infrastructure know-how.\n\nFollow us on [Twitter](https://twitter.com/zeithq).\n\n<br/>\n<br/>\n\n[![](https://assets.zeit.co/image/upload/v1556749970/repositories/vrs/zeit.svg)](https://zeit.co)\n\n"}, {"repo": "ashus3868/RasaCustomerService", "language": "Python", "readme_contents": "# RasaCustomerService\nThis is a conversational bot that can be used in the telecom sector for automating the voice bot process with the help of this Rasa Chatbot.\n"}, {"repo": "customerio/go-customerio", "language": "Go", "readme_contents": "# Customerio\n\n# go-customerio [![CircleCI](https://circleci.com/gh/customerio/go-customerio/tree/master.svg?style=svg)](https://circleci.com/gh/customerio/go-customerio/tree/master)\n\nA golang client for the [Customer.io](http://customer.io) [event API](https://app.customer.io/api/docs/index.html).\n_Tested with Go1.16_\n\nGodoc here: [https://godoc.org/github.com/customerio/go-customerio](https://godoc.org/github.com/customerio/go-customerio)\n\n## Installation\n\nAdd this line to your application's imports:\n\n```go\nimport (\n    // ...\n    \"github.com/customerio/go-customerio/v3\"\n)\n```\n\nAnd then execute:\n\n    go get\n\nOr install it yourself:\n\n    $ go get github.com/customerio/go-customerio\n\n## Usage\n\n### Before we get started: API client vs. JavaScript snippet\n\nIt's helpful to know that everything below can also be accomplished\nthrough the [Customer.io JavaScript snippet](http://customer.io/docs/basic-integration.html).\n\nIn many cases, using the JavaScript snippet will be easier to integrate with\nyour app, but there are several reasons why using the API client is useful:\n\n- You're not planning on triggering emails based on how customers interact with\n  your website (e.g. users who haven't visited the site in X days)\n- You're using the javascript snippet, but have a few events you'd like to\n  send from your backend system. They will work well together!\n- You'd rather not have another javascript snippet slowing down your frontend.\n  Our snippet is asynchronous (doesn't affect initial page load) and very small, but we understand.\n\nIn the end, the decision on whether or not to use the API client or\nthe JavaScript snippet should be based on what works best for you.\nYou'll be able to integrate **fully** with [Customer.io](http://customer.io) with either approach.\n\n### Setup\n\nCreate an instance of the client with your [Customer.io credentials](https://fly.customer.io/settings/api_credentials).\n\n```go\ntrack := customerio.NewTrackClient(\"YOUR SITE ID\", \"YOUR API SECRET KEY\", customerio.WithRegion(customerio.RegionUS))\n```\n\nYour account region\u2014`RegionUS` or `RegionEU`\u2014is optional. If you do not specify your region, we assume that your account is based in the US (`RegionUS`). If your account is based in the EU and you do not provide the correct region, we'll route requests from the US to `RegionEU` accordingly, however this may cause data to be logged in the US. \n\n### Identify logged in customers\n\nTracking data of logged in customers is a key part of [Customer.io](http://customer.io). In order to\nsend triggered emails, we must know the email address of the customer. You can\nalso specify any number of customer attributes which help tailor [Customer.io](http://customer.io) to your\nbusiness.\n\nAttributes you specify are useful in several ways:\n\n- As customer variables in your triggered emails. For instance, if you specify\n  the customer's name, you can personalize the triggered email by using it in the\n  subject or body.\n\n- As a way to filter who should receive a triggered email. For instance,\n  if you pass along the current subscription plan (free / basic / premium) for your customers, you can\n  set up triggers which are only sent to customers who have subscribed to a\n  particular plan (e.g. \"premium\").\n\nYou'll want to identify your customers when they sign up for your app and any time their\nkey information changes. This keeps [Customer.io](http://customer.io) up to date with your customer information.\n\n```go\n// Arguments\n// customerID (required) - a unique identifier string for this customers\n// attributes (required) - a ```map[string]interface{}``` of information about the customer. You can pass any\n//                         information that would be useful in your triggers. You\n//                         should at least pass in an email, and created_at timestamp.\n//                         your interface{} should be parseable as Json by 'encoding/json'.Marshal\n\ntrack.Identify(\"5\", map[string]interface{}{\n  \"email\": \"bob@example.com\",\n  \"created_at\": time.Now().Unix(),\n  \"first_name\": \"Bob\",\n  \"plan\": \"basic\",\n})\n```\n\n### Deleting customers\n\nDeleting a customer will remove them, and all their information from\nCustomer.io. Note: if you're still sending data to Customer.io via\nother means (such as the javascript snippet), the customer could be\nrecreated.\n\n```go\n// Arguments\n// customerID (required) - a unique identifier for the customer.  This\n//                          should be the same id you'd pass into the\n//                          `identify` command above.\n\ntrack.Delete(\"5\")\n```\n\n### Merge Duplicate Customers\n\nWhen you merge two people, you pick a primary person and merge a secondary, duplicate person into it. The primary person remains after the merge and the secondary is deleted. This process is permanent: you cannot recover the secondary person.\n\nThe first and third parameters represent the identifier for the primary and secondary people respectively\u2014one of `id`, `email`, or `cio_id`. The second and fourth parameters are the identifier values for the primary and secondary people, respectively.\n\n```go\ntrack.MergeCustomers(customerio.IdentifierTypeEmail, \"cool.person@company.com\", customerio.IdentifierTypeCioID, \"C123\")\n```\n\n### Tracking a custom event\n\nNow that you're identifying your customers with [Customer.io](http://customer.io), you can now send events like\n\"purchased\" or \"watchedIntroVideo\". These allow you to more specifically target your users\nwith automated emails, and track conversions when you're sending automated emails to\nencourage your customers to perform an action.\n\n```go\n// Arguments\n// customerID (required)  - the id of the customer who you want to associate with the event.\n// name (required)        - the name of the event you want to track.\n// attributes (optional)  - any related information you'd like to attach to this\n//                          event, as a ```map[string]interface{}```. These attributes can be used in your triggers to control who should\n//                         receive the triggered email. You can set any number of data values.\n\ntrack.Track(\"5\", \"purchase\", map[string]interface{}{\n    \"type\": \"socks\",\n    \"price\": \"13.99\",\n})\n```\n\n### Tracking an anonymous event\n\nYou can also send anonymous events representing people you haven't identified. An anonymous event requires an `anonymous_id` representing the unknown person and an event `name`. When you identify a person, you can set their `anonymous_id` attribute. If [event merging](https://customer.io/docs/anonymous-events/#turn-on-merging) is turned on in your workspace, and the attribute matches the `anonymous_id` in one or more events that were logged within the last 30 days, we associate those events with the person.\n\n```go\n// Arguments\n// anonymous_id (required)    - an identifier representing an unknown person.\n// name (required)            - the name of the event you want to track.\n// attributes (optional)      - any related information you'd like to attach to this\n//                              event, as a ```map[string]interface{}```. These attributes can be used in your triggers to control who should\n//                              receive the triggered email. You can set any number of data values.\n\ntrack.TrackAnonymous(\"anonymous_id\", \"invite\", map[string]interface{}{\n    \"first_name\": \"Alex\",\n    \"source\": \"OldApp\",\n})\n```\n\n### Adding a device to a customer\n\nIn order to send push notifications, we need customer device information.\n\n```go\n// Arguments\n// customerID (required) - a unique identifier string for this customer\n// deviceID (required)   - a unique identifier string for this device\n// platform (required)   - the platform of the device, currently only accepts 'ios' and 'andriod'\n// data (optional)        - a ```map[string]interface{}``` of information about the device. You can pass any\n//                         key/value pairs that would be useful in your triggers. We\n//                         currently only save 'last_used'.\n//                         your interface{} should be parseable as Json by 'encoding/json'.Marshal\n\ntrack.AddDevice(\"5\", \"messaging token\", \"android\", map[string]interface{}{\n\"last_used\": time.Now().Unix(),\n})\n```\n\n### Deleting devices\n\nDeleting a device will remove it from the customers device list in Customer.io.\n\n```go\n// Arguments\n// customerID (required) - the id of the customer the device you want to delete belongs to\n// deviceToken (required) - a unique identifier for the device.  This\n//                          should be the same id you'd pass into the\n//                          `addDevice` command above\n\ntrack.DeleteDevice(\"5\", \"messaging-token\")\n```\n\n### Send Transactional Messages\n\nTo use the Customer.io [Transactional API](https://customer.io/docs/transactional-api), create an instance of the API client using an [app key](https://customer.io/docs/managing-credentials#app-api-keys).\n\nCreate a `SendEmailRequest` instance, and then use `SendEmail` to send your message. [Learn more about transactional messages and optional `SendEmailRequest` properties](https://customer.io/docs/transactional-api).\n\nYou can also send attachments with your message. Use `Attach` to encode attachments.\n\n```go\nimport \"github.com/customerio/go-customerio\"\n\nclient := customerio.NewAPIClient(\"<extapikey>\", customerio.WithRegion(customerio.RegionUS));\n\n// TransactionalMessageId \u2014 the ID of the transactional message you want to send.\n// To                     \u2014 the email address of your recipients.\n// Identifiers            \u2014 contains the id of your recipient. If the id does not exist, Customer.io creates it.\n// MessageData            \u2014 contains properties that you want reference in your message using liquid.\n// Attach                 \u2014 a helper that encodes attachments to your message.\n\nrequest := customerio.SendEmailRequest{\n  To: \"person@example.com\",\n  TransactionalMessageID: \"3\",\n  MessageData: map[string]interface{}{\n    \"name\": \"Person\",\n    \"items\": map[string]interface{}{\n      \"name\": \"shoes\",\n      \"price\": \"59.99\",\n    },\n    \"products\": []interface{}{},\n  },\n  Identifiers: map[string]string{\n    \"id\": \"example1\",\n  },\n}\n\n// (optional) attach a file to your message.\nf, err := os.Open(\"receipt.pdf\")\nif err != nil {\n  fmt.Println(err)\n}\nrequest.Attach(\"receipt.pdf\", f)\n\nbody, err := client.SendEmail(context.Background(), &request)\nif err != nil {\n  fmt.Println(err)\n}\n\nfmt.Println(body)\n```\n\n## Contributing\n\n1. Fork it\n2. Clone your fork (`git clone git@github.com:MY_USERNAME/go-customerio.git && cd go-customerio`)\n3. Create your feature branch (`git checkout -b my-new-feature`)\n4. Commit your changes (`git commit -am 'Added some feature'`)\n5. Push to the branch (`git push origin my-new-feature`)\n6. Create new Pull Request\n"}, {"repo": "codingmonk21/CustomerSupportDesk", "language": "Java", "readme_contents": "\n[![](https://jitpack.io/v/dev-prajwal21/CustomerSupportDesk.svg)](https://jitpack.io/#dev-prajwal21/CustomerSupportDesk)\n\n# CustomerSupportDesk\nCustomerSupportDesk is a unique solution for adding Customer-Support module in any android app. The growth of any business purely depends on customer satisfaction, but the sustainance of growth in long term simply depends on the quality of support offered to help customers overcome the issues they face. Customer-Support, which is often neglected and taken for granted can sometime take a huge toll and make the users step out of the product in no time. The motive behind building this library is to address this common issue faced by all app developers so that they don't lose their users and keep increasing the userbase by offering a user-friendly and quality Customer-Support service. \n\nThe good thing about this library is that it is fully built on firebase! \nYes, you don't need any backend infrastructure to use this library. Right from filling the user details to raising a complaint/suggestion/feedback and resolving those tickets by the admin, everything happens on firebase.\n\nThe library also provides the solution for monitoring and resolving the tickets raised by users by providing a built in admin section! \nSo, you need not even think of building an admin panel to monitor and maintain the tickets raised by the users. It can be seen on the go and in real time.\n\nAnd the best part is, all the above said features comes in just ten lines of code!\n\n# Usage\nThe library has two sections,\n1) Customer-Support section\n2) Admin section\n\nUsing the admin section is optional and is upto the choice of the developer. But its recommended to use, as it comes handy on the go.\n\n# Customer-Support Section\n\n<h4> Step 1: </h4>\n  Create a project in firebase console.\n<h4> Step 2: </h4>\n  Add the below code in the project level gradle under the dependencies{} block,\n  \n          classpath 'com.google.gms:google-services:3.1.0'\n  \n  Add the below code in module level gradle outside the dependencies{} block,\n  \n          apply plugin: 'com.google.gms.google-services'\n  \n  Add the below code in module level gradle inside defaultConfig{} block,\n  \n          multiDexEnabled true\n          \n  Add the below code in the project level gradle inside repositories block{},\n          \n          maven { url 'https://jitpack.io' }\n          \n<h4> Step 3: </h4>\n  Download the google.json file from the project created in firebase console and paste it in 'app' folder of your project\n  \n<h4> Step 4: </h4>\n  Add the below code in the module level gradle to include CustomerSupportDesk library,\n  \n          compile 'com.github.dev-prajwal21:CustomerSupportDesk:1.0'\n  \n<h4> Step 5: </h4>\n  Goto firebase console > Select the project you created > Select firebase database from menu.\n  \n  Click on the rules tabs and replace the content with below code,\n  \n      {\n        \"rules\": {\n        \".read\": \"auth == null\",\n        \".write\": \"auth == null\"\n      }\n    }\n    \n  Now, Select the firebase storage from the menu.\n  \n  Click on the rules tabs and replace the content with below code,\n  \n      service firebase.storage {\n        match /b/{bucket}/o {\n          match /{allPaths=**} {\n            allow read, write: if request.auth == null;\n        }\n      } \n    }\n\n<h4> Step 6: </h4>\n  Place a button anywhere in the app according to your design to navigate to customer-support screen.\n  <h4> Note: </h4> \n  Its not necessary that you need to use only button. It can be any view.\n\n<h4> Step 7: </h4> \n  Get a reference to sharedPreferenceHelper object in the activity/fragment where the button (or any view) used to               navigate to customer-support screen.\n  \n  Implement the UserRegistrationListener in the activity/fragment where the button(or any view) used to navigate to             customer-support screen as shown below,\n      \n      public class MainActivity extends AppCompatActivity implements UserRegistrationListener {\n\n        private SharedPreferenceHelper sharedPreferenceHelper;\n\n        @Override\n        protected void onCreate(Bundle savedInstanceState) {\n          super.onCreate(savedInstanceState);\n          setContentView(R.layout.activity_main);\n\n          sharedPreferenceHelper = new SharedPreferenceHelper(this, Config.SP_ROOT_NAME);\n        \n        }\n\n        @Override\n        public void registrationStatus(boolean isUserRegistered) {\n        \n        }\n        \n      }\n\n<h4> Step 8: </h4>\n  Copy paste the below code in the interface method registrationStatus(),\n        \n        if (isUserRegistered) {\n            if (sharedPreferenceHelper.getBoolean(Config.SP_ADMIN_LOGIN, false)) {\n                sharedPreferenceHelper.putBoolean(Config.SP_ADMIN_LOGIN, false);\n            }\n            startActivity(new Intent(this, CustomerDashboard.class));\n        } else {\n            startActivity(new Intent(this, ProfileFillUpActivity.class));\n        }\n        \n<h4> Step 9: </h4> \n  Get a reference for the button (or any view) used to navigate to customer-support screen and attach a click listener to it.\n\n      contactSupportBtn = (Button) findViewById(R.id.contactSupportBtn);\n      contactSupportBtn.setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n            \n            }\n        });\n        \n  Copy paste the below code inside the listener,\n      \n      UserRegistration userRegistration = new UserRegistration(MainActivity.this);\n      userRegistration.setUserRegistrationListener(MainActivity.this);\n      userRegistration.isUserRegistered(\"12345\");\n      \n  Replace \"12345\" by a uniqueID with which you identify your user. The ID should be unique for each user. \n      \n  Thats it!! Now your app has a customer-support module integrated to it.<br/> \n  Click on the button and fill the user details to proceed to the dashboard and add complaints, suggestions, feedback.\n  \n  Each time a complaint/suggestion/feedback is added, you can view the data in the firebase console. \n  \n# Admin Section\n\n  To add admin facility to monitor and resolve the tickets raised, you can use the same login system your app uses and           navigate to admin section. \n  \n  Authentication of admin can be done via webservice or you can even simply hardcode the admin credentials within the app so     that if the admin credentials are given it is navigated to admin dashboard.\n  \n  As an example here I'm using a button to simply go to admin section. \n  Modify this according to your need in the login screen so that a web-service or hardcoded values of admin credentials can be   used to navigate to admin dashboard.\n  \n      adminBtn = (Button) findViewById(R.id.adminBtn);\n      adminBtn.setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                sharedPreferenceHelper.putBoolean(Config.SP_ADMIN_LOGIN, true);\n                startActivity(new Intent(MainActivity.this, AdminDashboard.class));\n            }\n        });\n        \n  <h4> Note: </h4>\n  While logging in as admin it is mandatory to use the below line of code and then start the AdminDashboard activity, \n            \n      sharedPreferenceHelper.putBoolean(Config.SP_ADMIN_LOGIN, true);\n      \n # Screenshots\n <h3> Customer-Support Section: </h3>\n <p>\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-11-51-16-286.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-11-52-07-353.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-11-52-37-834.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n\n</p>\n\n<br/>\n\n<p>\n\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-11-52-58-747.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-11-52-47-667.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-12-21-31-744.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n</p>\n\n<br/>\n<p>\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-12-20-59-763.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n</p>\n\n<br/>\n\n <h3> Admin Section: </h3>\n\n<p>\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-11-53-17-997.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n\n<kbd>\n    <img src=\"https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/Screenshot_2017-07-23-11-53-36-677.jpeg\"            alt=\u201cScreenshot\u201d width=\u201c320px\u201d height = \"480px\"/>\n</kbd>\n</p>\n\n<br/>\n\n<b> Note: </b> <br/>\nPlease take a look at the sample app for more clarity on implementation. \n\n<b>Feel free to experiment on the project. Star the project if you liked my work and if it helped you.</b>\n\n<h4>Happy Coding :)</h4>\n\n<h3>Thanks</h3>\n<a href = 'https://github.com/ArthurHub/Android-Image-Cropper'>ArthurHub Cropper</a>\n\n<h3>License</h3>\n<a href = 'https://github.com/dev-prajwal21/CustomerSupportDesk/blob/master/LICENSE'>Apache License v2.0</a>\n"}, {"repo": "iris9112/Customer-Segmentation", "language": "Jupyter Notebook", "readme_contents": "# Customer Segmentation in Python\n\n## Course Description\n\nThe most successful companies today are the ones that know their customer so well that they can anticipate their needs. Data analysts play a key role in unlocking these in-depth insights, and segmenting the customers to better serve them. In this course, you will learn real-world techniques on customer segmentation and behavioral analytics, using a real dataset containing customer transactions from an online retailer. You will first identify which products are frequently bought together. Then, you will run cohort analysis to understand customer trends. On top of that, you will learn how to build easy to interpret customer segments. Finally, you will make your segments more powerful with k-means clustering, in just few lines of code! By the end of this course, you will be able to apply practical customer behavioral analytics and segmentation techniques.\n\n### Traducci\u00f3n:\n\nLas empresas m\u00e1s exitosas de hoy son las que conocen tan bien a sus clientes que pueden anticipar sus necesidades. Los analistas de datos desempe\u00f1an un papel clave en el desbloqueo de estos conocimientos en profundidad y en la segmentaci\u00f3n de los clientes para atenderlos mejor.\nEn este curso, aprender\u00e1 t\u00e9cnicas del mundo real sobre segmentaci\u00f3n de clientes y an\u00e1lisis de comportamiento, utilizando un conjunto de datos real que contiene transacciones de clientes de un minorista en l\u00ednea. Primero identificar\u00e1s qu\u00e9 productos se compran frecuentemente juntos. Luego, realizar\u00e1 un an\u00e1lisis de cohorte para comprender las tendencias de los clientes. Adem\u00e1s de eso, aprender\u00e1 c\u00f3mo crear segmentos de clientes f\u00e1ciles de interpretar. Finalmente, har\u00e1 que sus segmentos sean m\u00e1s potentes con el agrupamiento de k-means, \u00a1en solo unas pocas l\u00edneas de c\u00f3digo! Al final de este curso, podr\u00e1 aplicar t\u00e9cnicas pr\u00e1cticas de an\u00e1lisis de comportamiento y segmentaci\u00f3n del cliente.\n\nLink: [customer-segmentation-in-python](https://www.datacamp.com/courses/customer-segmentation-in-python)\n\n### Previous definition\n\nIn statistics, marketing and demography, a cohort is a group of subjects who share a defining characteristic (typically subjects who experienced a common event in a selected time period, such as birth or graduation). Cohort data can oftentimes be more advantageous to demographers than period data. Because cohort data is honed to a specific time period, it is usually more accurate. It is more accurate because it can be tuned to retrieve custom data for a specific study. \n\nSource: [wikiedia](https://en.wikipedia.org/wiki/Cohort_(statistics)) \n\n## Dataset\n\n### Source:\n\nDr Daqing Chen, Director: Public Analytics group. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\n\n### Data Set Information:\n\n[Online Retail Data Set](https://archive.ics.uci.edu/ml/datasets/Online%20Retail)\n\nThis is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\n### Attribute Information:\n\n- InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n- StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n- Description: Product (item) name. Nominal.\n- Quantity: The quantities of each product (item) per transaction. Numeric.\n- InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n- UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n- CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n- Country: Country name. Nominal, the name of the country where each customer resides.\n\n\n## Chapter 1: Cohort Analysis\n\nUnderstand customers based on their unique behavioral attributes. Cohort analysis provides deeper insights than the so-called vanity metrics. It helps with understanding the high level trends better by providing insights on metrics across both the product and the customer lifecycle.\n\n## Chapter 2:  Recency, Frequency, Monetary Value analysis\n\nUnderstand customers based on their unique behavioral attributes \n\n## Chapter 3: Data pre-processing for clustering\n\nLearn practical data preparation methods to ensure the k-means clustering algorithm uncovers well-separated segments \n\n## Chapter 4: Customer Segmentation with K-means\n\nUse data from previous chapter to build customer segments based on their recency, frequency, and monetary value \n\n## Prerequisites\n\n- pandas library\n- datetime objects\n- basic plotting with matplotlib or seaborn\n- basic knowledge of k-means clustering"}, {"repo": "sol-eng/customer-tracker", "language": "R", "readme_contents": "# Customer Tracker\n\nTurn your R code into reports, presentations, dashboards, apps, and API's with [R Markdown](https://rmarkdown.rstudio.com/), [Shiny](http://shiny.rstudio.com/), and [Plumber](https://www.rplumber.io/). This repos shows 9 different data products built on the same data set. Use whatever output type is appropriate for your situation. After you create your content, you can publish it to [RStudio Connect](https://www.rstudio.com/products/connect/).\n\n## Data Explanation\n\n<center><a href=\"http://colorado.rstudio.com:3939/Tracker/README.html\"><img src=\"img/tracker.png\" width=\"500\"/></a></center>\n\nThese reports show year over year performance for many customer segments and product groups. Top level performance (i.e. revenue) is decomposed into lower level performance metrics (i.e. customer visits, items purchased, and checkout spend). For example, in week 1 revenue for the all segments and products increased by 22.9 compared to last year. The main contributors to the increase were visits (+12.1%) and spend (+6.3%).\n\n## Output Types <img src=\"img/publish.png\" width=\"50\"/>\n\nThe following output formats for the customer tracker are hosted on [RStudio Connect](https://www.rstudio.com/products/connect/):\n\n#### Reports\n\n* [HTML Report](http://colorado.rstudio.com/rsc/tracker-report/tracker-report.html) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1609/access)</small>\n* [PDF Report](http://colorado.rstudio.com:3939/content/1644/tracker-pdf.pdf) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1644/access) </small>\n* [Word Report](http://colorado.rstudio.com:3939/content/1643/) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1643/access/1432) </small>\n\n#### Presentations\n\n* [HTML Presentation](http://colorado.rstudio.com:3939/content/1645/tracker-ioslides.html) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1645/access/1229)</small>\n* [PowerPoint Presentation](http://colorado.rstudio.com:3939/content/1646/) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1646/access/1429)</small>\n\n#### Shiny apps and docs\n\n* [Shiny App](http://colorado.rstudio.com:3939/tracker-app/) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1120)</small>\n* [Shiny Document](http://colorado.rstudio.com:3939/content/1121/) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1121/access)</small>\n* [Flexdashboard](http://colorado.rstudio.com:3939/content/1642/) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1642/access)</small>\n\n#### API's\n\n* [Plumber API](http://colorado.rstudio.com:3939/content/1117/) <small>[[login]](http://colorado.rstudio.com:3939/connect/#/apps/1117/access)</small>\n\n## RStudio Connect <img src=\"img/rsc-logo.png\" width=\"45\"/>\n\n[RStudio Connect](https://www.rstudio.com/products/connect/) is a server product from RStudio for secure sharing of R content. It is on-premises software you run behind your firewall. You keep control of your data and who has access. With RStudio Connect you can see all your content, decide who should be able to view and collaborate on it, tune performance, schedule updates, and view logs.\n\n## References \n\n* [Communicating Results with R Markdown](https://rviews.rstudio.com/2018/11/01/r-markdown-a-better-approach/)\n* [Enterprise Dashboards with R Markdown](https://rviews.rstudio.com/2018/05/16/replacing-excel-reports-with-r-markdown-and-shiny/)\n* [Enterprise Dashboards with Shiny and Databases](https://rviews.rstudio.com/2017/09/20/dashboards-with-r-and-databases/)\n\n"}, {"repo": "mageplaza/magento-2-customer-approval", "language": "PHP", "readme_contents": "# Magento 2 Customer Approval Module\r\n\r\n[Customer Approval by Mageplaza](http://www.mageplaza.com/magento-2-customer-approval/) is a solution which helps store owner to approve or reject new account registration from customers in specific cases. This is regarded as an effective control method of customer accounts in online stores using Magento 2 platform. \r\n\r\n\r\n## 1. Documentation\r\n\r\n- [Installation guide](https://www.mageplaza.com/install-magento-2-extension/)\r\n- [User guide](https://docs.mageplaza.com/customer-approval/index.html)\r\n- [Introduction page](http://www.mageplaza.com/magento-2-customer-approval/)\r\n- [Contribute on Github](https://github.com/mageplaza/magento-2-customer-approval)\r\n- [Get Support](https://github.com/mageplaza/magento-2-customer-approval/issues)\r\n\r\n\r\n## 2. FAQ\r\n\r\n**Q: I got error: Mageplaza_Core has been already defined**\r\n\r\nA: Read solution [here](https://github.com/mageplaza/module-core/issues/3)\r\n\r\n**Q: I have many sales campaigns, I would like give approval automatically or manually for a specific time? Can I do it?**\r\n\r\nA: Yes, for each period of your campaigns, you can set the approval to be automatic or manual from Auto Approve section of Customer Approval.\r\n\r\n**Q: After a new customer registers an account, how can I inform them to wait for verification?**\r\n\r\nA: You can set the message to customers at After-registration notification section. \r\n\r\n**Q: I am an admin. How can I know when a new account has been registered?**\r\n\r\nA: From the backend, kindly enable the function sending admin notification emails. You just need to add your emails on recipients part. \r\n\r\n**Q: How can I send customers the notification when their accounts has been approved?**\r\n\r\nA: You can configure this at Approve Notification section.\r\n\r\n\r\n**Q: How can I send customers the notification when their accounts has not been approved?**\r\n\r\nA: You can configure this at Not Approve Notification section.\r\n\r\n**Q: If customers are not approved to access page, can I redirect them to another page?**\r\n\r\nA: Yes, you can do it easily via Redirect CMS Page section from the backend.  \r\n\r\n\r\n## 3. How to install Magento 2 Customer Approval extension\r\n\r\nInstall via composer (recommend), run the following command in Magento 2 root folder:\r\n\r\nWith Marketing Automation (recommend):\r\n```\r\ncomposer require mageplaza/module-customer-approval mageplaza/module-smtp\r\nphp bin/magento setup:upgrade\r\nphp bin/magento setup:static-content:deploy\r\n```\r\n\r\nWithout Marketing Automation:\r\n```\r\ncomposer require mageplaza/module-customer-approval mageplaza/module-smtp\r\nphp bin/magento setup:upgrade\r\nphp bin/magento setup:static-content:deploy\r\n```\r\n\r\n## 4. Highlight Features\r\n\r\n\r\n### Auto or manual approval                                                                                                                                                               \r\nOne of the most noticeable features of Customer Approval is that the store owner can give accept customers\u2019 account registration automatically or manually.  \r\n\r\nIn case the approval is automatic, customers will be approved immediately when finishing account registration and can log in easily. By contrast, if store owners would like to control carefully visitors, the approval can be managed manually from store backend. \r\n\r\nThis flexibility in the approval methods supports stores in specific purposes and strategies, suitably in various types of business, various strategies in different periods.  \r\n\r\n![Auto or manual approval](https://i.imgur.com/ACGRXeO.png)\r\n\r\n\r\n### Error Notice or redirect\r\n\r\nWhat will happen when customers try to log in without approved accounts? Customer Approval offers two options with different actions: Error Notice or Redirect.\r\n\r\n#### Error notice \r\n\r\nIn case a customer uses their accounts which have not received permission to log in, an error notice will appear to notify them with a specific message. Admins can set any messages with ease from the backend. \r\n\r\n\r\n#### Redirect visitors to another page\r\n\r\nIn another way, customers whose accounts have not approved yet can be redirected to a specific page configured by the store admin. This is the page which is allowed to freely access without accounts or a simple notice page. \r\n\r\nThe store owner can choose redirected URL which suits a particular purpose such as introduction or marketing and so on. This navigation can avoid customers to feel disappointed or annoyed by their login failure. They take time with the recommended page and wait for the account acceptance.\r\n\r\n![Error Notice or redirect](https://i.imgur.com/ydIVVPY.gif)\r\n\r\n\r\n### Mass or Partial Approval\r\nAnother feature of Customer Approval is multi-select to approve or disapproved many accounts on the backend list. From admin backend, admins can select one, many or all customer accounts. Then, the select accounts can be approved or disapproved by just one click on a button as Approve or Not Approve quickly.\r\n\r\nThis helps admin can save a deal of time significantly in case there are a number of customer accounts waiting for verification. Moreover, it is very easy to change the status of already-approved accounts to prevent from login for some reason. \r\n\r\n![Mass or Partial Approval](https://i.imgur.com/zWp4pZG.png)\r\n\r\n\r\n### Instant notification emails \r\nNotification emails are supported in this extension. Both customers and admins can be informed instantly on any account updates. \r\n\r\nWith admins, when a new account has been registered, they will receive emails with details of customer accounts and remind them of approval. \r\nBesides, with customers, they will receive the notification emails accordingly when they register accounts successfully and when the accounts are approved or not approved. \r\n\r\n![Instant notification emails](https://i.imgur.com/KZRzrEs.png)\r\n\r\n### API is supported\r\n\r\nAPI is generated to support data among systems in stores. API can help systems in collecting the list of approved customer accounts, approving or not newly registered emails. \r\n\r\nAPI access the extension components so the delivery of functions and information is more flexible. Through API, businesses can update workflows to make them quicker and more productive.\r\n\r\n\r\n## 5. More Features\r\n\r\n### Select customer groups\r\n\r\nSet approval for accounts of specific customer groups.\r\n\r\n### Notification label\r\n\r\nThe content of notice message can be customized easily.\r\n\r\n### Email template\r\n\r\nTemplates for emails to admins and customers are supported. \r\n\r\n### Command line\r\n\r\nQuickly approve/ disapprove via command lines. \r\n\r\n### Compatible with SMTP\r\n\r\nPrevent emails to spam box with [Mageplaza SMTP extension](https://www.mageplaza.com/magento-2-smtp/)\r\n\r\n### Mobile friendly \r\n\r\nProperly display on both PC and mobile devices \r\n\r\n\r\n## 6. Full Features List\r\n\r\n### For store admins\r\n\r\n- Enable/ Disable the module \r\n- Set account approval to be automatic or manual\r\n- Set notification after an account is registered \r\n- Show error notice when an account is not approved \r\n- Redirected to another page when an account is not approved \r\n- Enable sending emails to admins when customers register new accounts\r\n- Select the sender and input recipients of admin notification emails\r\n- Select email templates for admin notification emails \r\n- Enable sending emails to customers when they register successfully\r\n- Enable sending emails to customers when their accounts are approved \r\n- Enable sending emails to customers when their accounts are not approved \r\n\r\n### For customers\r\n\r\n- Be informed after registering new accounts \r\n- Be informed when the accounts are approved \r\n- Be informed when the accounts are not approved \r\n\r\n## 7. User Guide\r\n\r\n### 7.1. Configuration\r\n\r\nLogin to the **Admin Magento**, choose `Stores> Configuration> Customer Approval`.\r\n\r\n![](https://i.imgur.com/bEYNaih.gif)\r\n\r\n\r\n#### 7.1.1. General\r\n\r\n![](https://i.imgur.com/1IQ8Tle.png)\r\n\r\n- **Enable**: Select `Yes` to turn on the Module and use **Approve Customer Account** function\r\n\r\n- **Auto Approve**: If selecting `Yes`, it will automatically Approve when the customer registers in Frontend.\r\n\r\n- **After-registration Notification**:\r\n  - Enter a notification when the account is successfully registered.\r\n  - If left blank, the default is \"Your account requires approval\".\r\n\r\n- **Not Approve Customer Login**: Select the **Not Approve Customer Login** notification type and still login:\r\n  - **Show Error**: Will display the **Not Approve Customer Login** error message. Displaying additional **Error Message** field.\r\n    - **Error Message**: Enter the notification when the client account is not accepted or still has not been approved but try to log in. If left blank, the default is \"Your account is not approved\".\r\n    \r\n    ![](https://i.imgur.com/GxJrvZR.png)\r\n    \r\n  - **Redirect CMS Page**: Select to redirect to the **Not Approve Customer Login page**:\r\n  \r\n  ![](https://i.imgur.com/9RDDRv6.png)\r\n  \r\n    - **For Not Approve Customer Page**\r\n    \r\n    ![](https://i.imgur.com/D40ZRq7.png)\r\n\r\n#### 7.1.2. Admin Notification Email\r\n\r\n![](https://i.imgur.com/DMI6oQF.png)\r\n\r\n- **Enable**:\r\n  - Select \"Yes\" to turn on email notification for admin when a customer successfully registers an account.\r\n  - Install [Mageplaza_SMTP](https://www.mageplaza.com/magento-2-smtp/) to avoid sending to spam box.\r\n\r\n- **Sender**: Select the person to send email to notify admin:\r\n\r\n![](https://i.imgur.com/yJ5ygIX.png)\r\n\r\n- **Email Template**: Select an email template to notify admin when the customer successfully registered an account. You can go to `Marketing> Email Templates`, select **Add New Template** to choose to create a notification email template.\r\n\r\n- **Recipient(s)**:\r\n\u00a0 - Enter the email who receives the notification when the customer registers the account.\r\n  - You can enter multiple email recipients at the same time and they must be separated by commas.\r\n\r\n#### 7.1.3. Customer Notification Email\r\n\r\n![](https://i.imgur.com/CIJx0vf.png)\r\n\r\n- **Sender**: Select the person who sent the email to notify the customer.\r\n\r\n![](https://i.imgur.com/yJ5ygIX.png)\r\n\r\n##### 7.1.3.1. Successful Register\r\n\r\n- **Enable**: Select \"Yes\" to enable email notification for customers when Successful Register.\r\n\r\n- **Email Template**: Choose an email template to notify customers of successful account registration. You can go to `Marketing> Email Templates`, select **Add New Template** to choose to create a notification email template.\r\n \r\n##### 7.1.3.2. Approve Notification\r\n\r\n- **Enable**: Select \"Yes\" to enable email notification to customers when approved with a registered account.\r\n\r\n- **Email Template**: Choose an email template to notify customers when approved with a registered account. You can go to `Marketing> Email Templates`, select **Add New Template** to choose to create a notification email template.\r\n\r\n##### 7.1.3.3. Not Approve Notification\r\n\r\n- **Enable**: Select \"Yes\" to turn on email notifications for customers when not approved with the registered account.\r\n\r\n- **Email Template**: Select an email template to notify customers when not approved with a registered account. You can go to `Marketing> Email Templates`, select **Add New Template** to choose to create a notification email template.\r\n\r\n### 7.2. Customers\r\n\r\n#### 7.2.1. Grid\r\n- Login to the **Magento Admin**, choose `Customers> All Customers`.\r\n- This section lists the information of the registered customer with fields such as **Name, Email, Group, Approval Status, Date of Birth, etc.** Here you can manually approve or not approve at **Action and edit** with any customer you want.\r\n\r\n![](https://i.imgur.com/i0c4Q9M.png)\r\n\r\n#### 7.2.2. Edit Customer\r\n\r\n- Click on `Edit` to edit or approve/not approve any client. With **Approval Status** shows the status of customer account registration.\r\n\r\n\r\n![](https://i.imgur.com/tViNc7w.png)\r\n\r\n## 8. Using API\r\n\r\n- You can use the API integrated with Magento to view the Approved Customers, Waiting for Approval and Not Approval Customers when they sign up for an account.\r\n- Here, we use Postman to support this. You can register Postman [here](https://www.getpostman.com/). Also, you can use other apps to support approval and not approval.\r\n\r\n### 8.1. Integration with Magento:\r\n\r\n#### Step 1: Login to the **Magento Admin**, choose `System> Extensions> Integrations> Add New Integrations` to create new integration.\r\n\r\n\r\n**Note**: For the API tab you should select **Customers** and **Mageplaza Customer Approval**.\r\n\r\n    \r\n![](https://i.imgur.com/Kwo7RJv.png)\r\n    \r\n![](https://i.imgur.com/jK2IONR.png)\r\n\r\n\r\n\r\n#### Step 2: After creating the Integration, please select Activate\r\n\r\n![](https://i.imgur.com/nJ2bsr3.png)\r\n\r\n\r\n#### Step 3: Click `Allow` to get the information of the **Access Token** field.\r\n\r\n![](https://i.imgur.com/CknBYeA.png)\r\n\r\n\r\n\r\n### 8.2. Guide for using Postman to get customers list of waiting for approval, approved and not approved accounts. \r\n\r\n\r\n#### 8.2.1 To list the approved customers, you can use the GET method:\r\n- For example:\r\n  - Url: http://example.com//rest/V1/customer/id\r\n  - For example: http://example.com/rest/V1/customers/1\r\n  - With Key and Value: Get the information of the **Access Token** field that you have just integrated to fill it out below. For example:  Authorization: bearer access_token v\u00e0 Content-Type: application/json\r\n  - Click Send to get the list of approved customers.\r\n\r\n![](https://i.imgur.com/OnFGBBu.png)\r\n\r\n\r\n#### 8.2.2 Approve with customers who have registered an account are in the status of Pending or Not Approval, you can use POST method.\r\n\r\n- `Note`: At the **Body** part, fill in the email you want to approve. As for the **Header** section, fill the same as above with the GET method.\r\n\r\n- Example: Url: http://example.com/rest/V1/customer/approve/email\r\n\r\n\r\n![](https://i.imgur.com/P0NHkTd.png)\r\n\r\n\r\n\r\n#### 8.2.3 Not Approve with customers who have registered an account are in Pending or Approval status, you can use POST method.\r\n\r\n- Example: Url: http://example.com/rest/V1/customer/not-approve/email\r\n\r\n\r\n![](https://i.imgur.com/W7jIVES.png)\r\n\r\n## 9. Instructions to run the command to Approve or Not Approve customer accounts\r\n\r\n- **Approve**: You want approval when the registered account is in pending status or not approval, please run the following command:\r\n\r\n```\r\nphp bin/magento customer:approve \"email customer\"\r\n```\r\n- Example: `php bin/magento customer:approve email\"mageplaza@gmail.com\"`\r\n\r\n\r\n- **Not Approve**:\r\n\r\n```\r\nphp bin/magento customer:notapprove\"email customer\"\r\n```\r\n\r\n- Example: ` php bin/magento customer:notapprove\"mageplaza@gmail.com\"`\r\n\r\n\r\n\r\n## Note\r\n\r\nWhen installing, you should run the following command to update customer grid:\r\n\r\n```\r\n\u00a0 php bin / magento indexer: reindex customer_grid\r\n  ```\r\n  \r\nWhen you want to remove the extension, you should go to the database to delete. Access to `eav_attribute` table, in the `attribute_code` column, you find and delete the `is_approved` attribute\r\n\r\n ![](https://i.imgur.com/aiFNWrY.png)\r\n\r\n\r\n## Explore more Magento 2 extensions from Mageplaza\r\n\u261e [Mageplaza Login as Customer](https://www.mageplaza.com/magento-2-login-as-customer/)\r\n\r\n\u261e [Magento 2 Image Optimizer](https://github.com/mageplaza/magento-2-image-optimizer)\r\n\r\n\u261e [Magento 2 customer attributes](https://www.mageplaza.com/magento-2-customer-attributes/)\r\n\r\n\u261e [Mageplaza Layered Navitaion Ultimate](https://www.mageplaza.com/magento-2-layered-navigation-extension/)\r\n\r\n\u261e [Magento 2 Google Tag Manager Enhanced eCommerce](https://www.mageplaza.com/magento-2-google-tag-manager/)\r\n\r\n\u261e [Mageplaza Ajax Cart](https://www.mageplaza.com/magento-2-quick-view/)\r\n\r\n\u261e [Mageplaza Store Pickup](https://www.mageplaza.com/magento-2-store-pickup-extension/)\r\n\r\n\u261e [Mageplaza Product Options](https://www.mageplaza.com/magento-2-product-options/)\r\n\r\n\u261e [Mageplaza SMS Notification](https://www.mageplaza.com/magento-2-sms-notification/)\r\n\r\n\u261e [Mageplaza Follow Up Email](https://www.mageplaza.com/magento-2-follow-up-email/)\r\n"}, {"repo": "google/customer-match-upload-script", "language": "Python", "readme_contents": "DISCLAIMER: This is not an officially supported Google product.\n\n# Customer Match upload script\n\n## About\n\nThis tool uploads Customer Match lists to Google Ads via API.\n\n## Usage\n\n### Google Ads setup\n\nBefore running the script, you should create a remarketing list in Google Ads.\nIt is recommended to set the membership duration to the N+3, where N is the\nnumber of days between script runs. This way, the list will be maintained with\nonly those users that are in the audience file. If a user is removed from the\nfile, it won't be added in the next script run and it will be eventually\ndeleted. Matching process may take time; the *+3* is a safeguard period to avoid\nunwanted temporary removals during this processing time.\n\n### Set-up\n\nFirst, you must obtain the appropriate credentials so that the script can\nconnect to Google Ads API and upload the data. Follow the instructions\n[here](https://developers.google.com/google-ads/api/docs/oauth/cloud-project#create_a_client_id_and_client_secret)\nto obtain OAuth 2.0 Client IDs for an *Installed App*. Then, download the\nclient secrets json file to the working directory (this file contains a\n*Client ID* and *Client secret*).\n\nOnce you have the file, you must obtain a *Refresh token* using this script:\n\n```shell\npython generate_refresh_token.py --client_secrets_path CLIENT_SECRETS_FILE\n```\n\nWhere `CLIENT_SECRETS_FILE` must be substituted with the client secrets file\npath you downloaded previously.\n\nThe script will generate a *Refresh token* and print it on screen. Make a copy\nof it as we'll need it later.\n\nNext, get a *Developer token* from Google Ads by following the instructions\n[here](https://developers.google.com/google-ads/api/docs/first-call/dev-token).\n\nOnce you have the *Client ID*, *Client secret*, *Refresh token* and *Developer\ntoken*, you must add them to the script config file. In order to do so, edit the\nfile `googleads_config.yaml` and enter the appropriate values. You'll also need\nthe Google Ads *MCC Customer ID*, which you can find in Google Ads UI.\n\n### Input preparation\n\nYour audience file should be named `audience.csv` and must be in CSV format. You\nmust specify the fields to upload in the header row. The available fields are:\n\n- Email\n- Phone\n- FirstName\n- LastName\n- CountryCode\n- ZipCode\n- MobileId\n- UserId\n- List\n\nYou can use plain text values and the script will hash them before uploading, or\nyou can hash them yourself (the desired behaviour is controlled with the\n`--hash_required` flag in the script).\n\nCurrently there are three types of customer lists available:\n\n- *Customers based on email, phone, and/or mailing address uploads* (`CONTACT_INFO`)\n- *Customers based on Mobile Device ID uploads* (`MOBILE_ADVERTISING_ID`)\n- *Customers based on User ID uploads* (`CRM_ID`)\n\nThe type of list to upload is controlled using the `list_type`flag\n(the default value is `CONTACT_INFO`)\n\nEach type of list only allows certain fields to be uploaded. You need to take\nthat into account while preparing the data. The script will filter those\nfields depending on the `list_type` specified, but if the lists already exists\nand doesn't match the type specified, it will show an error in the upload\noperation.\n\nThese are the fields allowed per type:\n\n- `CONTACT_INFO`:\n  - Email\n  - Phone\n  - FirstName\n  - LastName\n  - CountryCode\n  - ZipCode\n  - List\n- `MOBILE_ADVERTISING_ID`:\n  - MobileId\n  - List\n- `CRM_ID`:\n  - UserId\n  - List\n\n### Running the script\n\nOnce the configuration file is ready, and you have the audience file in\n`audience.csv` you can execute the script to upload the data to Google Ads:\n\n```shell\npython create_and_populate_list.py --customer_id CUSTOMER_ID\n```\n\nWhere `CUSTOMER_ID` represents the Customer ID of the account where the user\nlist will be created.\n\nIf your audience file doesn't contain a column for the audience name, you can\nspecify a default audience to which all entries will be added. Here,\n`YOUR_AUDIENCE_NAME` will typically be the name of the audience you manually\ncreated in Google Ads for the purposes of this script:\n\n```shell\npython create_and_populate_list.py --customer_id CUSTOMER_ID --audience_name YOUR_AUDIENCE_NAME\n```\n\nIf you don't specify any remarketing list in the audience file, the script will\ncreate a new audience list with a default name (controlled by the `GENERIC_LIST`\nvariable in the script) and will add all the users to that one.\n\nYou can also specify the list type by using the `--list_type` flag, and passing\none of the allowed values: `CONTACT_INFO`, `MOBILE_ADVERTISING_ID`,`CRM_ID`.\nThe default if not specified is `CONTACT_INFO`.\n\nThe data is not hashed by default. If you need the script to hash the customer\ndata, you can use the `--hash_required` flag to enable it.\n\nThe script has more optional parameters that allow working with custom\nconfiguration and audience file paths. For more info on them, run:\n\n```shell\npython create_and_populate_list.py --help\n```\n\n### Checking the results\n\nBy default, the script launches the upload jobs, and returns. You can use the\n`--wait` flag to wait for the job to finish, but as the upload jobs can take up\nto 48 hours to finish, waiting is not recommended.\n\nIf you don't specify the wait flag, the script will print the job ids in the\nstandard output, and the command you can use to check the job status.\n\nThis is an output example:\n\n```txt\nOffline user data job ID '9999999' with type 'CUSTOMER_MATCH_USER_LIST' has status: RUNNING\nTo check the status of the job periodically, use the following GAQL query with GoogleAdsService.Search:\n        SELECT\n          offline_user_data_job.resource_name,\n          offline_user_data_job.id,\n          offline_user_data_job.status,\n          offline_user_data_job.type,\n          offline_user_data_job.failure_reason\n        FROM offline_user_data_job\n        WHERE offline_user_data_job.resource_name =\n          'customers/0000000000/offlineUserDataJobs/9999999'\n        LIMIT 1\nOr you can use the check_job.py script with the following args:\n\npython check_job.py --config_file ./csemcc_config.yaml --customer_id 0000000000 --job_resource_name customers/0000000000/offlineUserDataJobs/9999999 --user_list_resource_name customers/0000000000/userLists/888888888\n```\n"}, {"repo": "quafzi/magento-CustomerGridOrderCount", "language": "PHP", "readme_contents": "This extension adds a column \"order count\" to your Magento customer grid.\n\nThe column will be added using event/observer pattern,\nsee http://www.webguys.de/magento/turchen-23-pimp-my-produktgrid/, which is a highly compatible solution\nIt supports csv export for this column thanks to luemic_.\n\n.. _luemic: https://github.com/luemic\n"}, {"repo": "AatiqUrRehman/customer-support-flutter-ui", "language": "Objective-C", "readme_contents": "# Customer Support UI design developed in Flutter By Attiuqe Ur Rehman\n\nScreenshorts\nLanding Page\n![alt text](https://github.com/AatiqUrRehman/customer-support-flutter-ui/blob/master/assets/images/Screen%20Shot%202019-03-30%20at%201.21.31%20AM.png)\nDetail Page\n![alt text](https://github.com/AatiqUrRehman/customer-support-flutter-ui/blob/master/assets/images/Screen%20Shot%202019-03-30%20at%201.21.58%20AM.png)\n\nA few resources to get you started if this is your first Flutter project:\n\n- [Lab: Write your first Flutter app](https://flutter.io/docs/get-started/codelab)\n- [Cookbook: Useful Flutter samples](https://flutter.io/docs/cookbook)\n\nany kind of suggestion, code improvement will be really appreciated.\n"}, {"repo": "eventuate-examples/eventuate-tram-core-dotnet-examples-customers-and-orders", "language": "C#", "readme_contents": "= Eventuate Tram Customers and Orders - .NET version\n\nThis is the .NET version of the https://github.com/eventuate-tram/eventuate-tram-examples-customers-and-orders/[Eventuate Tram Customers and Orders example application].\n\nimage::https://github.com/eventuate-tram/eventuate-tram-examples-customers-and-orders/raw/master/images/Eventuate_Tram_Customer_and_Order_Architecture.png[]\n\nIt illustrates how to build .NET microservices using the https://github.com/eventuate-tram/eventuate-tram-core-dotnet[.NET version of Eventuate Tram framework].\nThis example application illustrates how to implement the following key patterns:\n\n* https://microservices.io/patterns/data/saga.html[Choreography-based sagas]\n* https://microservices.io/patterns/data/cqrs.html[CQRS view using MongoDB] (WIP)\n"}, {"repo": "nexmo-community/nexmo-customer-service-chat-demo", "language": "Ruby", "readme_contents": "# Customer Service Chat in Rails 5 with ActionCable and Nexmo\n\nA demo that implements a customer service two way chat between a web app and customers on mobile, using Rails 5's ActionCable and the [Nexmo Text Messages API](https://www.nexmo.com/products/sms/).\n\n![Nexmo Customer Service App](docs/nexmo.gif)\n\n## The chat UI\n\nThe [before](../../tree/before) branch holds the starting point of this tutorial and is a basic Rails 5 app that lets an admin respond to a message thread. All messages are seed data and no actual integration with Nexmo or ActionCable is in place.\n\n### Usage\n\nPlease ensure Ruby 2.2.2 or higher is installed.\n\n- Clone this repository\n- Run `git checkout before`\n- Run `bundle install`\n- Run `rails db:create db:migrate db:seed`\n- Run `bundle exec rails server`\n\nNow open a browser to [localhost:3000](http://localhost:3000) and you will be presented with the bare basic UI.\n\n## ActionCable and Nexmo communication\n\n\nThe [after](../../tree/after) branch is the end point of our tutorial, allowing you to have two way communication between a phone number and the web app.\n\nThe differences between these two branches can be seen [here](../../compare/before...after) and in our in-dept tutorial.\n\n### Usage\n\nPlease ensure Ruby 2.2.2 or higher is installed.\n\n- Clone this repository\n- Run `git checkout before`\n- Run `bundle install`\n- Run `rails db:create db:migrate db:seed`\n- Copy `.env.example` to `.env` and add your [Nexmo API key and secret](https://dashboard.nexmo.com/settings), and add a Nexmo phone number\n- Run `bundle exec rails server`\n- Visit <http://localhost:3000>\n\nIf you now text your Nexmo phone number you should be able to see the message come in on the Rails app within a few seconds.\n\n### Incoming messages\n\nTo receive incoming messages make sure your local server is available publicly. An easy way to do this is to use [Ngrok](https://ngrok.com/).\n\nThen set up your SMS webhook which is easy to do with the [nexmo-cli](https://github.com/nexmo/nexmo-cli).\n\n```sh\nnexmo link:sms 445555555555 http://<your_id>.ngrok.com/response\n```\n"}, {"repo": "galacticlabs/customer-group-payment-filters", "language": "PHP", "readme_contents": "# Galactic Labs - Customer Group Payment Filters\n\nThis simple module adds a new section to the Customer Groups admin page, allowing you to define payment options that ***should not*** be allowed for the chosen customer group.\n\n***Note: These settings will affect both frontend and backend orders.***\n\n## Installation\n\nRequire the module\n\n```bash\ncomposer require galacticlabs/customer-group-payment-filters\n```\n\nEnable the module\n\n```bash\nphp bin/magento module:enable GalacticLabs_CustomerGroupPaymentFilters\n```\n\nRun setup to install module and set up table(s)\n\n```bash\nphp bin/magento setup:upgrade\n```\n\n## Usage\n\nOnce installed, you can choose which payment methods should be restricted by going to `Stores > Other Settings > Customer groups` and selecting the group you would like to place restrictions on.\n\nThe edit screen has a new section called Disallowed Payment Options. This contains a multiselect list of the payment options in the Magento system. Choose the ones you'd like to disable for the customer group, then save for changes to take effect. These options are also available when creating a new customer group.\n\n***Note: These options will not affect anything if the chosen payment options are themselves not enabled.***\n\n![Edit Page Screenshot](docs/edit-page-screenshot.png)\n\n## Bugs/Feature Requests & Contribution\n\nPlease do open a pull request on GitHub should you want to contribute, or create an issue.\n\n## Todo\n\nI plan on cleaning this up a bit and also adding some tests to prove functional correctness. I'd also be interested in discussions on how I could have achieve the same results using alternative/better techniques. I don't like the fact i'm accessing the request object inside the repository save method. Is it possible to perform this action in the admin controller instead? Unfortunately I hit an issue when saving a new customer group in that I couldn't access the ID of the new group in the same request. I'm sure with more time I would of figured it but this was created very quickly.\n\n* Caching should be added\n\n## License\n[MIT](https://opensource.org/licenses/MIT) - Do as you wish \ud83d\udc4d"}, {"repo": "eventuate-tram-examples/eventuate-tram-sagas-micronaut-examples-customers-and-orders", "language": "Java", "readme_contents": "\n= Eventuate Tram Sagas Customers and Orders - for Micronaut\n\nThis application  demonstrates how to maintain data consistency in an Java/JDBC/JPA-based  microservice architecture using http://microservices.io/patterns/data/saga.html[sagas].\n\nThe application consists of two services:\n\n* `Order Service` - creates orders\n* `Customer Service` - manages customers\n\nBoth services are implemented using https://micronaut.io[Micronaut], JPA and the https://github.com/eventuate-tram/eventuate-tram-sagas[Eventuate Tram Saga framework]\n\nThe `Order Service` uses a saga to enforce the customer's credit limit when creating orders.\n\n== About sagas\n\nhttp://microservices.io/patterns/data/saga.html[Sagas] are a mechanism for maintaining data consistency in a http://microservices.io/patterns/microservices.html[microservice architecture].\nA saga is a sequence of transactions, each of which is local to a service.\n\nTo learn more about why you need sagas if you are using microservices:\n\n* the http://microservices.io/patterns/data/saga.html[Saga pattern]\n* Look at https://chrisrichardson.net/post/antipatterns/2019/07/09/developing-sagas-part-1.html[Articles and presentation on sagas]\n* read about sagas in my https://microservices.io/book[Microservice patterns book]\n\nThere are two main ways to coordinate sagas: orchestration and choreography.\nPlease see https://github.com/eventuate-tram-examples/eventuate-tram-examples-micronaut-customers-and-orders[Choreograpy version of the Customers and Orders example] to learn about choreography-based sagas.\nThis example uses orchestration-based sagas, where a saga (orchestration) object invokes the participants.\n\nA saga orchestrator is a persistent object that does one of two things:\n\n1. On startup, it sends a command message to a participant\n2. When it receives a reply, it updates its state and sends a command message to the next participant.\n\n\n== The Create Order Saga\n\nThe following diagrams shows how the saga for creating an `Order` works:\n\nimage::./images/Orchestration_flow.jpeg[]\n\nIt consists of the follow steps:\n\n. The `Order Service` creates an `Order` in a pending state\n. The `Order Service` creates a `CreateOrderSaga` to coordinate the creation of the order.\n. The `CreateOrderSaga` sends a `ReserveCredit` command to the `CustomerService`\n. The `Customer Service` receives the command and attempts to reserve credit for that `Order`. It replies with a message indicating the outcome.\n. The `CreateOrderSaga` receives the reply\n. It send either an `ApproveOrder` or a `RejectOrder` command to the `OrderService`\n. The `Order Service` receives the command and changes state of the order to either `approved` or `rejected`.\n\n=== Architecture\n\nThe following diagram shows the architecture of the Customers and Orders application.\n\nimage::./images/Eventuate_Tram_Customer_and_Order_Orchestration_Architecture.png[]\n\nThe application consists of two services:\n\n* `Customer Service` - implements the REST endpoints for managing customers.\nThe service persists the `Customer` JPA entity in a MySQL/Postgres database.\nUsing the https://github.com/eventuate-tram/eventuate-tram-sagas[Eventuate Tram Saga framework], it processes command messages, updates its the `Customer` entity, and sends back a reply message.\n\n* `Order Service` - implements a REST endpoint for managing orders.\nThe service persists the `Order` JPA entity and the `CreateOrderSaga` in MySQL/Postgres database.\nUsing the https://github.com/eventuate-tram/eventuate-tram-sagas[Eventuate Tram Saga framework], it sends command messages and processes replies.\n\nThe Eventuate Tram CDC service tracks inserts into the `MESSAGE` table using the MySQL binlog and publishes messages to Apache Kafka.\n\n== Building and running\n\nNote: you do not need to install Gradle since it will be downloaded automatically.\nYou just need to have Java 8 installed.\n\nFirst, build the application\n\n```\n./gradlew assemble\n```\n\nNext, launch the services using https://docs.docker.com/compose/[Docker Compose]:\n\n```\n./gradlew mysqlComposeBuild\n./gradlew mysqlComposeUp\n```\n\nNote:\n\n1. You can also run the Postgres version using `./gradlew postgresComposeBuild` and `./gradlew postgresComposeUp`\n2. If the containers aren't accessible via `localhost` - e.g. you are using Docker Toolbox, you will have to use `localhost` instead of localhost.\nSee this http://eventuate.io/docs/usingdocker.html[guide to setting `DOCKER_HOST_IP`] for more information.\n\n== Using the application\n\nOnce the application has started, you can use the application via the Swagger UI:\n\n* `Order Service` - `http://localhost:8081/swagger-ui/index.html`\n* `Customer Service` - `http://localhost:8082/swagger-ui/index.html`\n\nYou can also use `curl` to interact with the services.\nFirst, let's create a customer:\n\n```bash\n$ curl -X POST --header \"Content-Type: application/json\" -d '{\n  \"creditLimit\": {\n    \"amount\": 5\n  },\n  \"name\": \"Jane Doe\"\n}' http://localhost:8082/customers\n\nHTTP/1.1 200\nContent-Type: application/json;charset=UTF-8\n\n{\n  \"customerId\": 1\n}\n```\n\nNext, create an order:\n\n```bash\n$ curl -X POST --header \"Content-Type: application/json\" -d '{\n  \"customerId\": 1,\n  \"orderTotal\": {\n    \"amount\": 4\n  }\n}' http://localhost:8081/orders\n\nHTTP/1.1 200\nContent-Type: application/json;charset=UTF-8\n\n{\n  \"orderId\": 1\n}\n\n```\n\nFinally, check the status of the `Order`:\n\n```bash\n$ curl -X GET http://localhost:8081/orders/1\n\nHTTP/1.1 200\nContent-Type: application/json;charset=UTF-8\n\n{\n  \"orderId\": 1,\n  \"orderState\": \"APPROVED\"\n}\n```\n\n== Got questions?\n\nDon't hesitate to create an issue or see\n\n* https://groups.google.com/d/forum/eventuate-users[Mailing list]\n* https://join.slack.com/t/eventuate-users/shared_invite/enQtNTM4NjE0OTMzMDQ3LTc3ZjYzYjYxOGViNTdjMThkZmVmNWQzZWMwZmQyYzhjNjQ4OTE4YzJiYTE2NDdlOTljMDFlMDlkYTI2OWU1NTk[Slack]\n* http://eventuate.io/contact.html[Contact us].\n"}, {"repo": "manishjoy/customer-ajax-login", "language": "PHP", "readme_contents": "# ManishJoy_CustomerLogin v1.0.0\n## Magento 2 Extension\n\n### Free magento 2 extension to login & signup using AJAX\n\n### Magento Doesn't have Ajax supported Login and Sign Up out of the box, this extension enables this feature\n\n## Features:\n1. Easy to use\n2. Looks like core functionality of Magento (at backend as well as frontend)\n3. Unencrypted code for easy customization\n\n___________________________________________________________________________________________________\n\n### Screenshots:\n\n--1--\n\n<img src=\"https://image.ibb.co/j600gm/Screenshot_from_2017_12_05_22_34_34.png\" alt=\"Screenshot 1\" title=\"Screenshot 1\">\n\n--2--\n\n<img src=\"https://image.ibb.co/f6t31m/Screenshot_from_2017_12_05_22_34_43.png\" alt=\"Adding-SS\" title=\"Screenshot 1\">\n\n___________________________________________________________________________________________________\n\n### The extension is free and always will be\n\n## Additional Charges:\n#### Installation: $ 20\n#### Support: $ 40 /6 months\n\n___________________________________________________________________________________________________\n## Liked my work?\n\n<a href=\"https://www.paypal.me/manishjoy\" rel=\"nofollow\"><img height=\"36\" src=\"https://manishjoy.github.io/img/coffee-btn-image.png\" border=\"0\" alt=\"Buy Me a Coffee\" data-canonical-src=\"https://manishjoy.github.io/img/coffee-btn-image.png\" style=\"max-width:100%;\"></a>\n\n--- OR ---\n\n<a href='https://www.patreon.com/manishjoy' target='_blank'><img src='https://i.ibb.co/rHdTFtj/patreon-btn.jpg' width='200' border='0' alt='SUPPORT ME ON PATREON' /></a>\n\n___________________________________________________________________________________________________\n## Prerequisites\n\n### Use the following table to verify you have the correct prerequisites to install this Extension.\n<table>\n\t<tbody>\n\t\t<tr>\n\t\t\t<th>Prerequisite</th>\n\t\t\t<th>How to check</th>\n\t\t\t<th>For more information</th>\n\t\t</tr>\n\t<tr>\n\t\t<td>Apache 2.2 or 2.4</td>\n\t\t<td>Ubuntu: <code>apache2 -v</code><br>\n\t\tCentOS: <code>httpd -v</code></td>\n\t\t<td><a href=\"http://devdocs.magento.com/guides/v2.0/install-gde/prereq/apache.html\">Apache</a></td>\n\t</tr>\n\t<tr>\n\t\t<td>PHP 5.6.x, 7.0.2, 7.0.4 or 7.0.6</td>\n\t\t<td><code>php -v</code></td>\n\t\t<td><a href=\"http://devdocs.magento.com/guides/v2.0/install-gde/prereq/php-ubuntu.html\">PHP Ubuntu</a><br><a href=\"http://devdocs.magento.com/guides/v2.0/install-gde/prereq/php-centos.html\">PHP CentOS</a></td>\n\t</tr>\n\t<tr><td>MySQL 5.6.x</td>\n\t<td><code>mysql -u [root user name] -p</code></td>\n\t<td><a href=\"http://devdocs.magento.com/guides/v2.0/install-gde/prereq/mysql.html\">MySQL</a></td>\n\t</tr>\n</tbody>\n</table>\n\n___________________________________________________________________________________________________\n### Feedback and Support <a href=\"mailto:manishjoy1993@hotmail.com\">manishjoy1993@hotmail.com</a>\n"}, {"repo": "smaex/customer-group-payments", "language": "PHP", "readme_contents": "# Magento 2: Customer Group Payments\n\nRestricts payment methods to specific customer groups in [Magento 2][1].\n\n## Intro\n\nAdding additional checks to payment methods \u2013 to decide if a certain payment method is applicable to a certain customer or not \u2013 is pretty easy and straight forward in Magento 2.\n\nMagento [provides a rather simplistic interface][2] for custom payment method checks, and [uses a composite check][3] to process these individual checks. Adding a custom check is therefore just a matter of injecting it into Magento\u2019s composite check via dependency injection.\n\nThis extension implements such a custom check to decide if a certain payment method is applicable to a customer based on the customer group, along with a corresponding system configuration field for payment methods.\n\n![Screenshot: Magento system configuration field for payment methods that restricts a payment method to selected customer groups.][4]\n\n## Prerequisite\n\nUnfortunately, there\u2019s currently a limitation in Magento\u2019s code regarding this mechanism that requires us to use a small workaround, so this extension depends on another extension providing this workaround. See [smaex/additional-payment-checks][5] for more information.\n\n## How to install\n\nSimply require the extension via [Composer][6].\n\n```sh\n$ composer require smaex/customer-group-payments ^2.0\n```\n\nFinally, enable the module via [Magento\u2019s CLI][7].\n\n```sh\n$ magento module:enable Smaex_CustomerGroupPayments\n```\n\n## How to use\n\nWhile the extension provides you with all the tools you need to restrict payment methods to specific customer groups, it doesn\u2019t do anything without some custom configuration on your part. That\u2019s because adding system configuration fields [is done via XML configuration][8] in Magento.\n\nTo properly configure your payment methods, the best way going forward is probably to [set up your own custom module][9] (e.g., `Acme_Payment`) under `app/code`.\n\nDeclaring a dependency on `Magento_Payment` and `Smaex_CustomerGroupPayments` in your `module.xml` is kinda mandatory, dependencies on other modules like `Magento_OfflinePayments` depend on the payment methods actually used in your project.\n\n```xml\n<config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"urn:magento:framework:Module/etc/module.xsd\">\n    <module name=\"Acme_Payment\" setup_version=\"1.0.0\">\n        <sequence>\n            <module name=\"Magento_OfflinePayments\"/>\n            <module name=\"Magento_Payment\"/>\n            <module name=\"Smaex_CustomerGroupPayments\"/>\n        </sequence>\n    </module>\n</config>\n```\n\nThe next and already final step is then to provide your own `system.xml` under `etc/adminhtml` in your custom module and extend the existing configuration for each payment method used in your project.\n\n```xml\n<config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"urn:magento:module:Magento_Config:etc/system_file.xsd\">\n    <system>\n        <section id=\"payment\">\n            <!--\n            Check / Money Order\n            -->\n            <group id=\"checkmo\">\n                <field id=\"customer_groups\" type=\"multiselect\" translate=\"label comment\" showInDefault=\"1\" showInWebsite=\"1\" showInStore=\"1\" canRestore=\"1\" sortOrder=\"52\">\n                    <include path=\"Smaex_CustomerGroupPayments::system/customer_groups.xml\"/>\n                </field>\n            </group>\n            <!--\n            Cash On Delivery Payment\n            -->\n            <group id=\"cashondelivery\">\n                <field id=\"customer_groups\" type=\"multiselect\" translate=\"label comment\" showInDefault=\"1\" showInWebsite=\"1\" showInStore=\"1\" canRestore=\"1\" sortOrder=\"52\">\n                    <include path=\"Smaex_CustomerGroupPayments::system/customer_groups.xml\"/>\n                </field>\n            </group>\n        </section>\n    </system>\n</config>\n```\n\n## Alternative\n\nIf ~~you\u2019re a lazy sloth~~ this looks like too much work (i.e., XML configuration), there\u2019s also [another extension tackling the same problem from a very different angle][10].\n\n## We\u2019re hiring!\n\nWanna work for [one of Germany\u2019s leading Magento partners][11]? With agile methods, small teams and big clients? We\u2019re currently looking for experienced ~~masochists~~ **PHP & Magento developers in Munich**. Sounds interesting? Just drop me a line via j.scherbl@techdivision.com\n\n [1]: https://github.com/magento/magento2\n [2]: https://github.com/magento/magento2/blob/2.3/app/code/Magento/Payment/Model/Checks/SpecificationInterface.php\n [3]: https://github.com/magento/magento2/blob/2.3/app/code/Magento/Payment/Model/Checks/Composite.php\n [4]: https://user-images.githubusercontent.com/1640033/47964356-25660200-e039-11e8-81dc-7ccc9785c2bc.png\n [5]: https://github.com/smaex/additional-payment-checks\n [6]: https://getcomposer.org\n [7]: https://devdocs.magento.com/guides/v2.3/install-gde/install/cli/install-cli-subcommands-enable.html\n [8]: https://github.com/magento/magento2-samples/blob/master/sample-module-payment-gateway/etc/adminhtml/system.xml\n [9]: https://devdocs.magento.com/guides/v2.3/architecture/archi_perspectives/components/modules/mod_intro.html\n[10]: https://github.com/galacticlabs/customer-group-payment-filters\n[11]: https://www.techdivision.com/karriere/offene-stellen/magento-developer-m-w.html\n"}, {"repo": "Vinai/customer-registration-ip", "language": "PHP", "readme_contents": "# This Magento 1 extension is orphaned, unsupported and no longer maintained.\n\nIf you use it, you are effectively adopting the code for your own project.\n\nCustomer Registration IP\n========================\nMagento module to display a customer's IP used during registration.\n\nFacts\n-----\n- version: check the [config.xml](https://github.com/Vinai/customer-registration-ip/blob/master/app/code/community/Netzarbeiter/CustomerRegIp/etc/config.xml)\n- extension key: Netzarbeiter_CustomerRegIp\n- [extension on Magento Connect](http://www.magentocommerce.com/magento-connect/customer-registration-ip.html)\n- Magento Connect 1.0 extension key: magento-community/Netzarbeiter_CustomerRegIp\n- Magento Connect 2.0 extension key: http://connect20.magentocommerce.com/community/Netzarbeiter_CustomerRegIp\n- [extension on GitHub](https://github.com/Vinai/customer-registration-ip)\n- [direct download link](https://github.com/Vinai/customer-registration-ip/zipball/master)\n\nDescription\n-----------\nThis small extension saves the remote IP address from where the customer was\nregistered or created, and displays it on the admin customer dashboard.\n\nTo include the customers registration IP in transactional email templates\nwhere the customer object is available you can use {{var customer.getRegistrationRemoteIp()}}\n\nIf you register a (free or commercial) API key for http://ipinfodb.com/ you can\nconfigure it at\nAdmin - System Configuration - Netzarbeiter Extensions - Customer Registration IP\n\nIf a API key is registered a button to query the API will beavailable beside the\ncustomers IP address. Regardless of the API key settings, a link to lookup addresses\nvia http://ip2location.com/ in a new window is always displayed.\n\nCompatibility\n-------------\n- Magento >= 1.4\n\nInstallation Instructions\n-------------------------\n1. Install the extension via Magento Connect with the key shown above or copy all the files into your document root.\n2. Clear the cache, logout from the admin panel and then login again.\n3. Configure and activate the extension under System - Configuration - Netzarbeiter Extensions - Customer Registration IP.\n\nUninstallation\n--------------\nTo uninstall this extension you need to run the following SQL after removing the extension files:\n```sql\n  DELETE FROM eav_attribute WHERE attribute_code = 'registration_remote_ip';\n  DELETE FROM core_resource WHERE code = 'customerregip_setup';\n```\n\nSupport\n-------\nIf you have any issues with this extension, open an issue on GitHub (see URL above)\n\nContribution\n------------\nAny contributions are highly appreciated. The best way to contribute code is to open a\n[pull request on GitHub](https://help.github.com/articles/using-pull-requests).\n\nDeveloper\n---------\nVinai Kopp\n[http://www.netzarbeiter.com](http://www.netzarbeiter.com)\n[@VinaiKopp](https://twitter.com/VinaiKopp)\n\nLicence\n-------\n[OSL - Open Software Licence 3.0](http://opensource.org/licenses/osl-3.0.php)\n\nCopyright\n---------\n(c) 2014 Vinai Kopp\n"}, {"repo": "motephyr/react-redux-socketio-slack-customer-service", "language": "JavaScript", "readme_contents": "# \u7dda\u4e0a\u5ba2\u670d(for slack)\n- \u7528slack\u7b49\u4f60\u5e38\u7528\u7684\u804a\u5929\u65b9\u5f0f\u5373\u6642\u7684\u548c\u4f60\u7684\u8a2a\u5ba2\u7dda\u4e0a\u6e9d\u901a\n- \u4ee5NodeJs,react,socketio\u958b\u767c\n\n# \u4f7f\u7528\u60c5\u5883\n\u7528\u6236\u53ef\u4ee5\u767c\u9001\u8a0a\u606f\u5230slack\n\n\u4e00\u822c\u7528\u6236\u9023\u4e0a\u4e86server\u6703\u5206\u914d\u4e00\u500bid,\u53bb\u8b58\u5225\u5b83\u662f\u9019\u500b\u4eba\u3002\n\n\u7136\u5f8c\u7576\u7528\u6236\u5beb\u4e86\u81ea\u5df2\u7684\u8b58\u5225Email,\u5c31\u53ef\u4ee5\u7d81\u5b9aEmail\u548cid\u4f86\u50b3\u8a0a\u606f\u7d66\u5b83\u3002\n\n#Installation\nnpm install\n\n#Usage Script\n\naction                              |command\n------------------------------------|-------\n\u958bproduction\u7684server                |npm start\n\u958bdevelopment\u7684server               |npm run dev\nbuild production\u7684client (\u7522\u751f\u6a94\u6848)  |npm run build\n\u958bdevelopment\u7684client (\u4e0d\u7522\u751f\u6a94\u6848)   |npm  run build:dev\nserver debug mode                   |npm run debug\n\u780d\u6389client                          |npm run clean\npm2\u4f48\u7f72                             |npm run deploy\npm2\u91cd\u958b                             |npm run restart\n\u6e2c\u8a66                                |npm test\n\u780d\u6389client build production\u7684client \u4e26\u958bdevelopment\u7684server         |./serv pack\n\u780d\u6389client \u958bdevelopment\u7684client \u4e26\u958bdevelopment\u7684server            |./serv pack:dev\n\n# react\u90e8\u5206\u958b\u767c\u7b46\u8a18\n## \u62c6\u5206\u7528\u6237\u754c\u9762\u4e3a\u4e00\u4e2a\u7ec4\u4ef6\u6811\n\n\u5f9eRoot\u51fa\u767c\nMainContainer\nMessageBoxComponent\n  MessageHeader\n  MessageTextarea\n  MessageInput\n\n## \u5229\u7528 React \uff0c\u521b\u5efa\u5e94\u7528\u7684\u4e00\u4e2a\u9759\u6001\u7248\u672c\n\u628ahtml\u505a\u51fa\u4f86\n\u7136\u5f8c\u628arender\u586b\u4e00\u586b\n\n## \u8bc6\u522b\u51fa\u6700\u5c0f\u7684\uff08\u4f46\u662f\u5b8c\u6574\u7684\uff09\u4ee3\u8868 UI \u7684 state\n\u7528\u6237\u8f93\u5165\u7684\uff0c\u6703\u96a8\u6642\u9593\u6539\u8b8a\u7684\n\n## \u786e\u8ba4 state \u7684\u751f\u547d\u5468\u671f\n\u653e\u5728\u6240\u6709\u9700\u8981\u5b83\u7684\u7d44\u4ef6\u7684\u6700\u4e0a\u4e00\u5c64\n\n## \u6dfb\u52a0\u53cd\u5411\u6570\u636e\u6d41\n\u7528redux\u4f86\u8655\u7406\n\n\n\n\n"}, {"repo": "IBM/analyze-customer-data-spark-pixiedust", "language": "Jupyter Notebook", "readme_contents": "# Analyze customer data using Jupyter notebooks, Apache Spark, and PixieDust\n\nIn this code pattern historical shopping data is analyzed with Spark and PixieDust. The data is loaded, cleaned and then analyzed by creating various charts and maps.\n\nWhen you have completed this code patterns, you will understand how to:\n\n* Use [Jupyter Notebooks](http://jupyter.org/) in [IBM Watson Studio](https://dataplatform.ibm.com/)\n* Load data with PixieDust and clean data with Spark\n* Create charts and maps with [PixieDust](https://github.com/pixiedust/pixiedust)\n\nThe intended audience is anyone interested in quickly analyzing data in a Jupyter notebook.\n\n## Flow\n\n![arch](doc/source/images/architecture.png)\n\n1. Log in to IBM Watson Studio\n1. Load the provided notebook into Watson Studio\n1. Load the customer data in the notebook\n1. Transform the data with Apache Spark\n1. Create charts and maps with PixieDust\n\n#### About the data\n\n* [x19_income_select.csv](data/x19_income_select.csv): Household income statistics for many categories of income, including wages, interest, social security, public assistance, and retirement. Compiled at the zip code geography level by the United States Census Bureau. Available as a data set on [Watson Studio](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/beb8c30a3f559e58716d983671b70337)\n* [customers_orders1_opt.csv](data/customers_orders1_opt.csv): Fictitious customer demographics and sales data. Published by IBM. Available as a data set on [Watson Studio](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/f8ccaf607372882403a37d9019b3abf4)\n\n## Included Components\n\n* [IBM Watson Studio](https://cloud.ibm.com/catalog/services/watson-studio): a suite of tools and a collaborative environment for data scientists, developers and domain experts\n* [PixieDust](https://github.com/pixiedust/pixiedust): Open source Python package, providing support for Javascript/Node.js code.\n\n## Steps\n\n1. [Create a project](#1-create-a-project)\n1. [Create a notebook](#2-create-the-notebook)\n1. [Load customer data in the notebook](#3-load-customer-data)\n1. [Transform the data with Apache Spark](#4-transform-data)\n1. [Create charts and maps with PixieDust](#5-create-charts)\n\n### 1. Create a project and add the Spark services\n\n* Log into IBM's [Watson Studio](https://dataplatform.cloud.ibm.com). Once in, you'll land on the dashboard.\n\n* Create a new project by clicking `+ New project` and choosing `Data Science`:\n\n  ![studio project](https://raw.githubusercontent.com/IBM/pattern-utils/master/watson-studio/new-project-data-science.png)\n\n* Enter a name for the project name and click `Create`.\n\n* **NOTE**: By creating a project in Watson Studio a free tier `Object Storage` service and `Watson Machine Learning` service will be created in your IBM Cloud account. Select the `Free` storage type to avoid fees.\n\n  ![studio-new-project](https://raw.githubusercontent.com/IBM/pattern-utils/master/watson-studio/new-project-data-science-name.png)\n\n* Upon a successful project creation, you are taken to a dashboard view of your project. Take note of the `Assets` and `Settings` tabs, we'll be using them to associate our project with any external assets (datasets and notebooks) and any IBM cloud services.\n\n  ![studio-project-dashboard](https://raw.githubusercontent.com/IBM/pattern-utils/master/watson-studio/overview-empty.png)\n\n## 2. Create a notebook\n\n* From the new project `Overview` panel, click `+ Add to project` on the top right and choose the `Notebook` asset type.\n\n  ![studio-project-dashboard](https://raw.githubusercontent.com/IBM/pattern-utils/master/watson-studio/add-assets-notebook.png)\n\n* Fill in the following information:\n\n  * Select the `From URL` tab. [1]\n  * Enter a `Name` for the notebook and optionally a description. [2]\n  * Under `Notebook URL` provide the following url: [https://raw.githubusercontent.com/IBM/analyze-customer-data-spark-pixiedust/master/notebooks/analyze-customer-data.ipynb](https://raw.githubusercontent.com/IBM/analyze-customer-data-spark-pixiedust/master/notebooks/analyze-customer-data.ipynb) [3]\n  * For `Runtime` select the `Spark Python 3.6` option. [4]\n\n  ![add notebook](https://github.com/IBM/pattern-utils/raw/master/watson-studio/notebook-create-url-spark-py36.png)\n\n* Click the `Create` button.\n\n* **TIP:** Once successfully imported, the notebook should appear in the `Notebooks` section of the `Assets` tab.\n\n## 3. Load customer data in the notebook\n\n* Run the cells one at a time. Select the first cell and press the `(\u25ba) Run` button to start stepping through the notebook.\n\n* Load the data set [customers_orders1_opt.csv](data/customers_orders1_opt.csv) into the notebook.\n\n## 4. Transform the data with Apache Spark\n\nBefore analyzing the data, it needs to be cleaned and formatted. This can be done with a few [pyspark](https://spark.apache.org/docs/latest/api/python/index.html) commands:\n\n* Select only the columns you are interested in with `df.select()`\n\n* Convert the AGE column to a numeric data type so you can run calculations on customer age with a user defined function ([udf](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=udf#pyspark.sql.functions.udf)).\n\n* Derive the gender information for each customer based on the salutation and rename the GenderCode column to GENDER with a second `udf`.\n\n## 5. Create charts and maps with PixieDust\n\nThe data can now be explored with PixieDust:\n\n* With `display()` explore the data in a table.\n\n* Then click on the below button to create one of the charts in the list.\n\n![notebook](doc/source/images/display.png)\n\n* Drag and drop the variables you want to display into the `Keys` and `Values` fields. Select the aggregation from the drop-down menu and click `OK`.\n\n* From the menu on the right of the chart you can select which renderer you want to use, where each one of them visualises the data in a different way. Other options are clustering by a variable, the size and orientation of the chart and the display of a legend.\n\n* Below are two examples of a bar chart and a map created in the notebook.\n\n> Histogram\n> ![notebook](doc/source/images/chart_histogram.png)\n\n> Map\n> ![notebook](doc/source/images/chart_map.png)\n\n## Related links\n\n* [Build a recommender with Apache Spark and Elasticsearch](https://developer.ibm.com/code/patterns/build-a-recommender-with-apache-spark-and-elasticsearch/)\n* [Create a web-based mobile health app using Watson services on IBM Cloud and IBM Watson Studio](https://developer.ibm.com/code/patterns/develop-web-based-mobile-health-app-uses-machine-learning/)\n* [Use machine learning to predict U.S. opioid prescribers with Watson Studio and scikit-learn](https://developer.ibm.com/code/patterns/analyze-open-medical-data-sets-to-gain-insights/)\n\n## Learn more\n\n* **Watson Studio**: Master the art of data science with IBM's [Watson Studio](https://datascience.ibm.com/)\n* **Data Analytics Code Patterns**: Enjoyed this Code Pattern? Check out our other [Data Analytics Code Patterns](https://developer.ibm.com/code/technologies/data-science/)\n\n## License\n\nThis code pattern is licensed under the Apache Software License, Version 2.  Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the [Developer Certificate of Origin, Version 1.1 (DCO)](https://developercertificate.org/) and the [Apache Software License, Version 2](http://www.apache.org/licenses/LICENSE-2.0.txt).\n\n[Apache Software License (ASL) FAQ](http://www.apache.org/foundation/license-faq.html#WhatDoesItMEAN)\n"}, {"repo": "microsoft/windows-tutorials-customer-database", "language": "C#", "readme_contents": "---\npage_type: sample\nlanguages:\n- csharp\nproducts:\n- dotnet\n- windows\n- windows-uwp\nstatusNotificationTargets:\n- codefirst@microsoft.com\ndescription: \"This project is the starting point for a tutorial that creates a simple app for managing a list of customers.\"\n---\n\n# Customer Database tutorial\n\nThis project is the starting point for a tutorial that creates a simple app for managing a list of customers. In doing so, it introduces a selection of basic concepts for enterprise apps in UWP. You'll learn how to:\n\n* Implement Create, Read, Update, and Delete operations against a local SQL database.\n* Add a data grid, to display and edit customer data in your UI.\n* Arrange UI elements together in a basic form layout.\n\n[Start the tutorial here.](https://docs.microsoft.com/windows/uwp/enterprise/customer-database-tutorial)\n\nThis starting point is a single-page app with minimal UI and functionality, based on a simplified version of the [Customer Orders Database sample app](https://github.com/Microsoft/Windows-appsample-customers-orders-database).\n\n## Run the sample\n\nTo run this sample, [ensure you have the latest version of Visual Studio and the Windows 10 SDK](https://developer.microsoft.com/windows/downloads/windows-10-sdk). Once you've cloned/downloaded this repo, you can edit the project by opening **CustomerDatabaseTutorial.sln** with Visual Studio.\n\nYou can also check out the **end-point-for-reference** branch to see the completed code for this tutorial.\n\n## Code of Interest\n\nIf you run your app immediately after opening it, you'll see a few buttons at the top of a blank screen. Though it's not visible to you, the app already includes a local SQLite database provisioned with a few test customers. From here, you'll start by implementing a UI control to display those customers, and then move on to adding in operations against the database. Before you begin, here's where you'll be working.\n\n### Views\n\n**CustomerListPage.xaml** is the app's View, which defines the UI for the single page in this tutorial. Any time you need to add or change a visual element in the UI, you'll do it in this file. This tutorial will walk you through adding these elements:\n\n* A **RadDataGrid** for displaying and editing your customers. \n* A **StackPanel** to set the initial values for a new customer.\n\n### ViewModels\n\n**ViewModels\\CustomerListPageViewModel.cs** is where the fundamental logic of the app is located. Every user action taken in the view will be passed into this file for processing. In this tutorial, you'll add some new code, and implement the following methods:\n\n* **CreateNewCustomerAsync**, which initializes a new CustomerViewModel object.\n* **DeleteNewCustomerAsync**, which removes a new customer before it's displayed in the UI.\n* **DeleteAndUpdateAsync**, which handles the delete button's logic.\n* **GetCustomerListAsync**, which retrieves a list of customers from the database.\n* **SaveInitialChangesAsync**, which adds a new customer's information to the database.\n* **UpdateCustomersAsync**, which refreshes the UI to reflect any customers added or deleted.\n\n**CustomerViewModel** is a wrapper for a customer's information, which tracks whether or not it's been recently modified. You won't need to add anything to this class, but some of the code you'll add elsewhere will reference it.\n\nFor more information on how the sample is constructed, check out the [app structure overview](https://docs.microsoft.com/windows/uwp/enterprise/customer-database-app-structure).\n"}, {"repo": "dvdl16/erpnext_customer_statements_sender", "language": "Python", "readme_contents": "## Deprecation Notice\n\nThis app is deprecated as this functionality is now available in ERPNext Version 13\n\nSee [this PR](https://github.com/frappe/erpnext/pull/22901) for more details\n\n\n\n---\n\n\n## ERPNext Customer Statements Sender\n\nThis app allows you to send out statements to your customers in bulk\n\n#### \ud83d\udee0 Please note that this is a Work in Progress \ud83d\udee0 Issues and PR's welcome. This app was designed for Frappe/ERPNext v12.\n\nThe interface is on a Single Doctype called `Customer Statements Sender`:\n\n![](screencast.gif)\n\n### Install\n\n```bash\ncd frappe-bench\nbench get-app https://github.com/dvdl16/erpnext_customer_statements_sender.git\nbench --site site1.local install-app erpnext_customer_statements_sender\n```\n\n(Replace `site1.local` with your site name.)\n\n\n#### License\n\nCopyright (C) 2020  Dirk van der Laarse\n\nMIT\n\n#### \ud83d\udee0 Please note that this is a Work in Progress \ud83d\udee0 Make sure you check the statements beforehand to avoid angry customers. This app was designed for Frappe/ERPNext v12.\n"}, {"repo": "pmgarman/wc-customer-order-index", "language": "PHP", "readme_contents": ""}, {"repo": "jbossdemocentral/brms-customer-evaluation-demo", "language": "Java", "readme_contents": "JBoss BPM Customer Evaluation Demo Quickstart Guide\n===================================================\n\n![Install Console](https://github.com/eschabell/brms-customer-evaluation-demo/blob/master/docs/demo-images/install-console.png?raw=true)\n\n![Process](https://github.com/eschabell/brms-customer-evaluation-demo/blob/master/docs/demo-images/process.png?raw=true)\n\n\n\nSetup and Configuration\n-----------------------\n\nSee Quick Start Guide in project as ODT and PDF for details on installation. For those that can't wait:\n\n- see README in 'installs' directory\n\n- add products \n\n- run 'init.sh' & read output\n\n- read Quick Start Guide\n\n- setup JBDS for project import, add JBoss EAP server\n\n- import projects\n\n- run 'mvn clean install' on project to build\n\n- start JBoss EAP server\n\n- login to BRM (http://localhost:8080/jboss-brms)\n\n- import repository-export from support dir\n\n- build and deploy project in BRM\n\n- login to Business Central (http://localhost:8080/business-central)\n\n- start process, view JBoss EAP logs for results\n\nWindows users see support/windows/README for installation.\n\nDetailed documentation and presentations can be found in docs directory.\n\nSupporting Articles\n-------------------\n\n[Customer Evaluation Demo Updated to EAP 6.1.1] (http://www.schabell.org/2013/09/jboss-brms-customer-eval-demo-updated-eap-611.html)\n\n[Customer Evaluation Demo Updated to EAP 6.1.0] (http://www.schabell.org/2013/05/jboss-brms-customer-eval-demo-eap-610.html)\n\n[Customer Evaluation Demo Updated to EAP 6.1.0.Beta] (http://www.schabell.org/2013/04/red-hat-jboss-brms-customer-evaluation.html)\n\n[Adding Declarative Model to Customer Evaluation Demo] (http://www.schabell.org/2013/03/jboss-brms-customer-eval-demo-declarative-model.html)\n\n[Customer Evaluation Demo Updated to EAP 6.0.0] (http://www.schabell.org/2013/01/jboss-brms-customer-evaluation-demo-update.html)\n\n[BPM made simple with Customer Evaluation Demo including video] (http://www.schabell.org/2012/06/jboss-enterprise-brms-bpm-made-simple.html)\n\n[How to setup SOA Tools in BRMS Example for JBoss Dev Studio 7] (http://www.schabell.org/2013/04/jboss-developer-studio-7-how-to-setup.html)\n\n[How to setup SOA Tools in BRMS Example for JBoss Dev Studio 6] (http://www.schabell.org/2013/04/jboss-developer-studio-6-how-to-setup.html)\n\n[How to setup SOA Tools in BRMS Example for JBoss Dev Studio 5] (http://www.schabell.org/2012/05/jboss-developer-studio-5-how-to-setup.html)\n\n[How to add Eclipse BPMN2 Modeller project to JBoss Dev Studio 5] (http://www.schabell.org/2013/01/jbds-bpmn2-modeler-howto-install.html)\n\n[Demo now available with Windows installation scripts] (http://www.schabell.org/2013/04/jboss-brms-demos-available-windows.html)\n\nBrazilian language translation of demo documentation avaialbe in support/Quick Start Guide (pt-BR).{odt|pdf}\n\n\nReleased versions\n-----------------\n\nSee the tagged releases for the following versions of the product:\n\n- v2.2 restructured by adding in a docs directory.\n\n- v2.1 is BRMS 5.3.1 deployable, JBDS 7, running on JBoss EAP 6.1.1.\n\n- v2.0 is BRMS 5.3.1 deployable, JBDS 7.0.0.Beta1, running on JBoss EAP 6.1.0, includes pt-BR documentation translation.\n\n- v1.9 is BRMS 5.3.1 deployable, running on JBoss EAP 6.1.0.\n\n- v1.8 is BRMS 5.3.1 deployable, running on JBoss EAP 6.1.0.Beta.\n\n- v1.7 demo project Mavenized.\n\n- v1.6 has Windows installation scripts.\n\n- v1.5 has patched designer that fixes removal of end-of-lines in code editor.\n\n- v1.4 is BRMS 5.3.1 deployable, running on JBoss EAP 6, added a declarative model example\n\t(support/repository_export_declarative_model.zip)\n\n- v1.3 is BRMS 5.3.1 deployable, running on JBoss EAP 6, cleaner logging, serialized object model fixes.\n\n- v1.2 is BRMS 5.3.1 deployable, running on JBoss EAP 6.\n\n- v1.0 is BRMS 5.3.0 standalone, running on JBoss EAP 5.\n\n"}, {"repo": "DaniSanchezSantolaya/RNN-customer-behavior", "language": "Jupyter Notebook", "readme_contents": "# RNN to predict customer behavior from interaction data\n\nCustomer behavior can be represented as sequential data describing the interactions\nof the customer with a company or a system through the time. Examples of these interactions\nare items that the customer purchases or views. Recurrent Neural Networks\nare able to model effectively sequential data and learn directly from low-level features\nwithout the need of feature engineering. In this work, we apply RNN to model this\ninteraction data and predict which items the user will consume in the future. Besides\nexploring how effective are RNNs in this scenario, we study how item embeddings\ncan help when there is a high quantity of different items, providing a comparison and\nanalysis of different methods to learn embeddings. Finally, we apply attention mechanisms\nto gain interpretability in the RNN model. We compare different variants of\nattention mechanism, providing their performance and the usefulness to explain the\npredictions of the model.\n\nThesis: https://esc.fnwi.uva.nl/thesis/centraal/files/f244841390.pdf\n"}, {"repo": "2427595858/Customer-Information-Management", "language": "Java", "readme_contents": "# Customer-Information-Management\n## \u9879\u76ee\u4ecb\u7ecd\n\u4f7f\u7528Servlet+JSP+SQL Server+Layui+intellij idea\u7f16\u5199\u7684\u5ba2\u6237\u4fe1\u606f\u7ba1\u7406\u7cfb\u7edf\n\n## \u76f8\u5173\u5de5\u5177\n\u5728\u9879\u76ee\u6587\u4ef6\u4e2d\u6709\u76f8\u5173\u7684\u8bf4\u660e\u6587\u6863\uff0c\u8bf7\u81ea\u884c\u4e0b\u8f7d\u67e5\u770b\n\n## \u5b9e\u73b0\u529f\u80fd\n- \u6e32\u67d3sqlserver\u4e2d\u7684\u5ba2\u6237\u4fe1\u606f\u5230\u524d\u7aef\n- \u5b9e\u73b0\u4e86\u67e5\u8be2\uff0c\u6dfb\u52a0\uff0c\u5220\u9664\u548c\u641c\u7d22\u529f\u80fd\n- \u5b9e\u73b0\u4e86\u5206\u9875\u67e5\u770b\u529f\u80fd\n- \u65b0\u589e\uff1a\u4f7f\u7528\u524d\u7aef\u6846\u67b6Layui\u63d0\u9ad8\u89c6\u89c9\u6548\u679c\n\n## \u6548\u679c\u9884\u89c8\n- \u663e\u793a\u5ba2\u6237\u4fe1\u606f\n![\u663e\u793a\u5ba2\u6237\u4fe1\u606f](https://github.com/2427595858/Customer-Information-Management/blob/master/img/\u663e\u793a\u5ba2\u6237\u4fe1\u606f.png)\n- \u6dfb\u52a0\u5ba2\u6237\u4fe1\u606f\n![\u6dfb\u52a0\u5ba2\u6237\u4fe1\u606f](https://github.com/2427595858/Customer-Information-Management/blob/master/img/\u6dfb\u52a0\u5ba2\u6237\u4fe1\u606f.png)\n- \u7f16\u8f91\u5ba2\u6237\u4fe1\u606f\n![\u7f16\u8f91\u5ba2\u6237\u4fe1\u606f](https://github.com/2427595858/Customer-Information-Management/blob/master/img/\u7f16\u8f91\u5ba2\u6237\u4fe1\u606f.png)\n- \u641c\u7d22\u76f8\u5173\u7528\u6237\n![\u9ad8\u7ea7\u641c\u7d22](https://github.com/2427595858/Customer-Information-Management/blob/master/img/\u9ad8\u7ea7\u641c\u7d22.png)\n\n\u5728\u6b64\u9879\u76ee\u57fa\u7840\u4e0a\uff0c\u4f7f\u7528ssm+maven\u8fdb\u884c\u6539\u9020\uff0c\u6709\u5174\u8da3\u7684\u8bdd\u53ef\u4ee5\u770b\u4e00\u4e0b\u3002\u9879\u76ee\u5730\u5740\uff1a[\u6539\u9020\u7684\u5ba2\u6237\u4fe1\u606f\u7ba1\u7406\u7cfb\u7edf](https://github.com/2427595858/customer-maven)\n\n\u5982\u679c\u89c9\u5f97\u4e0d\u9519\u7684\u8bdd\uff0c\u8bf7\u7ed9\u4e2astar\u5427\u3002\n\n\n"}, {"repo": "ChickenDinner8/Client-Customer", "language": "JavaScript", "readme_contents": "# Client-Customer\n\u70b9\u9910\u8f6f\u4ef6\u5ba2\u6237\u7aef\uff08\u987e\u5ba2\uff09 \u5fae\u4fe1\u5c0f\u7a0b\u5e8f\n\n# \u5206\u652f\u7ba1\u7406\u89c4\u8303\n\u53c2\u8003 Github Flow \u5728feature/xxx\u4e0b\u5f00\u53d1 -> dev -> master\n\n# DEMO\n- \u626b\u7801\u8fdb\u5165**Eat\u70b9\u70b9**\u70b9\u9910\u7cfb\u7edf\n\n![demo1.gif](https://github.com/ChickenDinner8/ChickenDinner8.github.io/blob/master/public/img/lumman/demo1.gif?raw=true)\n\n\n\n- \u4ece\u540e\u53f0\u83b7\u53d6\u6570\u636e\uff0c\u52a0\u8f7d\u5e76\u663e\u793a\u83dc\u5355\n\n![show menu](https://github.com/ChickenDinner8/ChickenDinner8.github.io/blob/master/public/img/lumman/demo2.gif?raw=true)\n\n\n\n- \u6dfb\u52a0\u83dc\u54c1\u5230\u8d2d\u7269\u8f66\u3001\u5e76\u5bf9\u8d2d\u7269\u8f66\u4e2d\u83dc\u54c1\u8fdb\u884c\u589e\u5220\n\n![modify cart](https://github.com/ChickenDinner8/ChickenDinner8.github.io/blob/master/public/img/lumman/demo3.gif?raw=true)\n\n\n\n- \u786e\u8ba4\u8ba2\u5355\u5e76\u53bb\u652f\u4ed8\n\n![submit](https://github.com/ChickenDinner8/ChickenDinner8.github.io/blob/master/public/img/lumman/demo4.gif?raw=true)\n\n\n"}, {"repo": "microsoft/Azure-Synapse-Solution-Accelerator-Financial-Analytics-Customer-Revenue-Growth-Factor", "language": "Jupyter Notebook", "readme_contents": "![Customer Revenue Growth Factor Solution Accelerator](./Resource_Deployment/imgs/CustomerRevenueGrowthFactor.png)\n\n## About this repository\nThis accelerator was built to provide developers with all of the resources needed to build a solution to identify the top factors for revenue growth from an e-commerce platform using Azure Synapse Analytics and Azure Machine Learning.\n\n\n## Introduction\n\n**[Watch Introduction](https://customergrowthmedia.blob.core.windows.net/media/CSG_Intro.mp4)**\n\n<a href=\"https://customergrowthmedia.blob.core.windows.net/media/CSG_Intro.mp4\"> \n\n<img src=\"https://customergrowthmedia.blob.core.windows.net/media/CGF_Still.jpg\" alt=\"customer growth factor video\"/> </a>\n\n## Getting Started \nStart by deploying the [resources](./Resource_Deployment/ResourceDeployment.md) needed for this solution: \n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzure-Synapse-Solution-Accelerator-Financial-Analytics-Customer-Revenue-Growth-Factor%2Fmain%2FResource_Deployment%2Fazuredeploy.json)\n\n1. Clone this repository and navigate to the root of the directory  \n2. Go to [Deployment guide](./Resource_Deployment/README.md) for the steps you need to take to deploy this solution \n\n## Prerequisites\nIn order to successfully complete your solution, you will need to have access to and or provisioned the following:\n1. Access to an Azure subscription\n2. A Power BI Pro License (or free trial)\n\nOptional\n1. Azure Storage Explorer\n\n## Azure and Analytics Platform\nThe directions provided for this repository assume fundemental working knowledge of Azure, Azure Synapse Analytics, Azure Machine Learning Services, and Power BI.\n\nFor additional training and support, please see:\n 1. [Azure Synapse Analytics](https://azure.microsoft.com/en-us/services/synapse-analytics/)\n 2. [Azure Machine Learning Services](https://azure.microsoft.com/en-us/services/machine-learning/)\n 3. [Power BI](https://docs.microsoft.com/en-us/power-bi/)\n\n## Process Overview  \n\nThe architecture diagram below details what you will be building for this Solution Accelerator:\n\n![Azure Synapse AI Architecture](./Reference/Architecture/SynapseArchitecture.png)\n![Azure Synapse AI + AutoML Architecture](./Reference/Architecture/SynapseAutoMLArchitecture.png)\n\n### [Resource Deployment](./Resource_Deployment)\nThe resources in this folder can be used to deploy the required resources into your Azure Subscription. You can do this in the Azure Portal\n\n### [Analytics Deployment](./Analytics_Deployment)\nThis folder contains the Notebooks needed to complete this solution accelerator. Once you have deployed all the required resources from ResourceDeploymnet.md, run through the Notebooks following the instructions in [Resource Deployment](./Resource_Deployment). \n\n## License\nCopyright (c) Microsoft Corporation\n\nAll rights reserved.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"\"Software\"\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED AS IS, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"}, {"repo": "fidley/Customers", "language": "ABAP", "readme_contents": "# Wrapper for CMD_EI_API class\nInstal with abapGIT and give me a feedback. More info soon at https://abapblog.com/articles/how-to/130-zcl-cmd-customer-wrapper-for-cmd-ei-api-classes\n"}, {"repo": "mkucz95/customer_segmentation", "language": "HTML", "readme_contents": "# Identifying Customer Segments\n\n## Installations\n- Python: v.3.6.3\n- Bertelsmann Data is Private and not avaliable for distribution\n- Libraries: Sci-Kit Learn, Pandas, Numpy\n\n## Project Motivation\nIn this project, I work with real-life data provided to by Bertelsmann partners AZ Direct and Arvato Finance Solution. The data here concerns a company that performs mail-order sales in Germany. Their main question of interest is to identify facets of the population that are most likely to be purchasers of their products for a mailout campaign. I use unsupervised learning techniques to organize the general population into clusters, then use those clusters to see which of them comprise the main user base for the company. Prior to applying the machine learning methods, I assess and clean the data in order to convert the data into a usable form.\n\n\nThe unsupervised learning branch of machine learning is key in the organization of large and complex datasets. While unsupervised learning lies in contrast to supervised learning in the fact that unsupervised learning lacks objective output classes or values, it can still be important in converting the data into a form that can be used in a supervised learning task. Dimensionality reduction techniques can help surface the main signals and associations in your data, providing supervised learning techniques a more focused set of features upon which to apply their work. Clustering techniques are useful for understanding how the data points themselves are organized. These clusters might themselves be a useful feature in a directed supervised learning task. This project will give you hands-on experience with a real-life task that makes use of these techniques, focusing on the unsupervised work that goes into understanding a dataset.\n\nIn addition, the dataset presented in this project requires a number of assessment and cleaning steps before you can apply your machine learning methods.\n\n## File Descriptions\n- Data_Dictionary.md: meanings of variables\n- [Identify_Customer_Segments.html](http://htmlpreview.github.io/?https://raw.githubusercontent.com/mkucz95/customer_segmentation/master/Identify_Customer_Segments.html): html version of python notebook\n- [Identify_Customer_Segments.ipynb](https://github.com/mkucz95/customer_segmentation/blob/master/Identify_Customer_Segments.ipynb): python notebook with work done\n\n### 1.1 Assessing Missing Data in Each Column\nThere were some columns with significant amounts of missing data. Out of the total of 891,221 observations, 6 of the features had more than 200,000 missing values. I decided that these were the outliers since close to 25% of the data was missing. Most of these were supposed to hold information on a personal or household level. Possibly people were uncomfortable disclosing this kind of information. The columns excluded from the dataset are shown above.\n\nIt is also interesting to note that many columns had identical numbers of missing values. The most common was 116515 missing values which occurred 7 time, including the following key/value paris of (#NaN,#occurences): (4854, 6), (133324, 6), (73499, 4), (93148, 4), (111196, 3), (99352, 3), (93740, 3), (77792, 2), (158064, 2), (97375, 2). Since so many features had identical numbers of missing values it is most likely that it was the same entries (rows) that didn't fill in particular information (perhaps because it was sensitive). This might also mean that these columns contain similar data and it might not be necessary to keep all the features in this case.\n\nFor example, the 7 features that all have 116515 are all from the `PLZ8` macro_cell features. This is all to do with information regarding the building types and family houses of the region a person lives in. Maybe all the people with mssing data live in the same region for which there is no data.\n\nThe features that each have 4845 missing values mostly come from different feature groupings/categories: `CJT_GESAMTTYP`, `GFK_URLAUBERTYP`, `RETOURTYP_BK_S`, `ONLINE_AFFINITAET`, apart from `LP_STATUS_FEIN` and `LP_STATUS_GROB` which are the same feature on a fine and rough scale. Since they are mostly different features describing different things maybe certain people here didn't feel comfortable disclosing this information.\n\nThis means that there are feasibly two reasons columns would have identical numer of NaN's:\n\nthey have similar meaning and if it is hard to get data on one then it is hard to get data for all features in that category (like RR3 micro-cell)\n\ncertain people didn't feel comfortable disclosing certain information and these people are likely to be more withdrawn in answering among various features.\n\n![Missing Values in Features](https://github.com/mkucz95/customer_segmentation/blob/master/nan_cols.png)\n\n\n### 1.2 Assess Missing Data in Each Row\nI looked at 6 features where the data had no missing values, and quite interestingly some of these features had even distributions of values between the two datasets, and some were quite uneven.\n\nThe highest difference in distributions was seen in the columns:\n\n- `FINANZ_ANLEGER`\n- `FINANZ_SPARER`\n- `FINANZ_VORSORGER`\nThis means that 3 out of the 6 features I looked at have a very different distribution of values between the two splits of data- NaN heavy data and NaN light data. This means that it might not be the best idea to drop a lot of the NaN datum, as it could disort the data. We should revist the high NaN rows later as some seem to be qualitatively different.\n\n![Missing Values in Rows](https://github.com/mkucz95/customer_segmentation/blob/master/nan_rows.png)\n\n![Distribution of Values Between Features](https://github.com/mkucz95/customer_segmentation/blob/master/nan_dist.png)\n\n### 1.3 Re-Encode Categorical Features\n- The non-numerical binary variable is: `OST_WEST_KZ`\n- `CAMEO_DEU_2015` and `CAMEO_DEUG_2015` are a multi-level categorical: alphanumeric\n- `SOHO_KZ` and `GREEN_AVANTGARDE` is a regular binary variable\n- `ANREDE_KZ` is binary, but takes values `[1,2]`\n\n\nThe variables that need to be re-encoded are:\n- non-numerical binary var: `OST_WEST_KZ` will be converted to regular binary\n- all multi-level categoricals will be one hot encoded\n- The variables I dropped are:\n      > 1. `KK_KUNDENTYP`\n      > 2. `TITEL_KZ`\n      > 3. `AGER_TYP`\n      \nI dropped them because they had very few (relative to length of dataset) non-missing values. Which wouldn't be useful for analysis anyway.\n\n**Engineering Steps**\nFor alphanumeric binary var `OST_WEST_KZ`:\n\nconverted `OST_WEST_KZ` to binary variable: `WEST_KZ`\nW maps to 1 (signifies True/1 for \"west\")\nO maps to 0\nFor categorical variables: `['CJT_GESAMTTYP', 'FINANZTYP','GFK_URLAUBERTYP', 'LP_FAMILIE_FEIN', 'LP_FAMILIE_GROB', 'LP_STATUS_FEIN', 'LP_STATUS_GROB', 'NATIONALITAET_KZ', 'SHOPPER_TYP', 'ZABEOTYP', 'GEBAEUDETYP', 'CAMEO_DEUG_2015', 'CAMEO_DEU_2015']`\n\nFirst, I imputed each variable column to prevent getting a one hot encoded column for NaN - missing values.\n\nto do this I find the highest frequency value in each common (most likely category), and then fill the missing values with that instead of a mean for example. The mean would be a bad choice to impute categorical variables as it makes the variable continuous rather than discrete.\n\n### 1.4 Engineer Mixed-Type Features\nEngineering `PRAEGENDE_JUGENDJAHRE`\ncreated mapping from the current variable to a categorical `YOUTH_DECADE` variable denotes which decade the persons childhood was in. for example 60s became 60 created mapping from the current variable to a binary `AVANTGARDE` variable if the predominant movement during this persons childhood was avantgarde, the binary var denotes 1.0 otherwise it denotes 0.0\n\n**Engineering  CAMEO_INTL_2015**\nfirst defined two mapping functions: decode_wealth and decode_life\n\n- `decode_wealth()` returns the first digit of the orginial variable which shows wealth status\n- `decode_life()` returns the second digit of the original variable showing life stage\nI applied this function to the data set and created a new ordinal variable for each wealth and life stage\n-- `CAMEO_INTL_2015_WEALTH`\n-- `CAMEO_INTL_2015_LIFE`\n\n***Other mixed variables***\nthe remaining mixed variables that were still in data set after engineering step were: `['LP_LEBENSPHASE_FEIN', 'LP_LEBENSPHASE_GROB', 'WOHNLAGE','PLZ8_BAUMAX']`\n\n- `LP_LEBENSPHASE_FEIN` this variable essentially encodes the same things as `CAMEO_INTL_2015`, so I decided to drop it\n- `LP_LEBENSPHASE_GROB` this variable is the same as the variable above but on rougher scale, dropping as well\n-`CAMEO_DEU_2015` contains similar but more defined information. `WOHNLAGE` is encoded as 1: rural, 0: not-rural in variable `RURAL`\n- `PLZ8_BAUMAX` this variable is encapsulated by the information in other PLZ8 variables, is dropped.\n\nSecondly, I used pandas.get_dummies to one hot encode each column, dropped the original column from the dataset, and then added the new encoded data columns to the dataset.\n\n### 2.1 Apply Feature Scaling\nI first had to make sure that there were no null values in the data set. Most of the data had very few nulls left. Only two features had more than 20,000 NaN, which is only about 5% of all the values.\n\nI decided that the best imputation method was mode instead of mean. Most of the values are categorical, and even some of the numeric values like birth year wouldnt make sense if they were continuous. Therefore, I imputed using most_frequent\nI used the `StandardScaler()` provided by `sklearn.preprocessing` to scale all the values to mean 0 and stdDev = 1. This was suggested in the writeup.\n\n\n### 2.2 Perform Dimensionality Reduction\nThe Principal Component Analysis with all of the Principal Components showed that the first 6 Principal Components each explain more than 3% of the total variance, with the highest explained variance for a single Principal Component being ~8%. The first 15 Principal Components explain 40% of all the variance. I am retaining 75 of the 185 Prinicpal Components for the analysis, as they explain about 75% of the total variance.\n\nIt is interesting to note that the last 50 components (#125-175) explain very little of the variance.\n\n![PCA Variance Explanation](https://github.com/mkucz95/customer_segmentation/blob/master/pca_var.png)\n\n### 2.3: Interpret Principal Components\nEach principal component is a unit vector that points in the direction of highest variance (after accounting for the variance captured by earlier principal components). The further a weight is from zero, the more the principal component is in the direction of the corresponding feature. If two features have large weights of the same sign (both positive or both negative), then increases in one tend expect to be associated with increases in the other. To contrast, features with different signs can be expected to show a negative correlation: increases in one variable should result in a decrease in the other.\n\n#### 1st Principal Component\nThe first principal component is explained mostly by the `LP_STATUS_GROB_1.0` variable which shows the social status: low-income earner of a specific person. The next three features with highest weight: `HH_EINKOMMEN_SCORE`, `CAMEO_INTL_2015_WEALTH`, `PLZ8_ANTG3`, are all also a certain measure of wealth or income. We can reconcile this with the fact that features with large weights in the same sign are a sign of correlation between the variables. Realistically, we would expect that each feature portraying wealth would be correlated. The three most negatively correlated features are: `FINANZ_MINIMALIST`, `KBA05_ANTG1`, `MOBI_REGIO`. For example: `FINANZ_MINIMALIST`, shows the lack of interest of someone in finance, which has an obvious negative correlation to wealth.\n\n#### 2nd Principal Component\nThe second principal component is explained mostly by: `ALTERSKATEGORIE_GROB`, `FINANZ_VORSORGER`, `ZABEOTYP_3`, and the least by: `YOUTH_DECADE`, `FINANZ_SPARER`, `SEMIO_REL`. This is a mixture of age, financial awareness, and environmental/religious awareness respectively.\n\n#### 3rd Principal Component\nI find this third component most interesting, as many of the features that explain this principal component relate to personality traits. The most explanatory feature is `SEMIO_VERT` which is a measure of 'dreamfulness', followed by `SEMIO_FAM`: family-mindedness, `SEMIO_SOZ`: socially-mindedness and so forth. The least explanatory features, and those most negatively correlated are interestingly: `ANREDE_KZ` which shows gender, `SEMIO_KAEM` which is a combative-minded feature, `SEMIO_DOM` which is a dominant-minded feature and so forth.\n\nWhat is interesting is that as we move through the principal components, at least the first three, the weights on features become larger. This means that those features more strongly explain the principal component, after accounting for the variance captured by earlier principal components.\n\n\n###  3.1: Apply Clustering to General Population\nWhen investigating clustering and drawing scree-plot, I did not find a specific kink in the graph, but rather the average distance kept decreasing. The largest decrease happened in the first 5 clusters, before the benefit of adding an extra cluster starts levelling out. This is why I chose to segment the population into 10 clusters, a nice round number where the average distance is starting to level out on the scree plot.\n\n### 3.3: Compare Customer Data to Demographics Data\n\n(Double-click this cell and replace this text with your own text, reporting findings and conclusions from the clustering analysis. Can we describe segments of the population that are relatively popular with the mail-order company, or relatively unpopular with the company?)\n\nWhen including the population count for all including those we dropped for analysis as they had missing data, the culsters: `8`,`2`,`4` were the most over represented in the customer data compared to the general population data (`16.42%`, `4.06%%`, `3.51%`. These percentages were higher for low-NaN data. Based on the full general population data, it seems that clusters `4`, `5`, `6`, `9`, `10` were closely represented in both general population and customer data (difference of < 4%).\n\nThe must under-represented compared to full population data were clusters: `3`, `1`, and `7` by a substantial `10.84%`, `7.57%`, and `6.74%` respectively.\n\nTo see which features define each cluster the most, I took the inverse transformation of the PCA, which gives us back the original features but on a standard scale. This is much easier to interpret than the principal components themselves.\n\n#### Over-Represented Cluster: #8\n* `LP_STATUS_FEIN_10.0` is the highest impact feature in the 8th cluster. It represents the highest income housholds and highest social status.\n    * `'LP_STATUS_GROB_5.0` represents the same aspect but on a less refined scale\n* `GREEN_AVANTGARDE` is the feature with the third highest impact in the 8th cluster. It represents participation and interest in environmental issues during a person's youth.\n* the least impactful feature in the 8th cluster is `HH_EINKOMMEN_SCORE`, which corroberates our first point as it is correlated with low income, but high income and status is the biggest definer of our cluster.\n\nTherefore, people with a high income and high social status who are also environmentally conscious are significantly over represented in the customer data in comparison to the general population. This is a good target audience for any marketing campaing.\n\n----\n\n#### Over-Represented Cluster: #2\nTwo of the features cluster two is characterized by are:\n* `LP_STATUS_GROB_2.0`: average income and social status\n* `LP_STATUS_FEIN_4.0`: villagers\n\nWhile average earning villagers are over-represented in the customer data, they are more closely represented than the people in cluster #8 above. Nevertheless, it could still probably be worth to target these people in a campaign\n\n----\n\n#### Under-Represented Cluster: #3\n* `LP_STATUS_FEIN_2.0` is the biggest factor in the third cluster, and that represents orientation-seeking low-income earners. \n* `FINANZTYP_1` is the second most important factor, and it represents people who are not interested in finance.\n* `SEMIO_RAT` is a variable that portrays rationality (higher values means less rational).\n\nThis means that low income people who are both irrational and do not care for finance much are under-represented in the customer population (which makes sense since this is a financial services company)\n\n\n## Licensing, Authors, Acknowledgements, etc.\n Data provided by Bertelsmann SE & Co. KGaA, organized by Udacity.\n"}, {"repo": "lll618xxx/quick-customer", "language": "JavaScript", "readme_contents": "# quick-customer\n\n### \u4ecb\u7ecd\n\n\u901f\u5ba2\u4f18\u9c9c\uff0c\u4e00\u4e2a\u5b8c\u6574\u7684\u524d\u540e\u7aef JS \u6808\u9879\u76ee\uff0c\u524d\u7aef\u57fa\u4e8e uni-app \u5b9e\u73b0\uff0c\u540e\u7aef\u57fa\u4e8e express \u5b9e\u73b0\n\n<div>\n<img src=\"https://gw.alipayobjects.com/os/q/cms/images/kbusg743/8f8f8232-0687-4308-b480-03f5a2054185_w328_h578.png\" alt=\"\u9879\u76ee\u622a\u56fe\" width=\"218\" height=\"385\">\n&nbsp;\n<img src=\"https://gw.alipayobjects.com/os/q/cms/images/kbusgiqr/83bc9b72-4291-4ca9-88d0-89669e5d4020_w326_h577.png\" alt=\"\u9879\u76ee\u622a\u56fe\" width=\"218\" height=\"385\">\n&nbsp;\n<img src=\"https://gw.alipayobjects.com/os/q/cms/images/kbusgo30/50d9c73a-c100-4927-9611-b32e66a43ecf_w325_h578.png\" alt=\"\u9879\u76ee\u622a\u56fe\" width=\"218\" height=\"385\">\n&nbsp;\n<img src=\"https://gw.alipayobjects.com/os/q/cms/images/kbusgtyb/9344e6fe-eb0e-460f-aaca-d85749321ba6_w326_h580.png\" alt=\"\u9879\u76ee\u622a\u56fe\" width=\"218\" height=\"385\">\n&nbsp;\n<img src=\"https://gw.alipayobjects.com/os/q/cms/images/kbush07r/a18f3112-b155-42cf-b47f-5afc7dd34a81_w328_h580.png\" alt=\"\u9879\u76ee\u622a\u56fe\" width=\"218\" height=\"385\">\n&nbsp;\n<img src=\"https://gw.alipayobjects.com/os/q/cms/images/kbush5ee/7ba81de6-103d-42b2-ba53-b230e06b69ac_w323_h577.png\" alt=\"\u9879\u76ee\u622a\u56fe\" width=\"218\" height=\"385\">\n</div>\n\n### \u5b89\u88c5\n\n```\nnpm i\n```\n\n### \u4f7f\u7528\n\n- 1\u3001\u5c06 skyx.sql \u5bfc\u5165\u672c\u5730\u6570\u636e\u5e93\n\n&emsp;&emsp;\u4fee\u6539 server/config/index.js \u7684\u7528\u6237\u540d\u548c\u7528\u6237\u5bc6\u7801\uff0c\u786e\u4fdd\u80fd\u8fde\u4e0a\u6570\u636e\u5e93\n\n- 2\u3001\u542f\u52a8\u670d\u52a1\u7aef\n\n```\ncd server && node app.js\n```\n\n- 3\u3001\u901a\u8fc7 Hbuilder X \u542f\u52a8\u5ba2\u6237\u7aef\uff08\u6d4f\u89c8\u5668/\u5c0f\u7a0b\u5e8f/APP\uff09\n\n### \u524d\u7aef\u76ee\u5f55\u7ed3\u6784\n\n```\napp\n    --components (\u516c\u5171\u7ec4\u4ef6)\n        --goodItem.vue (\u5546\u54c1\u7c7b\u76ee\u7ec4\u4ef6)\n    --http (http\u8bf7\u6c42)\n        --AxiosConfig.js (axios\u914d\u7f6e)\n        --HttpService.js (\u8bf7\u6c42\u65b9\u5f0f\u548c\u8d44\u6e90\u8def\u5f84\u914d\u7f6e)\n        --PathConfig.js (\u63a5\u53e3\u5730\u5740\u914d\u7f6e)\n        --ServerBase.js (axios\u8bf7\u6c42\u5c01\u88c5)\n    --pages (\u9875\u9762\u7ec4\u4ef6)\n        --home (\u9996\u9875)\n            --index.vue (\u9996\u9875\u5165\u53e3)\n        --good (\u5546\u54c1)\n            --detail.vue (\u5546\u54c1\u8be6\u60c5)\n        --classify (\u5546\u54c1)\n            --index.vue (\u5206\u7c7b\u5165\u53e3)\n        --shopCart (\u8d2d\u7269\u8f66)\n            --index.vue (\u8d2d\u7269\u8f66\u5165\u53e3)\n        --mine (\u6211\u7684)\n            --index.vue (\u6211\u7684\u5165\u53e3)\n            --login.vue (\u767b\u5f55)\n            --register.vue (\u6ce8\u518c)\n    --static (\u9759\u6001\u76ee\u5f55)\n        --iconfont.css (\u56fe\u6807\u5b57\u4f53)\n        --reset.css (\u6837\u5f0f\u91cd\u7f6e)\n    --router (\u8def\u7531\u76ee\u5f55)\n        --config.js (\u8def\u7531\u914d\u7f6e)\n        --index.js (\u8def\u7531\u5165\u53e3)\n    --utils (\u5de5\u5177\u76ee\u5f55)\n        --index.js (\u5de5\u5177\u5e93)\n```\n\n### \u540e\u7aef\u76ee\u5f55\u7ed3\u6784\n\n- \u6570\u636e\u5e93\u91c7\u7528 mysql\n- ORM \u6846\u67b6 sequelize\n\n```\nserver\n    --config(\u914d\u7f6e\u76ee\u5f55)\n        --index.js (\u6570\u636e\u5e93\u914d\u7f6e\u5165\u53e3)\n    --config(\u6a21\u578b\u76ee\u5f55)\n        --goods.js (\u5546\u54c1\u6a21\u578b)\n        --users.js (\u7528\u6237\u6a21\u578b)\n    --router (\u8def\u7531\u76ee\u5f55)\n        --action (\u8def\u7531\u64cd\u4f5c)\n        --index.js (\u8def\u7531\u5165\u53e3)\n    --app.js (\u9879\u76ee\u96c6\u6210\u5165\u53e3)\n```\n"}, {"repo": "Lierbook/Cloud-customer-service", "language": "Vue", "readme_contents": "# myapp\n\n## Project setup\n```\nnpm install\n```\n\n### Compiles and hot-reloads for development\n```\nnpm run serve\n```\n\n### Compiles and minifies for production\n```\nnpm run build\n```\n\n### Lints and fixes files\n```\nnpm run lint\n```\n\n### Customize configuration\nSee [Configuration Reference](https://cli.vuejs.org/config/).\n"}, {"repo": "js-lee-AI/Realtime_Customer-yolov3", "language": "Python", "readme_contents": "#   Android app Using YOLO Real-time customer number and congestion in Your Cafe \n\n \uc2e4\uc2dc\uac04\uc73c\ub85c \uce74\ud398\uc5d0 \uc874\uc7ac\ud558\ub294 \uace0\uac1d \uc218\uc640 \uc774\uc6a9\uac00\ub2a5\ud55c \uc88c\uc11d \uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n \n ## \ubca0\uc774\uc2a4 \ubaa8\ub378 (Based on Models)\n - **YOLO(Python)** : \uc2e4\uc2dc\uac04 \uc774\uc6a9 \uace0\uac1d \uc218 \ud0d0\uc9c0\n - **flask** : \uc6f9 \uc11c\ubc84 \uc5f0\uacb0\n -  **mysql** : DB\n\n\n## \uc0ac\uc6a9 \ubc29\ubc95 (How to use)\n -  Mysql DB \uc0dd\uc131 :\n\t -  mysql> CREATE TABLE Cafe (cafe_name VARCHAR(100), \t           customer_num INT);\n   \t -  mysql> INSERT INTO Cafe VALUES( '*Your_Cafe_name*', 0)\n\t \n\t - ex) <br>\n\t<img src=\"img/1.png\">\n\n- cam_demo.py \uc2e4\ud589 :\n     - 17\ubc88\uc9f8 \uc904<br>\n      connect = pymysql.connect(host='localhost', user='*User_name*', password='*User_password*', db='*db_name*', charset='utf8', autocommit=True)\n     \uc0dd\uc131\ud55c DB \uacc4\uc815\uacfc Password, DB \uba85\uc744 \ub123\uc5b4\uc11c \uc5f0\uacb0\ud574\uc8fc\uc2dc\uba74 \ub429\ub2c8\ub2e4.\n\t<br>\n\t<img src=\"img/2.png\">\n\t<br>\n\t - Query\ubb38 \uc218\uc815 164\ubc88\uc9f8 \uc904 \ucffc\ub9ac\ub85c DB\ub0b4\uc5d0 \uc874\uc7ac\ud558\ub294 \uc870\ud68c\ud560 \uce74\ud398 \uc870\uc815<br>\n\t  sql = \"UPDATE Cafe set customer_num={} where cafe_name={} ;\".format(person, '*Your_Cafe_name*')\n\t  \n- Web server \uc5f0\uacb0 \n    - run.py \uc2e4\ud589\n    - app.run(host=\"0.0.0.0\", port=\"*Your_port_num*\")\n\n\n## \n\n- cam_demo.py\n<img src=\"img/4.gif\">\n\n<br><br>\n- application in the phone\n\n\t<img src=\"img/3.jpg\">\n\n\n\n## References\n\n*eriklindernoren*<br>\n\noriginal code\nhttps://github.com/eriklindernoren/PyTorch-YOLOv3\n\n\n## Created by\n- Jungseob, Lee. (\uc774\uc815\uc12d) - omanma1928@naver.com - b.s. Dongguk Universitiy in Seoul, Data Mining Lab.\n- Jinsoo, Lee. (\uc774\uc9c4\uc218) - jinsoo8409@naver.com - b.s. Dongguk Universitiy in Seoul, Data Mining Lab.\n- Juwon, Seo. (\uc11c\uc8fc\uc6d0)  - wndnjs3334@gmail.com - b.s. Dongguk Universitiy in Seoul, Data Mining Lab.\n"}, {"repo": "Bugdragon/customer_value_recognition", "language": "Python", "readme_contents": "customer_value_recognition\n==========================\n\u822a\u7a7a\u516c\u53f8\u5ba2\u6237\u4ef7\u503c\u8bc6\u522b\n---------------------\n+ \u6839\u636e\u5ba2\u6237\u4fe1\u606f\uff0c\u5bf9\u5ba2\u6237\u8fdb\u884c\u5206\u7c7b\u3002\n+ \u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u5ba2\u6237\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5206\u6790\u4e0d\u540c\u7c7b\u578b\u5ba2\u6237\u7684\u4ef7\u503c\u3002\n+ \u91c7\u53d6\u4e2a\u6027\u5316\u670d\u52a1\uff0c\u6839\u636e\u5ba2\u6237\u7c7b\u578b\uff0c\u5236\u5b9a\u76f8\u5e94\u8425\u9500\u7b56\u7565\u3002\n\n\u535a\u5ba2\uff1ahttp://blog.csdn.net/nwu_NBL/article/details/79468263\n"}, {"repo": "dprothero/Loosely.CustomerPortal", "language": "C#", "readme_contents": "A Real-World MassTransit Customer Portal Example\n================================================\n\nNow that we\u2019ve seen [some](http://looselycoupledlabs.com/2014/06/masstransit-publish-subscribe-example/) simple [examples](http://looselycoupledlabs.com/2014/07/creating-a-rabbitmq-cluster-for-use-with-masstransit/) of how to use [MassTransit](http://masstransit-project.com/) with the Publish/Subscribe pattern on multiple machines, let\u2019s build something that resembles a more real-world app. In this article, we\u2019ll build an ASP.NET MVC Customer Portal app where a customer can create a new support ticket. The ticket will be published onto the service bus. We\u2019ll create a Windows Service to be the subscriber of these messages and it will handle the tickets, in this example, sending a confirmation email to the customer.\n\nThis is a big one, so roll up your sleeves\u2026\n\n[Continue Reading](http://looselycoupledlabs.com/2014/07/a-real-world-masstransit-customer-portal-example/)\n"}, {"repo": "yemiwebby/typescript-react-customer-app", "language": "TypeScript", "readme_contents": "This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\n\n## Available Scripts\n\nIn the project directory, you can run:\n\n### `npm start`\n\nRuns the app in the development mode.<br>\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\n\nThe page will reload if you make edits.<br>\nYou will also see any lint errors in the console.\n\n### `npm test`\n\nLaunches the test runner in the interactive watch mode.<br>\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\n\n### `npm run build`\n\nBuilds the app for production to the `build` folder.<br>\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\nThe build is minified and the filenames include the hashes.<br>\nYour app is ready to be deployed!\n\nSee the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.\n\n### `npm run eject`\n\n**Note: this is a one-way operation. Once you `eject`, you can\u2019t go back!**\n\nIf you aren\u2019t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.\n\nInstead, it will copy all the configuration files and the transitive dependencies (Webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own.\n\nYou don\u2019t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it.\n\n## Learn More\n\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\n\nTo learn React, check out the [React documentation](https://reactjs.org/).\n"}, {"repo": "IBM/icp4d-customer-churn-classifier", "language": "Jupyter Notebook", "readme_contents": "# Infuse AI into your application\n\nIn this code pattern, we will create and deploy a customer churn prediction model using IBM Cloud Pak for Data. The basis for our model is a data set that contains customer demographics and trading activity data. We will use a Jupyter notebook to visualize the data, build hypotheses for prediction, and then build, test, and save a prediction model. Finally, we will enable a web service and use the model from an app.\n\n> This code pattern has been updated to include images from the latest version of Cloud Pak for Data, v3.5.0.\n\nThe use case describes a stock trader company that can use churn prediction to target offers for at-risk customers. Once deployed, the model can be used for inference from an application using the REST API. A simple app is provided to demonstrate using the model from a Python app.\n\nWhen the reader has completed this code pattern, they will understand how to:\n\n* Load customer data into Db2 Warehouse\n* Run a Jupyter notebook\n* Visualize data using Brunel\n* Create a model using Spark ML library\n* Deploy the model as a web service\n* Access the model from an external app for inference (churn risk prediction)\n\n![architecture](doc/source/images/architecture.png)\n\n## Flow\n\n1. Data is loaded locally, or optionally into Db2 Warehouse\n1. Jupyter notebook accesses data\n1. Jupyter notebook uses Brunel for information visualization\n1. Jupyter notebook uses Spark ML library to create a model\n1. Jupyter notebook saves the model to the repository for deployment\n1. Applications access the model via the REST API\n\n## Watch the Video\n\n[![video](https://img.youtube.com/vi/rnoqAagpRaM/0.jpg)](https://www.youtube.com/watch?v=rnoqAagpRaM)\n\n## Prerequisites\n\nThe instructions in this code pattern assume you are using IBM Cloud Pak for Data version 3.5.0.\n\n## Steps\n\nSign in to your IBM Cloud Pak for Data web client. All of the steps are performed using the web client unless stated otherwise.\n\n1. [Clone the repo](#1-clone-the-repo)\n2. [Set up an analytics project](#2-set-up-an-analytics-project)\n3. [Create a Space for Machine Learning Deployments](#3-create-a-space-for-machine-learning-deployments)\n4. [Create the notebook](#4-create-the-notebook)\n5. [Insert pandas DataFrame](#5-insert-pandas-dataframe)\n6. [Initialize Watson Machine Learning client](#6-initialize-watson-machine-learning-client)\n7. [Provide the deployment space information](#7-provide-the-deployment-space-information)\n8. [Run the notebook](#8-run-the-notebook)\n9. [Analyze the results](#9-analyze-the-results)\n10. [Test the model](#10-test-the-model)\n11. [Use the model in an app](#11-use-the-model-in-an-app)\n12. [(OPTIONAL) Use Db2 Warehouse to store customer data](#12-optional-use-db2-warehouse-to-store-customer-data)\n\n### 1. Clone the repo\n\nClone the `icp4d-customer-churn-classifier` repo locally. In a terminal, run the following command:\n\n```bash\ngit clone https://github.com/IBM/icp4d-customer-churn-classifier\n```\n\n### 2. Set up an analytics project\n\nTo get started, open the `Projects` page and set up an analytics project to hold the assets that you want to work with, and then get data for your project.\n\n#### Create a project\n\n* Launch a browser and navigate to your Cloud Pak for Data deployment.\n\n* Go to the (\u2630) menu, expand `Projects` and click `All projects`:\n\n![(\u2630) Menu -> Projects](doc/source/images/cpd-projects-menu.png)\n\n* Click on *New project +*. In the dialog that pops up, select the project type as `Analytics project` and click `Next`:\n\n![Start a new project](doc/source/images/cpd-new-project.png)\n\n* Click on the top tile for `Create an empty project`:\n\n![Create an empty project](doc/source/images/cpd-create-empty-project.png)\n\n* Give the project a unique name, an optional description and click `Create`:\n\n![Pick a name](doc/source/images/cpd-new-project-name.png)\n\n#### Add the data asset\n\n> NOTE: You can optionally load the data into Db2 Warehouse. For instructions, go to [use Db2 Warehouse to store customer data](#12-optional-use-db2-warehouse-to-store-customer-data).\n\n* In your project, on the `Assets` tab, click the `01/00` icon and the `Load` tab, then either drag the [data/mergedcustomers.csv](data/mergedcustomers.csv) file from the cloned repository to the window or navigate to it using `browse for files to upload`:\n\n![Add data set](doc/source/images/cpd-add-data-set.png)\n\n### 3. Create a Space for Machine Learning Deployments\n\nBefore we create a machine learning model, we will have to set up a deployment space where we can save and deploy the model.\n\nFollow the steps in this section to create a new deployment space.\n\n* Navigate to the left-hand (\u2630) hamburger menu and choose `Deployments`:\n\n![(\u2630) Menu -> Deployments](doc/source/images/ChooseAnalyticsDeployments.png)\n\n* Click on `New deployment space +`:\n\n![Add New deployment space](doc/source/images/addNewDeploymentSpace.png)\n\n* Click on the top tile for 'Create an empty space':\n\n![Create empty deployment space](doc/source/images/createEmptyDeploymentSpace.png)\n\n* Give your deployment space a unique name, an optional description, then click `Create`.\n\n![Create New deployment space](doc/source/images/createNewDeploymentSpace.png)\n\n* When you get a notification that the space is ready, click on `View new space`.\n\n![View deployment space](doc/source/images/viewDeploymentSpace.png)\n\n### 4. Create the notebook\n\n* Back in your project, on the `Assets` tab, either click the `Add to project +` button, and choose `Notebook`, or, if the *Notebooks* section exists, to the right of *Notebooks* click `New notebook +`:\n\n![Add notebook](doc/source/images/wml-1-add-asset.png)\n\n* On the next screen, select the *From file* tab, give your notebook a *name* and an optional *description*, choose the `Python 3.7` environment as the *Runtime* and then either drag the [notebooks/TradingCustomerChurnClassifierSparkML.ipynb](notebooks/TradingCustomerChurnClassifierSparkML.ipynb) file from the cloned repository to the window or navigate to it using `Drag and drop files here or upload.`. Click `Create`:\n\n![Add notebook name and upload file](doc/source/images/wml-2-add-name-and-upload-file.png)\n\n* When the Jupyter notebook is loaded and the kernel is ready then we can start executing cells.\n\n![Notebook loaded](doc/source/images/wml-3-notebook-loaded.png)\n\n> **Important**: *Make sure that you stop the kernel of your notebook(s) when you are done, in order to conserve memory resources!*\n\n![Stop kernel](doc/source/images/JupyterStopKernel.png)\n\n### 5. Insert pandas DataFrame\n\nNow that you are in the notebook, add generated code to insert the data as a DataFrame and fix-up the notebook reference to the DataFrame.\n\n- Place your cursor at the last line of the following cell:\n\n  ```python\n  # Use the find data 01/00 icon and under your remote data set\n  # use \"Insert to code\" and \"Insert pandas DataFrame\n  # here.\n\n  # Add asset from file system\n  ```\n\n* Click the *find data* `01/00` icon on the menu bar (last icon). On the *Files* tab, find the data set that you added to the project, click `Insert to code` and `pandas DataFrame`.\n\n![insert_spark_dataframe.png](doc/source/images/insert_spark_dataframe.png)\n\n* The inserted code will result in a DataFrame assigned to a variable named `df1` or `df_data_1` (perhaps with a different sequence number). Find the code cell like the following code block and edit the `#` to make it match the variable name.\n\n  ```python\n  # After inserting the pandas DataFrame code above, change the following\n  # df_data_# to match the variable used in the above code. df_churn_pd is used\n  # later in the notebook.\n  df_churn_pd = df_data_#\n  ```\n\n### 6. Initialize Watson Machine Learning client\n\nThe Watson Machine Learning client is required to save and deploy our customer churn predictive model, and should be available on your IBM Cloud Pak for Data platform. Find the cell containing the code given below and insert the url, username and password for your IBM Cloud Pak for Data instance:\n\n  ```python\n from ibm_watson_machine_learning import APIClient\n\n# get URL, username and password from your IBM Cloud Pak for Data administrator\nwml_credentials = {\n  \"url\": \"https://X.X.X.X\",\n  \"username\": \"*****\",\n  \"password\": \"*****\",\n  \"instance_id\": \"wml_local\",\n  \"version\" : \"3.5\"\n}\n\nclient = APIClient(wml_credentials)\nprint(client.version)\n  ```\n\n### 7. Provide the deployment space information\n\nIBM Cloud Pak for Data uses the concept of deployment spaces, which is where models can be deployed. You can list all the spaces using the .list() function. \n\nProvide the name of the deployment space that you created in [Step 3](#3-create-a-space-for-machine-learning-deployments) above in the cell containing the following text.\n\n```python\n#Insert the name of your deployment space here:\nDEPLOYMENT_SPACE_NAME = 'INSERT-YOUR-DEPLOYMENT-SPACE-NAME-HERE'\n```\n\nThe next cell, then, looks up the deployment space id based on the name that you have provided and prints it out. If you do not receive a space ID as an output to the next cell, verify that you have created a deployment space and have provided the correct deployment space name. Do not proceed until this next cell runs successfully and returns the space_id.\n\n![lookup deployment space](doc/source/images/lookup_deployment_space.png)\n\nOnce you know the deployment space id, update the next cell with this id to set this deployment space as the default deployment space. Further down the notebook, when you deploy the model, it will be deployed to this default deployment space.\n\n```python\n# Now set the default space to the GUID for your deployment space. If this is successful, you will see a 'SUCCESS' message.\nclient.set.default_space('INSERT_SPACE_ID_HERE')\n```\n\n### 8. Run the notebook\n\n* Run the entire notebook using the menu `Cell \u25b7 Run All` or run the cells individually with the play button as shown here.\n\n![press_play.png](doc/source/images/press_play.png)\n\n### 9. Analyze the results\n\n#### When the notebook was created\n\n* A pod was instantiated \u2013 which means loading a complete compute Jupyter notebook environment (7+ GB) with all the artifacts from the ICP4D registry.\n* This pod is scheduled on any VM in your cluster \u2013 wherever CPU and memory resources are available.\n* IP addresses and connections are all configured automatically.\n* The same working environment can be used by multiple users. If a single pod's resources are not sufficient, another environment is created automatically.\n* When the number of users grow, you can add more machines to the ICP4D cluster and scheduling of resources is handled automatically.\n* ICP4D's scale-out model is pretty effective.\n* You no longer have to wait days or even weeks to get the compute resources.\n* More users can be accommodated with same compute capacity. As one task completes, its resources are freed up to work on next one.\n\n#### When you ran the notebook\n\n* During the execution of a cell in a Jupyter notebook, an asterisk `[*]` displays in the square bracket which changes to a sequence number when execution of that cell completes.\n\n* The mix of documentation, code, and output can make a Jupyter output self-explanatory. This also makes it a great environment to \"show your work\" if you have a hypothesis, do some analysis, and come up with a conclusion.\n\n<!--  TODO: describe and show more key output/analysis\n -->\n* Example Brunel chart:\n\n![churn_risk_chart.png](doc/source/images/churn_risk_chart.png)\n\n* The model was saved and deployed to the Watson Machine Learning service. Next, we will test the model in the UI. Later, we'll deploy the model for external use.\n\n![save_model.png](doc/source/images/save_model.png)\n\n#### Sample notebook output\n\nSee the notebook with example output [here](https://nbviewer.jupyter.org/github/IBM/icp4d-customer-churn-classifier/blob/master/examples/TradingCustomerChurnClassifierSparkML.ipynb).\n\n### 10. Test the model\n\nIBM Cloud Pak for Data provides various options for analytics models such as testing, scoring, evaluating, and publishing. \n\nWe can start testing using the built-in tooling.\n\n* Navigate to the left-hand (\u2630) hamburger menu and choose `Deployments`:\n\n![Analytics Analyze deployments](doc/source/images/ChooseAnalyticsDeployments.png)\n\n* On the `Spaces` tab, choose the deployment space you setup previously by clicking on the name of the space.\n\n![Deployment space](doc/source/images/deployment-space.png)\n\n* In your space overview, click the model name that you just built in the notebook.\n\n> **NOTE**: There may be more than one model listed in the 'Models' section. This can happen if you have run the Jupyter notebook more than once. Although you could select any of the models you see listed in the page, the recommendation is to start with whichever model is available that is using a spark-mllib_2.4 runtime.\n\n![select model](doc/source/images/deployment-select-model.png)\n\n* Click on the deployment that was created using the notebook. \n\n![select deployment](doc/source/images/deployment-select-deployment.png)\n\n* The Deployment *API reference* tab shows how to use the model using *cURL*, *Java*, *Javascript*, *Python*, and *Scala*. Click on the corresponding tab to get the code snippet in the language that you want to use:\n\n![Deployment API reference tab](doc/source/images/api-reference-curl-command.png)\n\n#### Test the saved model with built-in tooling\n\n* To get to the built-in test tool, click on the Test tab. You can now test the model by either providing the input data using a form, or by providing the input data as a JSON.\n\n> **NOTE**: Testing using JSON is enabled for this model because we have specified the input fields for the model during model creation in the notebook as shown below:\n\n![model input fields](doc/source/images/model-input-fields.png)\n\n* To test the model by providing data using the form, click on the `Provide input using form` icon and enter the following input in the form fields:\n\n  * `ID`: *4*\n  * `GENDER`: *F*\n  * `STATUS`: *M*\n  * `CHILDREN`: *2*\n  * `ESTINCOME`: *52004*\n  * `HOMEOWNER`: *N*\n  * `AGE`: *25*\n  * `TOTALDOLLARVALUETRADED`: *5030*\n  * `TOTALUNITSTRADED`: *23*,\n  * `LARGESTSINGLETRANSACTION`: *1257*\n  * `SMALLESTSINGLETRANSACTION`: *125*\n  * `PERCENTCHANGECALCULATION`: *3*\n  * `DAYSSINCELASTLOGIN`: *2*\n  * `DAYSSINCELASTTRADE`: *19*\n  * `NETREALIZEDGAINS_YTD`: *0*\n  * `NETREALIZEDLOSSES_YTD`: *251*\n\n* Click the `Predict` button  and the model will be called with the input data. The results will display in the *Result* window. Scroll down to the bottom (Line #110) to see either a \"High\", a \"Low\" or a \"Medium\" for Churn:\n\n![Testing the deployed model](doc/source/images/TestingDeployedModelUsingForm.png)\n\n* To test the model by providing an input JSON, click on the `Provide input data as JSON` icon and paste the following data under Body:\n\n```json\n{\n   \"input_data\":[\n      {\n         \"fields\":[\n            \"ID\", \n            \"GENDER\", \n            \"STATUS\",\n            \"CHILDREN\",\n            \"ESTINCOME\",\n            \"HOMEOWNER\",\n            \"AGE\",\n            \"TOTALDOLLARVALUETRADED\",\n            \"TOTALUNITSTRADED\",\n            \"LARGESTSINGLETRANSACTION\",\n            \"SMALLESTSINGLETRANSACTION\",\n            \"PERCENTCHANGECALCULATION\",\n            \"DAYSSINCELASTLOGIN\",\n            \"DAYSSINCELASTTRADE\",\n            \"NETREALIZEDGAINS_YTD\",\n            \"NETREALIZEDLOSSES_YTD\"\n         ],\n         \"values\":[\n            [\n               4,\n               \"F\",\n               \"M\",\n               2,\n               52004,\n               \"N\",\n               60,\n               5030,\n               23,\n               1257,\n               125,\n               3,\n               1,\n               1,\n               1000,\n               0\n            ]\n         ]\n      }\n   ]\n}\n```\n\n* Click the `Predict` button  and the model will be called with the input data. The results will display in the *Result* window. Scroll down to the bottom (Line #110) to see either a \"High\", a \"Low\" or a \"Medium\" for Churn:\n\n![Testing the deployed model](doc/source/images/TestingDeployedModelUsingJSON.png)\n\n#### Test the saved model using cURL\n\nNow that the model is deployed, we can also test it from external applications. One way to invoke the model API is using the cURL command.\n\n> NOTE: Windows users will need the *cURL* command. It is recommended to [download gitbash](https://gitforwindows.org/) for this, as you will also have other tools and you will be able to easily use the shell environment variables in the following steps. Also note that if you are not using gitbash, you may need to change *export* commands to *set* commands.\n\n* In a terminal window (or command prompt in Windows), run the following command to get a token to access the API. Use your Cloud Pak for Data cluster `URL`, `username` and `password`:\n\n```bash\ncurl -k -X GET https://<cluster-url>/v1/preauth/validateAuth -u <username>:<password>\n```\n\n* A json string will be returned with a value for \"accessToken\" that will look *similar* to this:\n\n```json\n{\"username\":\"snyk\",\"role\":\"Admin\",\"permissions\":[\"access_catalog\",\"administrator\",\"manage_catalog\",\"can_provision\"],\"sub\":\"snyk\",\"iss\":\"KNOXSSO\",\"aud\":\"DSX\",\"uid\":\"1000331002\",\"authenticator\":\"default\",\"accessToken\":\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6InNueWstYWRtaW4iLCJyb2xlIjoiQWRtaW4iLCJwZXJtaXNzaW9ucyI6WyJhZG1pbmlzdHJhdG9yIiwiY2FuX3Byb3Zpc2lvbiIsIm1hbmFnZV9jYXRhbG9nIiwibWFuYWdlX3F1YWxpdHkiLCJtYW5hZ2VfaW5mb3JtYXRpb25fYXNzZXRzIiwibWFuYWdlX2Rpc2NvdmVyeSIsIm1hbmFnZV9tZXRhZGF0YV9pbXBvcnQiLCJtYW5hZ2VfZ292ZXJuYW5jZV93b3JrZmxvdyIsIm1hbmFnZV9jYXRlZ29yaWVzIiwiYXV0aG9yX2dvdmVycmFuY2VfYXJ0aWZhY3RzIiwiYWNjZXNzX2NhdGFsb2ciLCJhY2Nlc3NfaW5mb3JtYXRpb25fYXNzZXRzIiwidmlld19xdWFsaXR5Iiwic2lnbl9pbl9vbmx5Il0sInN1YiI6InNueWstYWRtaW4iLCJpc3MiOiJLTk9YU1NPIiwiYXVkIjoiRFNYIiwidWlkIjoiMTAwMDMzMTAwMiIsImF1dGhlbnRpY2F0b3IiOiJkZWZhdWx0IiwiaWp0IjoxNTkyOTI3MjcxLCJleHAiOjE1OTI5NzA0MzV9.MExzML-45SAWhrAK6FQG5gKAYAseqdCpublw3-OpB5OsdKJ7whrqXonRpHE7N7afiwU0XNrylbWZYc8CXDP5oiTLF79zVX3LAWlgsf7_E2gwTQYGedTpmPOJgtk6YBSYIB7kHHMYSflfNSRzpF05JdRIacz7LNofsXAd94Xv9n1T-Rxio2TVQ4d91viN9kTZPTKGOluLYsRyMEtdN28yjn_cvjH_vg86IYUwVeQOSdI97GHLwmrGypT4WuiytXRoQiiNc-asFp4h1JwEYkU97ailr1unH8NAKZtwZ7-yy1BPDOLeaR5Sq6mYNIICyXHsnB_sAxRIL3lbBN87De4zAg\",\"_messageCode_\":\"success\",\"message\":\"success\"}\n```\n\n* Use the export command to save the \"accessToken\" part of this response in the terminal window to a variable called `WML_AUTH_TOKEN`. \n\n```bash\nexport WML_AUTH_TOKEN=<value-of-access-token>\n```\n\n* Back on the model deployment page, gather the `URL` to invoke the model from the *API reference* by copying the `Endpoint`, and export it to a variable called `URL`:\n\n![Model Deployment Endpoint](doc/source/images/api-reference-curl.png)\n\n```bash\nexport URL=https://blahblahblah.com\n```\n\nNow run this curl command from a terminal window to invoke the model with the same payload that was used previously:\n\n```bash\ncurl -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header \"Authorization: Bearer  $WML_AUTH_TOKEN\" -d '{\"input_data\":[{\"fields\":[\"ID\", \"GENDER\", \"STATUS\", \"CHILDREN\",  \"ESTINCOME\", \"HOMEOWNER\", \"AGE\", \"TOTALDOLLARVALUETRADED\", \"TOTALUNITSTRADED\", \"LARGESTSINGLETRANSACTION\", \"SMALLESTSINGLETRANSACTION\", \"PERCENTCHANGECALCULATION\", \"DAYSSINCELASTLOGIN\", \"DAYSSINCELASTTRADE\", \"NETREALIZEDGAINS_YTD\", \"NETREALIZEDLOSSES_YTD\"],\"values\":[[4, \"F\", \"M\", 2, 52004, \"N\", 60, 5030, 23, 1257, 125, 3, 1, 1, 1000, 0]]}]}' $URL\n```\n\nA json string similar to the one below will be returned with the response, including a \"High\", a \"Low\" or a \"Medium\" at the end indicating the risk of churn for this customer.\n\n```json\n{\"predictions\":[{\"fields\":[\"ID\",\"GENDER\",\"STATUS\",\"CHILDREN\",\"ESTINCOME\",\"HOMEOWNER\",\"AGE\",\"TOTALDOLLARVALUETRADED\",\"TOTALUNITSTRADED\",\"LARGESTSINGLETRANSACTION\",\"SMALLESTSINGLETRANSACTION\",\"PERCENTCHANGECALCULATION\",\"DAYSSINCELASTLOGIN\",\"DAYSSINCELASTTRADE\",\"NETREALIZEDGAINS_YTD\",\"NETREALIZEDLOSSES_YTD\",\"GENDERIndex\",\"GENDERclassVec\",\"STATUSIndex\",\"STATUSclassVec\",\"HOMEOWNERIndex\",\"HOMEOWNERclassVec\",\"features\",\"rawPrediction\",\"probability\",\"prediction\",\"predictedLabel\"],\"values\":[[4,\"F\",\"M\",2,52004,\"N\",60,5030,23,1257,125,3,1,1,1000,0,0.0,[1,[0],[1.0]],0.0,[2,[0],[1.0]],0.0,[1,[0],[1.0]],[1.0,1.0,0.0,1.0,4.0,2.0,52004.0,60.0,5030.0,23.0,1257.0,125.0,3.0,1.0,1.0,1000.0,0.0],[2.9466019417475726,8.67282872405483,8.380569334197599],[0.14733009708737863,0.4336414362027415,0.4190284667098799],1.0,\"Low\"]]}]}\n```\n\n### 11. Use the model in an app\n\nYou can also access the online model deployment directly through the REST API. This allows you to use your model for inference in any of your apps. For this code pattern, we'll be using a Python Flask application to collect information, score it against the model, and show the results.\n\n#### Install dependencies\n\n> **NOTE**: This application only runs on Python 3.6 and above, so the instructions here are for Python 3.6+ only.\n\nThe general recommendation for Python development is to use a virtual environment ([`venv`](https://docs.python.org/3/tutorial/venv.html)). To install and initialize a virtual environment, use the `venv` module:\n\nIn a terminal, go to the `stocktraderapp` folder within the cloned repo directory.\n\n```bash\ngit clone https://github.com/IBM/icp4d-customer-churn-classifier\ncd icp4d-customer-churn-classifier/stocktraderapp\n```\n\nInitialize a virtual environment with [`venv`](https://docs.python.org/3/tutorial/venv.html).\n\n```bash\n# Create the virtual environment using Python. \n# Note, it may be named python3 on your system.\npython -m venv venv       # Python 3.X\n\n# Source the virtual environment. Use one of the two commands depending on your OS.\nsource venv/bin/activate  # Mac or Linux\n./venv/Scripts/activate   # Windows PowerShell\n```\n> **TIP** To terminate the virtual environment use the `deactivate` command.\n\nFinally, install the Python requirements.\n\n```bash\npip install -r requirements.txt\n```\n\n#### Update environment variables\n\nIt is best practice to store configurable information as environment variables, instead of hard-coding any important information. To reference our model and supply an API key, we will pass these values in via a file that is read; the key-value pairs in this file are stored as environment variables.\n\nCopy the `env.sample` file to `.env`.\n\n```bash\ncp env.sample .env\n```\n\nEdit the .env file to provide the `URL` and `TOKEN`.\n\n* `URL` is your web service URL for scoring.\n* `TOKEN` is your deployment access token.\n\n```bash\n# Copy this file to .env.\n# Edit the .env file with the required settings before starting the app.\n\n# Required: Provide your web service URL for scoring.\n# E.g., URL=https://9.10.222.3:31843/dmodel/v1/project/pyscript/tag/score\n\nURL=\n\n# Required: Provide your web service deployment access token.\n# E.g., TOKEN=abCdwFghIjKLMnO1PqRsTuV2wWX3YzaBCDE4.fgH1r2... (and so on, tokens are long).\n\nTOKEN=\n\n# Optional: You can override the server's host and port here.\n\nHOST=0.0.0.0\nPORT=5000\n```\n\n#### Start the application\n\nStart the flask server by running the following command:\n\n```bash\npython StockTraderChurn.py\n```\n\nUse your browser to go to [http://0.0.0.0:5000](http://0.0.0.0:5000) and try it out.\n\n> **TIP**: Use `ctrl`+`c` to stop the Flask server when you are done.\n\n#### Sample Output\n\nEnter some sample values into the form:\n\n![sample_output_reset.png](doc/source/images/sample_output_reset.png)\n\nClick the `Submit` button and the churn prediction is returned:\n\n![sample_output.png](doc/source/images/sample_output.png)\n\nPressing `Reset` allows you to go back and enter new values.   \n\n### 12. (OPTIONAL) Use Db2 Warehouse to store customer data\n\nThis section provides an alternative to accessing a local csv file in your notebook. This requires that you have created a Db2 Warehouse database deployment in your IBM Cloud Pak for Data cluster or on IBM Cloud. With it, you can access the integrated database console to complete common tasks, such as loading data into the database.\n\nYou can follow the instructions provided in Steps 4, 5 and 6 of the [Virtualizing DB2 Warehouse data with data virtualization](https://developer.ibm.com/tutorials/virtualizing-db2-warehouse-data-with-data-virtualization/) tutorial to seed the DB2 warehouse (using the [mergedcustomers.csv](data/mergedcustomers.csv) file provided in this repo), obtain the connection details for your DB2 warehouse and use the connection details to add a connection to your IBM Cloud Pak for Data cluster.\n\n**IMPORTANT**: For this code pattern, remember to seed the DB2 warehouse with the `data/mergedcustomers.csv` file from your cloned repo and not the file mentioned in the *Virtualizing DB2 Warehouse data with data virtualization* tutorial.\n\n#### Add the data asset to your project\n\n* Go the (\u2630) menu and click *Projects* > *All projects*:\n\n![(\u2630) Menu -> Projects](doc/source/images/cpd-projects-menu.png)\n\n* Click on your project. On your project main page, click on `Add to project +` and select `Connection`:\n\n![add connection to project](doc/source/images/add-connection.png)\n\n* On the `From platform` tab, select the DB2 Warehouse connection that was added earlier:\n\n![add DB2 warehouse](doc/source/images/add-db2-warehouse.png)\n\n* Provide the username and password for the connection, then click `Test` to test the connection. Once you get a message that says `Connection test passed`, click `Add`:\n\n![add connection to project](doc/source/images/add-connection-to-project.png)\n\n* You should see the connection listed under *Data Assets* in your project's landing page. Click on `Add to project +` and choose `Connected data`:\n\n![add connected data](doc/source/images/add-connected-data.png)\n\n* Click on `Select source`:\n\n![add connected data - select source](doc/source/images/add-connected-data-select-source.png)\n\n* Select your *DB2 warehouse connection*, select the *schema*, and then select the *table* you had created when loading the file to the DB2 warehouse. Finally click `Select`:\n\n![add connected data - select table](doc/source/images/add-connected-data-select-table.png)\n\n* Provide a *name* and an optional *description* for this data asset and click `Create`:\n\n![add connected data - create](doc/source/images/add-connected-data-create.png)\n\n* The database table should now be visible in your project under *Data assets*:\n\n![add connected data - completed](doc/source/images/add-connected-data-completed.png)\n\n#### Complete the code pattern\n\nFollow the remaining instructions above starting from [3. Create a Space for Machine Learning Deployments](#3-create-a-space-for-machine-learning-deployments). When adding the pandas dataFrame in your notebook, choose the asset name that you had provided when adding the connected data to your project.\n\n## License\n\nThis code pattern is licensed under the Apache License, Version 2. Separate third-party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the [Developer Certificate of Origin, Version 1.1](https://developercertificate.org/) and the [Apache License, Version 2](https://www.apache.org/licenses/LICENSE-2.0.txt).\n\n[Apache License FAQ](https://www.apache.org/foundation/license-faq.html#WhatDoesItMEAN)\n"}, {"repo": "kiwicommerce/magento2-customer-password", "language": "PHP", "readme_contents": "# We're not maintaining this extension, if you need any support please contact us at hello@kiwicommerce.co.uk\n"}, {"repo": "MicrosoftLearning/MB-230-Dynamics365forCustomerService", "language": null, "readme_contents": "# MB-230 - Dynamics 365 for Customer Service\n\n## Omnichannel Lab Notice: 10/15/2021\nDue to an issue with the tenant, it is not possible to perform labs Omnichannel for Customer Service in the lab environment. However, there are new labs for Unified routing, Customer Service workspace, and App profile manager. If MCTs would still like access to the Omnichannel labs to demo in their own environment, they can be found in the branch of this GitHub repository titled **MasterBackup_OC.**\n\n- **[Download Latest Student Handbook and AllFiles Content](../../releases/latest)**\n- **Are you a MCT?** - Have a look at our [GitHub User Guide for MCTs](https://microsoftlearning.github.io/MCT-User-Guide/)\n- **Need to manually build the lab instructions?** - Instructions are available in the [MicrosoftLearning/Docker-Build](https://github.com/MicrosoftLearning/Docker-Build) repository\n\n## What are we doing?\n\n- To support this course, we will need to make frequent updates to the course content to keep it current with the Azure services used in the course.  We are publishing the lab instructions and lab files on GitHub to allow for open contributions between the course authors and MCTs to keep the content current with changes in the Azure platform.\n\n- We hope that this brings a sense of collaboration to the labs like we've never had before - when Azure changes and you find it first during a live delivery, go ahead and make an enhancement right in the lab source.  Help your fellow MCTs.\n\n## How should I use these files relative to the released MOC files?\n\n- The instructor handbook and PowerPoints are still going to be your primary source for teaching the course content.\n- These files on GitHub are designed to be used in conjunction with the student handbook, but are in GitHub as a central repository so MCTs and course authors can have a shared source for the latest lab files.\n- It will be recommended that for every delivery, trainers check GitHub for any changes that may have been made to support the latest Azure services, and get the latest files for their delivery.\n\n## What about changes to the student handbook?\n\n- We will review the student handbook on a quarterly basis and update through the normal MOC release channels as needed.\n\n## How do I contribute?\n\n- Any MCT can submit a pull request to the code or content in the GitHub repro, Microsoft and the course author will triage and include content and lab code changes as needed.\n- You can submit bugs, changes, improvement and ideas.\n\n## Notes\n\n### Classroom Materials\n\nIt is strongly recommended that MCTs and Partners access these materials and in turn, provide them separately to students. Pointing students directly to GitHub to access Lab steps as part of an ongoing class will require them to access yet another UI as part of the course, contributing to a confusing experience for the student. An explanation to the student regarding why they are receiving separate Lab instructions can highlight the nature of an always-changing cloud-based interface and platform. Microsoft Learning support for accessing files on GitHub and support for navigation of the GitHub site is limited to MCTs teaching this course only.\n"}, {"repo": "josdejong/customerservice", "language": "JavaScript", "readme_contents": "# Customer Service\n\nCustomer Service is a demo showing the ease, power, and flexibility of\na web application build with\n[AngularJS](http://angularjs.org/) for the user interface and\n[CouchDB](http://couchdb.apache.org/) as database.\nThe demo consists of a basic customer service application.\nOne can create, edit, and delete customers.\nFor each customer, one can create, edit and delete reports.\nThe customers can be searched by name and by last updated.\n\nThe complete application consists of a single file:\n[index.html](https://github.com/josdejong/customerservice/blob/master/index.html).\nThe file is only a few hundred lines of code, including everything:\ninterface, controller, database, search queries.\n(Of course, for a serious application one should separate html, css, and javascript.)\n\n<img src=\"https://raw.github.com/josdejong/customerservice/master/screenshot.png\">\n\n\n## Installation\n\nTo install the demo (on Ubuntu):\n\n- install CouchDB via the command\n\n  `sudo apt-get install CouchDB`\n\n- open the web interface in your browser at http://localhost:5984/_utils\n- create a database, for example named \"service\"\n- in this database, create a document with id with an easy to remember\n  id, for example \"app\", and save it.\n- in the document with id \"app\", add the file \"index.html\" as attachment\n- run the application in your browser via\n\n  `http://localhost:5984/service/app/index.html`\n\n\nInstead of installing CouchDB locally, one can create an account at a hosting\nservice for couchdb, such as [Cloudant](https://cloudant.com/).\n\n\n## References\n\n- AngularJS: http://angularjs.org/\n- CouchDB: http://couchdb.apache.org/\n"}, {"repo": "SyncfusionSuccinctlyE-Books/Customer-Success-for-CSharp-Developers-Succinctly", "language": "C#", "readme_contents": "# Customer Success for C# Developers Succinctly\nThis is the companion repo for [*Customer Success for C# Developers Succinctly*](https://www.syncfusion.com/ebooks/customer_success_for_c_sharp_developers) by Ed Freitas. Published by Syncfusion.\n\n[![cover](https://github.com/SyncfusionSuccinctlyE-Books/Customer-Success-for-CSharp-Developers-Succinctly/blob/master/cover.png)](https://www.syncfusion.com/ebooks/customer_success_for_c_sharp_developers)\n\n## Looking for more _Succinctly_ titles?\n\nCheck out the entire library of more than 130 _Succinctly_ e-books at [https://www.syncfusion.com/ebooks](https://www.syncfusion.com/ebooks).\n"}, {"repo": "lafranceinsoumise/woocommerce-customer-order-csv-export", "language": "PHP", "readme_contents": "=== WooCommerce Customer/Order CSV Export ===\nAuthor: skyverge\nTags: woocommerce\nRequires at least: 4.4\nTested up to: 5.2.2\nRequires PHP: 5.6\n\nEasily download customers & orders in CSV format and automatically export FTP or HTTP POST on a recurring schedule\n\nSee https://docs.woocommerce.com/document/ordercustomer-csv-exporter/ for full documentation.\n\n== Installation ==\n\n1. Upload the entire 'woocommerce-customer-order-csv-export' folder to the '/wp-content/plugins/' directory\n2. Activate the plugin through the 'Plugins' menu in WordPress\n"}, {"repo": "php-cuong/magento2-redirect-customer", "language": "PHP", "readme_contents": "# Magento 2 Redirect the customer to a custom page after logging in\nBy default, Magento 2 provides us with a feature, that allows redirecting the customer to account dashboard after logging in, you can enable this feature by going to Stores \u2192 Settings \u2192 Configuration \u2192 Customers \u2192 Customer Configuration \u2192 Login Options\n\nSet the field named \"Redirect Customer to Account Dashboard after Logging in\" to Yes\n\nThis feature is working perfectly, however, Magento 2 doesn't support to redirect the customer to a particular page after logging in, if your project requires to use this feature, today I show you the best codes to complete your task.\n\nSo what will we do in this practice?\n\n1. We will create a new module called PHPCuong_CustomerRedirecting\n2. We will create an additional field saves the custom page in the configuration\n3. We will use the event named customer_login to redirect the customer to that custom page after logging in successfully\n\nBefore doing this practice, you need to understand, How to use events and observers in Magento 2, if you don't watch the video about this lesson yet, you can watch it here http://bit.ly/2QwDfpL\n\n## Step 1: Declaring the new module called PHPCuong_CustomerRedirecting\n- Create the namespace PHPCuong in the path app\\code\n- Create the module named CustomerRedirecting in the path app\\code\\PHPCuong\n- Create the file named registration.php in the path app\\code\\PHPCuong\\CustomerRedirecting\n- Create the file named module.xml in the path app\\code\\PHPCuong\\CustomerRedirecting\\etc\n\n## Step 2: Create an additional field saves the custom page in the configuration\n- Create the new file named system.xml in the path app\\code\\PHPCuong\\CustomerRedirecting\\etc\\adminhtml\n\n## Step 3: Subscribing to the event named customer_login\n- Create the new file named events.xml in the path app\\code\\PHPCuong\\CustomerRedirecting\\etc\\frontend\n- Create the new file named CustomerLogin.php in the path app\\code\\PHPCuong\\CustomerRedirecting\\Observer\n\n## Step 4: Test and see the results\n1. Run the following command lines:\nphp bin/magento setup:upgrade --keep-generated\n\n2. Test the results\nGo to the Magento Admin Panel \u2192 Stores \u2192 Settings \u2192 Configuration \u2192 Customers \u2192 Customer Configuration \u2192 Login Options\nSet the custom page.\n\n## See the video about this tutorial\n1. Youtube: https://www.youtube.com/watch?v=oyPD3Jy17EA&list=PL98CDCbI3TNvPczWSOnpaMoyxVISLVzYQ&index=45\n2. Facebook: https://www.facebook.com/giaphugroupcom/videos/457967798069780/\n"}, {"repo": "pfortin-expertime/MageFix-Customer", "language": "PHP", "readme_contents": "# MageFix-Customer\n\nIn Magento 2 you can't (for the moment) watch what your customers have in their shopping carts.\nThis module provides a fix to do it.\n\n\nInstallation\n--\nAs usual for Magento 2 modules\n\n\nVersion\n--\n0.0.3 - Fixes a fatal error that occurs when products have options in the cart\n\n0.0.2 - Allow products deletion\n\n0.0.1 - Initial commit\n\n\n"}, {"repo": "cereniyim/Customer-Segmentation-Unsupervised-ML-Model", "language": "Jupyter Notebook", "readme_contents": "In this procject, I worked on a real-world e-commerce sales data and segmented its customer base with Python, [scikit-learn library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [Plotly](https://plot.ly/python/plotly-express/). \n\nThe workflow of the project is as follows:\n\n1. Business Case\n2. Data Preparation\n3. Segmentation with K-means Clustering\n4. Hyperparameter Tuning\n5. Visualization and Interpretation of the Results\n\nArticle of the project \"Customer Segmentation with Machine Learning\" can be read [here](https://towardsdatascience.com/customer-segmentation-with-machine-learning-a0ac8c3d4d84?source=friends_link&sk=91a45f28699eda78766335947bed7044)!\n\nThe notebook can be viewed in the nbviewer [here](https://nbviewer.jupyter.org/github/cereniyim/Customer-Segmentation-Unsupervised-ML-Model/blob/3c4374dd16861ea365cdf468bd9b2c28a964f4e3/Customer_Segmentation_Kmeans_Clustering.ipynb) with interactive plotly plots.\n"}, {"repo": "b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking", "language": "Jupyter Notebook", "readme_contents": "# Understanding Customer Conversion <br> with Snowplow Web Event Tracking <br> <sub> Benjamin S. Knight, January 27th 2017 </sub>\n\n### Project Overview\nHere I apply machine learning techniques to Snowplow web event data to infer whether trial account holders will become paying customers based on their history of visiting the marketing site. By predicting which trial account holders have the greatest likelihood of adding a credit card and converting to paying customers, we can more efficiently deploy scarce Sales Department resources. \n\n[Snowplow](http://snowplowanalytics.com/) is a web event tracker capable of handling tens of millions of events per day. The Snowplow data contains far more detail than the [MSNBC.com Anonymous Web Data Set](https://archive.ics.uci.edu/ml/datasets/MSNBC.com+Anonymous+Web+Data) hosted by the University of California, Irvine\u2019s Machine Learning Repository. At the same time, we do not have access to demographic data as was the case with the [Event Recommendation Engine Challenge](https://www.kaggle.com/c/event-recommendation-engine-challenge) hosted by [Kaggle](https://www.kaggle.com/). Given the origin of the data, there is no industry-standard benchmark for model performance. Rather, assessing baseline feasibility is a key objective of this project. \n\n### Problem Statement\nTo what extent can we infer a visitor\u2019s likelihood of becoming a paying customer based upon that visitor\u2019s activity history on the company marketing site? We are essentially confronted with a binary classification problem. Will the trial account in question add a credit card (cc_date_added IS NOT NULL \u2018yes\u2019/\u2018no\u2019)? This labeling information is contained in the \u2018cc\u2019 column within the file \u2018munged df.csv.\u2019 \n\nThere is no clear precedent for how effective a model our analysis may ultimately yield, and so a key component of the project is ultimately assessing feasibility of using visitor history on the marketing site to predict conversion. Regarding the most appropriate model, there is no way of knowing which family of algorithms will prove to be most effective in predicting customer conversion. With this in mind, we adopt an \"all of the above\" approach - applying all algorithms that are feasible given the nature of the classification problem and the structure of the data (see the section entitled 'Algorithms and Techniques' for additional details). That being said, certain families of algorithms are more promising than others (at least initially). Based on findings from [Wainer (2016)](https://arxiv.org/pdf/1606.00930v1.pdf), we predict that a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel is most likely to yield the best fit. \n\n### Metrics\nAs we discuss later, the data is highly imbalanced (successful customer conversions average 6%). Thus, we are effectively searching a haystack for rare, but exceeedingly valuable needles. In more technical terms, we want to maximize recall as our first priority. Selecting the model that maximizes precision is a subsequent priority. To this end, our primary metric is the F2 score shown below.\n<div align=\"center\">\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/F2_Score_Equation.png\" align=\"middle\" width=\"453\" height=\"113\" />\n</div>\n\nThe F2 score is derived from the [F1 score](https://en.wikipedia.org/wiki/F1_score) by setting the weight of the \\beta parameter to 2, effectively increasing the penalty for false negatives. While the F2 score is the arbiter for ultimate model selection, we also use [precision-recall curves](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) to clarify model performance. We have opted for precision-recall curves as opposed to the more conventional [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (ROC) curve due to the highly imbalanced nature of the data [(Saito, 2016)](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432).\n\n### Data Preprocessing \nThe raw Snowplow data available is approximately 15 gigabytes spanning over 300 variables and tens of millions of events from November 2015 to January 2017. When we omit fields that are not in active use, are redundant, contain personal identifiable information (P.I.I.), or which cannot have any conceivable bearing on customer conversion, then we are left with 14.6 million events spread across 22 variables. \n\n<p align=\"center\"><b>Table 1: Selected Snowplow Variables Prior to Preprocessing</b></p>\n\n<sub>Snowplow Variable Name</sub>   | <sub>Snowplow Variable Description</sub>                                         \n---------------------------------- | ---------------------------------------------------------------------------------\n<sub>event_id</sub>              | <sub>The unique Snowplow event identifier</sub>                                 \n<sub>account_id</sub>            | <sub>The account number if an account is associated with the domain userid</sub>\n<sub>reg_date</sub>              | <sub>The date an account was registered </sub>                                  \n| <sub>*cc_date_added*</sub>    | <sub>The date a credit card was added </sub>                                                   |\n| <sub>*collector_tstamp*</sub> | <sub>The timestamp (in UTC) when the Snowplow collector first recorded the event </sub>          |\n| <sub>*domain_userid*</sub>    | <sub>This corresponds to a Snowplow cookie and will tend to correspond to a single internet device</sub> |\n| <sub>*domain_sessionidx*</sub>     | <sub>The number of sessions to date that the domain userid has been tracked</sub>                |\n| <sub>*domain_sessionid*</sub>      | <sub>The unique identifier for the Snowplow cookie/session</sub>                                 |\n| <sub>*event_name*</sub>            | <sub>The type of event recorded</sub>                                                            |\n| <sub>*geo_country*</sub>           | <sub>The ISO 3166-1 code for the country that the visitor\u2019s IP address is located</sub>          |\n| <sub>*geo_region_name*</sub>       | <sub>The ISO-3166-2 code for country region that the visitor\u2019s IP address is in</sub>            |\n| <sub>*geo_city*</sub>              | <sub>The city the visitor\u2019s IP address is in</sub>                                               |\n| <sub>*page_url*</sub>              | <sub>The page URL</sub>                                                                          |\n| <sub>*page_referrer*</sub>         | <sub>The URL of the referrer (previous page)</sub>                                               |\n| <sub>*mkt_medium*</sub>            | <sub>The type of traffic source (e.g. \u2019cpc\u2019, \u2019affiliate\u2019, \u2019organic\u2019, \u2019social\u2019)</sub>             |\n| <sub>*mkt_source*</sub>            | <sub>The company / website where the traffic came from (e.g. \u2019Google\u2019, \u2019Facebook\u2019)</sub>         |\n| <sub>*se_category*</sub>           | <sub>The event type</sub>                                                                        |\n| <sub>*se_action*</sub>             | <sub>The action performed / event name (e.g. \u2019add-to-basket\u2019, \u2019play-video\u2019)</sub>                |\n| <sub>*br_name*</sub>               | <sub>The name of the visitor\u2019s browser</sub>                                                     |\n| <sub>*os_name*</sub>               | <sub>The name of the vistor\u2019s operating system</sub>                                             |\n| <sub>*os_timezone*</sub>           | <sub>The client\u2019s operating system timezone</sub>                                                |\n| <sub>*dvce_ismobile*</sub>         | <sub>Is the device mobile? (1 = \u2019yes\u2019)</sub>                                                     |\n\nI use the phrase 'variable' as opposed to 'feature', since this dataset will need to undergo substantial transformation before we can employ any supervised learning technique. Each row has an 'event_id' along with an 'event_name' and a \u2018page url.\u2019 The event id is the row\u2019s unique identifier, the event name is the type of event, and the page url is the URL within the marketing site where the event took place.\n\nThe distillation of the raw data into a transformed feature set with labels is handled by the iPython notebook 'Notebook 1 - Data Munging.' In transforming the data, we will need to create features by creating combinations of event types and distinct URLs, and counting the number of occurrences while grouping on accounts. For instance, if \u2018.../pay-ment plan.com\u2019 is a frequent page url, then the number of page views on payment plan.com would be one feature, the number of page pings would be another, as would the number of web forms submitted, and so forth. Given that there are six distinct event types and dozens of URLs within the marketing site, then the feature space quickly expands to encompass hundreds of features. This feature space will only widen as we add additional variables to the mix including geo region, number of visitors per account, and so forth.\n<p align=\"center\"><b>Figure 1: Management of Original Categorical Variables into Features </b></p>\n<div>\n<div align=\"center\">\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/Data_Transformation.png\" align=\"middle\" width=\"565\" height=\"368\" />\n</div>\n</div>\n\n\nWith the raw data transformed, our observations are no longer individual events but indivual accounts spanning the period November 2015 to January 2017. Our data set has 16,607 accounts and 581 features. 290 of these represent counts of various combinations of web events and URLs grouped by account. Next there are two aggregated features - the total number of\ndistinct cookies associated with the account, and the sum total of all Internet sessions linked to that account.There are also 151 features that represent counts of page view events linked to IP addresses within a certain country (e.g. a count of page views from China, a count of page views from France, and so forth). <br>\n\n46 of the features represent counts of page views coming from a specific marketing medium ('mkt medium'). Recall that 'mkt medium' is the type of traffic. Examples include \u2018partner link,' 'adroll,' or 'appstore.' The \u2018mkt medium\u2019 subset of features is followed by 86 features that correspond to Snowplow\u2019s 'mkt source' field. 'mkt source' designates the company / website where the traffic came from. Examples from this subset of the feature space include counts of page views from Google.com ('mkt source google com') and Squarespace ('mkt source square'). There are two additional feature:'mobile pageviews' and 'non-mobile pageviews' that represent counts of page views that took place on mobile versus non mobile devices. I have also included an additional feature derived from these two - the share of page views that took place on a mobile device.<br>\n\nWith the aggregations completed, we then take the transformed data and drop all features that are uniformly zero for all observations. Finally, we scale the features using [robust scaling](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html). \n\nIt bears noting that 'br_name' (the name of the visitor\u2019s browser), 'os_name' (the name of the vistor\u2019s operating system), and 'os_timezone' (the client\u2019s operating system timezone) were not included in the ultimate version of the transformed data. The transformed variables of 'br_name' and 'os_name' were used initially. However, their incorporation added +40 features to the already expansive feature space resulting in inferior performance and so were subsequently dropped.<br>\n\n### Data Exploration\nExploring the transformed data, two features quickly become apparent. First, we can see that the data is highly imbalanced. Only approximately 6% of the labeled accounts show a succesful conversion to paying customer. \n\n<div>\n<div align=\"center\">\n<p align=\"center\"><b>Figure 2: Summary Statistics - Distribution of Labels (16,607 Observations)</b></p>\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/exploratory_analysis-labels.png\" align=\"middle\" width=\"600\" height=\"225\" />\n</div>\n</div>\n\nThe second feature of note is that in addition to our feature space being wide with over 500 features, the features themselves are fairly sparse as the histograms below make clear. This is to be expected. The Snowpow features are highly specific. Examples include counts of certain types of events localized within Bangladesh, or the number page views associated with a bit of on-line content that was only made briefly available. As a result, the majority of features are extremely sparse.       \n\n<div align=\"center\">\n<p align=\"center\"><b>Figure 3: Summary Statistics - Means and Standard Deviations of Sparse Feature Space (581 Features)</b></p>\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/exploratory_analysis-feature_means.png\" align=\"middle\" width=\"528\" height=\"198\" />\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/exploratory_analysis-feature_sds.png\" align=\"middle\" width=\"528\" height=\"198\" />\n</div>\n\n### Benchmark \nHow do we know if our ultimate model is any good? To establish a baseline of model performance, I implement a [K-Nearest Neighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) model within the iPython notebook 'Notebook 3 - KNN (Baseline).' In the same manner as the subsequent model selection, I allocate 90% of the data for training (14,946 observations) and 10% for model testing (1,661 observations). I use the model's default setting of 5 neighbors. I run the resulting model on the test data using 100-fold cross validation. Averaging the 100 resultant F2 scores, we thus establish a benchmark model performance of F2 = 0.04.\n\n### Algorithms and Techniques\nWe start our analysis with establishing a benchmark using K-Nearest Neighbors (KNN) before moving on to more sophisticated algorithms. The KNN classifer works by selecting the target observation's n closest neighbors. The target observation is then classifed as being a member of the same class as the majority class within the n-sample. We use Sci-Kit Learn's \n[KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) which defaults to n = 5. Regarding what qualifies as a 'neighbor,' the KNeighborsClassifier uses the [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance), which at its default setttings is effectively the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance).\n\nLike KNN, logistic regression is computationally inexpensive - a definite strength given the size of the data set (n = 16,607). In addition, logistic regression is uniquely well-suited to the binary nature of the outcome variable. Sci-Kit Learn's [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier works through [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation). Through many iterations, the algorithm determines what sequence of weights will, when applied to our 581 features, maximize the likelihood that the pattern of successes and failures seen in the training set will emerge. An added strength of the classifier (of potential interest to future, more in-depth analysis) is that it generates the 'coef_' attribute - a vector of coefficients that can indicate which features are the most useful predictors of the outcome variable. \n\nThe second and third algorithms selected are Support Vector Machines (SVM) - the first with a [Radial Basis Function](Radial basis function kernel) (RBF) kernel and the other using a linear function. SVM works by placing multiple hyperplane (support vectors) through the data. The set of hyperplanes that maximizes the distance between the classes is then selected, with the center of the space delimited by the hyperplanes becoming the threshold for classification. \n\n<div>\n<div align=\"center\">\n<p align=\"center\"><b>Figure 4: A System of Linear Support Vector Machines Finding the Optimal Separation Between Two Classes</b></p>\n<p>Source:<a href=\"https://commons.wikimedia.org/wiki/File%3ASvm_max_sep_hyperplane_with_margin.png\"> Wikipedia</a></p>\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/SVM_example_image.png\" align=\"middle\" width=\"360\" height=\"388\" />\n</div>\n</div>\n\nFor the SVM + RBF kernel model, we use Sci-Kit Learn's [SVM](http://scikit-learn.org/stable/modules/svm.html) functionality. The RBF kernel works by creating an additional dimension of information derived fom the squared Euclidean distance of one point vis-a-vis all of the other points. Models using SVM with RBF kernels tend to be perform well with binary data [(Wainer, 2016)](https://arxiv.org/pdf/1606.00930v1.pdf), but are far less expensive than random forest models. \n\nA strength of RBF kernels is that they can accomodate data that is not [linearly separable](https://en.wikipedia.org/wiki/Linear_separability). However, if our data is already linearly separable, then such an approach is not just expensive - it may lead to over-fitting (Webb, 2002, p.138). For this reason, we also use a linear SVM. Linear SVM assumes that the data is linearly separable, and as a result of this assumption, is relatively inexpensive. Linear SVM is also well-suited for the high dimensionality of our data set (581 features).\n\n### Implementation\n##### Quantifying Success\nOne of the edifying aspects of this project was the refinement of the success metric. Given that the task is at its core, a binary classification problem, the initial metric intended was the area under the curve (AUC) as delimited by the receiver operating characteristic [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). The ROC AUC nicely captures the models' efficacy in terms of the rate of true postives versus false positives. However, the data is highly imbalanced, with only 6% of the customers sucessfully converting to paying accounts. Thus, the challange is less about maximizing the number of true positives relative to the number of false postives, but rather maximizing the share of customer conversions sucessfully capatured while at the same time ensuring the greatest possible precision.     \n\nIn more technical terms, recall - not precision - became the metric of highest priority. With this in mind, we discarded the ROC curve in favor of a precision-recall curve. A precision-recall curve plots the average precision (the Y-axis) over a given threshold of recall (the X-axis). From this, we could more meaningfully gauge the trade-off between maximizing recall versus precision. However, this new metric also proved to be inadequate. Speaking to the Sales Department prompted us to narrow the success metric even further. Capturing the majority of customer conversion events was the highest priority, and if a member of the Sales Department had to make due with a less accurate model, then so be it. Thus, seeing things from the end user perspective, we settled on the final success metric - the F2 score.   \n\n##### Transforming the Data\nWith the success metric in hand, the project begins in earnest with transforming the raw, categorical event data into quantitative account-level data. This task is handled by 'Notebook 1 - Data Munging.' After reading in the necessary packages and the data itself, we re-cast the most important fields in order to support the necessary filtering and grouping functions. \n\nCreating a trial account is a prerequisite for becoming a paying customer, and so we start by filtering out visitors that have never created a trial account. We then drop those events that occured after that account's conversion event (cc_date_added). In this way, we confirm that marketing site activity is a predictor of conversion to paying customer and not vice versa. After filtering, our data set is considerably smaller - just shy of two million events.  \n\nThe next step of data preparation is more labor-intensive. When a visitor visits the site for the first time (e.g. the pricing page), then the URL tracked by Snowplow will read '*https:<i></i>//www<i></i>.company-name.<i></i>com/pricing/*.' However, if that visitor is coming via an advertisement - say Google Adwords - then a [Urchin Traffic Monitor](https://en.wikipedia.org/wiki/UTM_parameters) (UTM) parameter will come into play, inserting itself into the URL. Thus, instead of '*https:<i></i>//www<i></i>.company-name.<i></i>com/pricing/*,' Snowplow will see '*https:<i></i>//www<i></i>.company-name.<i></i>com/pricing/utm_source=google*.' To further complicate matters, when a trial account is created a subdomain is added to the URL. For example, if Acme Inc. created a trial account and subsequently visited the pricing page, then the recorded URL would appear as '*https:<i></i>//www<i></i>.company-name/acme_inc.<i></i>com/pricing/*.'\n\nWe could continue, but the core of the problem is that there is not a 1:1 mapping of distinct URLs to distinct locations within the marketing site. To enable any meaningful analysis, UTM parameters, subdomains, and other substrings will need to be removed from Snowplow's recorded URLs. To simplfy matters further, we drop all prefixes including '*www*,' '*https*,' and so forth.  \n\nAs we proceed to distill marketing site pages and content from the URLS, it becomes clear that there remains an exceedingly large number of distinct URLs (29,245). Our next step is creating features from combinations of URLs and event types, and at this stage we are on track for a feature space ranging into the hundreds of thousands. Fortunately, the overwhelming majority of marketing site activity is captured within a hundred URLs, and so we make the strategic decision to eliminate all but those URLs from the data set. \n\nIn the next stage of data transformation, we create new variables from every combination of event type and event location. Snowplow utilizes six different types of events: 'page_view', 'event', 'link_click', 'change_form', 'submit_form', and 'page_ping.' When complete, we should have six hundred new features. We start with, we use the 'get_dummies' function from the [Pandas](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) package to turn event_type into six boolean variables. Starting with 'page_view,' we then create six new data objects for each event type. These data objects are event aggregations - sums of the new boolean variables while grouping on account_id and page_url. We then join these objects onto the original data set by mapping on the account_id. Following a similiar process, we create aggregations for the number of distinct visitors (cookies), as well as the number of sessions.\n\nWe implement a similiar process with country codes. There are over 200 distinct country codes in the data set, and if we counted every distinct combination of country : event_type (e.g. page pings from Kenya, form submissions from Estonia), then the feature space would increase untenably. Here we make another strategic decision to only include the aggregation of page views from a given country. \n\nThe next categorical variable to be transformed is 'mkt_medium.' This variable denotes the type of traffic. For example 'cpc' represents cost-per-click, i.e. paid advertising. The medium 'affiliate' represents a visitor coming from a partner website (we can confirm because of the presence of that partner's UTM parameter). 'Organic' represents vistors who come to the marketing site without any external prodding - as in they are typing in the company's name in a search bar and clicking the results. Lastly, Snowplow categorizes visitors coming from [LinkedIn](https://www.linkedin.com), [Facebook](https://www.facebook.com), or other social networks as 'social.' Just as with the country codes before, we only include the aggregations of page views, ignoring the other event types. \n\nSimiliar to 'mkt_medium' is 'mkt_source.' The key difference is that 'mkt_source' represents a specific URL (e.g. Google, Adroll, etc.). After creating and aggregating the boolean variables, we are left with 42 features from the 'mkt_medium' variable. Here a complication arises. The fact that 'mkt_source' is a portion of a URL means that the newly created column headers often contain elements (e.g. spaces) that can interfere with Panda's functionality. To prevent errors, we relabel the columns, dropping odd characters and utilizing underscores. \n\nThe final feature transformation is the 'dvce_ismobile' variable. We create a feature representing the count of page views taking place via a mobile device, a count of page views from non-mobile devices, and a feature detailing the percentage of an account's recorded page views to take place on a mobile device.  \n\nWith feature transformation complete, we create a vector of labels - 'cc' - using conditional logic (is there an associated date when a credit card was added? yes/no). Fortunately, the NULL values in the data set are not an indication of missing data, but rather represent events with counts equal to zero. To prevent errors, we zero fill the data set using the *fillna()* command. \n\nAs a final touch, we drop all columns which contain no useful information (i.e. columns only consisting of zeros). In this fashion we condense 14.6 million events into a working data set of 16,607 observations and 581 features.\n\n##### Exploring the Transformed Data\nBy using 'Notebook 2 - Exploratory Analysis,' we can get a sense of the scope and structure of the transformed data. After reading in the data and necessary packages, we print such essential information as the number of observations and the number of features. This iPython notebook also creates the visualizations used in the **Data Exploration** sections, including a horizonatal barchart illustrating the imbalanaced nature of the data, as well as histograms of the features' means and standard deviations. \n\n##### Establishing a Benchmark\nIt is at this stage that we employ machine learning - our first task being to establish a benchmark of model performance. A detailed discussion of why we chose the algorithms we did is available in the section entitled **Algorithms and Techniques**. To create this benchmark, we use 'Notebook 3 - KNN (Baseline).' After reading in the transformed data and packages, we subset the features into the X_all object, and the labels into the y_all object. Bias can be a potential issue if the features' variance varies significantly. To prevent this, we rescale the features using Scikit-Learn's [RobustScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) function. This function removes the median and re-scales the data according to the interquartile range - the range between the 1st and 3rd quartiles. This rescaling technique tends to be more robust to outliers compared to simply subtracting the mean and dividing by the standard deviation.\n\nWith the features rescaled, we then split the data into training and testing sets using the [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. For the purposes of replicability, we set the 'random_state' argument to *random_state=1*. Originally, the models were implemented using a 80%:20% split between training and testing data. However, experimenting with the various models showed that increasing the size of the training data set significantly improved model performance. Ultimately, all models were run using a 90%:10% split between training (14,946 observations) and testing (1,661 observations). \n\nSampling the data into training and testing sets is complicated by the highly imbalanced nature of the data. More often than not, the ratio of converted customers to non-converted customers should be comparable between the training and testing sets. However, with relative few members in the 'converted' class, there is always the possibility of unequal sampling and subsequent bias. To address this concern, I use stratified sampling on the dependent variable - 'cc.' \n\nStratified sampling works by aggregating observations on the class variable. In effect, instead of randomly taking 90% of the data for the training set, the computer takes 90% of the data where cc = 0, takes 90% of the data where cc = 1, and then combines the results into our training set. With this approach, we can ensure that our training and test sets do not vary in their composition of classes. \n\n<div>\n<div align=\"center\">\n<p align=\"center\"><b>Figure 5: Stratified Sampling</b></p>\n<p>Source:<a href=\"http://www.six-sigma-material.com/Samples.html\"> Six Sigma Material</a></p>\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/StratifiedSampling.GIF\" align=\"middle\" width=\"482\" height=\"164\" />\n</div>\n</div>\n\nWith the data partitioned, we then use [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) to create a classifer, and apply it to the training data. With our model fit, we then use Scikit-Learn's [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to score the model's performance when applied to the test data. [Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) works by taking *k* subsets of the data, deriving a metric of interest from each subset, and then averaging the results. To guard against anomalous results, we use 100 subsets for our cross-validation.\n\nAs for our \"metric of interest,\" recall that our success metric is the F2 score. To instruct the *cross_val_score* function to derive the F2 score, we use the [make_scorer](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) function with 'fbeta_score' as the metric, making sure to set the beta argument to *beta=2*. With our scorer in hand, we derive 100 F2 score and average them together for our benchmark (F2 = 0.04). \n\n##### Applying More Sophisticated Models\nBy this point, our basic procedure is established: \n* (1.) Read in the data\n* (2.) Create separate data objects for the features and labels\n* (3.) Rescale the data\n* (4.) Create testing and training sets with a 90% to 10%, making sure to use stratified sampling on the labels\n* (5.) Create the classifier and train it on the training set\n* (6.) Use the newly trained classifer on the testing data set\n* (7.) Derive the F2 score one hundred times and take the mean result, repeat for recall and precision\n\nWe repeat this workflow in the iPython notebooks 'Notebook 4 - SVM with RBF Kernel,' 'Notebook 5 - Linear SVM,' and 'Notebook 6 - Logistic Regression' - the only varying element being the type of classifier used. As a final note, it quickly became evident that the SVM + RBF kernel model was by far, the most computationally expensive. Including the tuning of hyper-parameters (see below), implementation of the SVM + RBF model took approximately 17 hours.\n\n### Refinement\nIn theory, we should be able to improve upon the baseline models by tuning the models' hyper-parameters. Our primary hyper-parameters of interest are C and gamma for the SVM + RBF model, and just C for the linear SVM model. Recall that C is the penalty parameter - how much we penalize our model for incorrect classifications. A higher C value holds the promise of greater accuracy, but at the risk of overfitting. The selection of the the gamma hyper-parameter determines the variance of the distributions generated by the [RBF kernel](https://www.youtube.com/watch?v=3liCbRZPrZA), with a large gamma tending to lead to higher bias but lower variance. \n\nThe C and gamma hyper-parameters can vary by several orders of magnitude, so finding the optimal configuration is no small task. Employing a [grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) can be computationally expensive - almost prohibitatively expensive without parallel computing resources. Fortunately, we do not have to exhaustively scan the hyper-parameter space. Rather, we can use Bayesian optimization to find the optima within the hyper-parameter space using surprisingly few iterations.\n\nHere I am indebted to Fernando Nogueira and his development of the [BayesianOptimization](https://github.com/fmfn/BayesianOptimization) for Python. By means of this package, we are able to scan the hyper-parameter space of the SVM + RBF kernel model for suitable values of C and gammma within the range 0.0001 to 1,000. We are able to scan for suitable values for C within the linear SVM model in similiar fashion. \n\nThe below figure illustrates the second and third iterations of this process in a hypothetical unidimensional space - for instance, the hyper-parameter C. Thus, the horizontal axis represents the individual values of C while the horizontal axis represents the metric that we are trying to optimize - in this case, the F2 score. \n\nThe true distribution of F1 scores is represented by the dashed line, but in reality is unknown. The dots represent derived F2 scores. The continuous line represents the inferred distribution of F2 score. The blue areas represent aa 95% confidence interval for the inferred distribution, or in other words, represent areas of potential information gain.  \n<div>\n<div align=\"center\">\n<p align=\"center\"><b>Figure 6: An Acquisition Function Combing a Unidimensional Space for Two Iterations</b></p>\n<p>Source:<a href=\"https://advancedoptimizationatharvard.wordpress.com/2014/04/28/bayesian-optimization-part-ii/\"> Bayesian Optimization and Its Applications Part II, gauravbharaj, April 28, 2014</a></p>\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/bayesian_optimization.png\" align=\"middle\" width=\"591\" height=\"387\" />\n</div>\n</div>\n\nThe Bayesian optimizer resolves the perennial dilemma between exploration and optimization by use of an acquisition function, shown above in green. The red triangle denotes the global maximum of the acquisition function, with the subsequent iteration deriving the F2 score for that value of C. Note how the acquisition function derives high value from regions of relatively low information (the exploration impetus), yet achieves even greater values when in the vicinity of known maxima of the inferred distribution (the optimization impetus).\n\nFor the purposes of Bayesian optimization, we used 20-fold cross validation with a [custom scoring function](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) to maximize the F2 scores. The optimization yielded values of C = 998 and gamma = 0.2 for the SVM + RBF model, C = 335 for the linear SVM model, and C = 585 for the logistic regression model.\n\n\n### Results \nThe optimal model in terms of F2 score, recall, but also precision was the linear SVM model with hyper-parameter tuning via Bayesian optimization. The linear SVM model was so successful that the non-optimized version was the second best performing model. The linear SVM model achieved a mean F2 score of 0.25 versus 0.04 for the benchmark KNN model. For recall, the linear SVM achieved a mean score of 0.33. In other words, the model sucessfully found a third of the valuable needles within our haystack. The model also achieved a mean precision of 0.14 - effectively tying with the hyper-parameter tuned logistic regression model. To put this in context, a sales representative engaged in blind guessing which acccounts would convert to paying customers would be hard pressed to be accurate more than 6% of the time (the rate of customer conversion).        \n\n<div align=\"center\">\n<p align=\"center\"><b>Table 2: Comparision of Performance Metrics Averaged from 100-Fold Cross Validation</b></p>\n</div>\n\n|              Model Used                                                 | F2 Score | Recall  | Precision |\n| :---------------------------------------------------------------------- | :------: | :-----: | :-------: |\n|<sub> K-Nearest Neighbors (Baseline)                                     |   0.04   |  0.04   | 0.04      |\n|<sub> Logistic Regression                                                |   0.16   |  0.19   | 0.13      |\n|<sub> Logistic Regression with Hyper-Parameter Tuning                    |   0.15   |  0.18   | 0.14      |\n|<sub> Support Vector Machines with RBF Kernel                            |   0.00   |  0.00   | 0.00      |\n|<sub> Support Vector Machines with RBF Kernel and Hyper-Parameter Tuning |   0.03   | 0.03    | 0.03      |\n|<sub> Linear Support Vector Machines                                     |   0.16   | 0.20    | 0.13      |\n|<sub> Linear Support Vector Machines with Hyper-Parameter Tuning         | **0.25** | **0.33**| **0.14**  |\n\nIt is striking how the AUC scores for the precision-recall curves imply a very different performance ranking than what the F2 scores report. Looking to the figures below, we can see that the linear SVM with hyper-parameter tuning actually has the lowest AUC of any of the curves (AUC = 0.10). These AUC scores are based upon average precision as opposed to recall. However, it is recall, not precision, that is our priority here. Nevertheless, it is worthing bearing in mind that more often than not, the price for greater recall is precision and vice versa.  \n\nThe results suggest that our winning model - linear SVM with Bayesian optimization - represents a reasonably robust result. Our success metrics were derived from 100-fold cross validation, and so our F2 score, precision, and recall scores are unlikley to be the results of a statistical fluke. More intuitively, the hyper-parameter tuned linear SVM model had the smallest C value of any of the models above (C=335), the implication being that the linear SVM model should be less likely to overfit vis-a-vis the competing models.  \n\n<div align=\"center\">\n<p align=\"center\"><b>Figure 7: Precision-Recall Curves of All Three Algorithms with and without Hyper-Parameter Tuning </b></p>\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/SVM_with_RBF.png\" width=\"432\" height=\"360\" />\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/Linear_SVM.png\" width=\"432\" height=\"360\" />\n</div>\n<div align=\"center\">\n<img src=\"https://github.com/b-knight/Understanding-Customer-Conversion-with-Snowplow-Web-Event-Tracking/blob/master/Images/Logistic_Regression.png\" width=\"432\" height=\"360\" />\n</div>\n\n### Conclusion \nThis project sought to differentiate soon-to-be paying customers from non-paying account-holders based solely on their activity history on the marketing site. This is a tall order, and while the linear SVM did achieve a F2 score of 0.25 (a more than five-fold improvement vis-avis the KNN benchmark) in practical terms the model is not yet successful enough to be used as part of the sales process. Should the company see a drastic influx of new leads, then our linear SVM model can conceivably be used to prioritize sales outreach. However, in the mean time we should re-examine the model for areas of improvement. \n\nA major challange of this project is the high dimensionality of the data combined with the sparse feature space. Dimensionality reduction via [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) was attempted, but yielded inferior model performance and was discontinued. Nevertheless, dimensionality reduction could be used to open up an array of robut models such as random forests. A possible alternative to PCA is [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) (LDA). Unlike PCA, LDA emphasizes discrimination between classes. Moreover, given the large number of observations available to us, PCA is generally less likely to perform well relative to LDA [(Martinez and Kak, 2001)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.5303&rep=rep1&type=pdf).\n\nLDA coupled with random forest modeling is one potential area to pursue. Another, more radical aproach would be to reconceive the problem not as one of supervised classification, but rather outlier detection. Given that the accounts of interest only make up 6% of the data set, there is an argument to be made that such observations are in fact - outliers. In pratical terms, we could leverage Sci-Kit Learn's [One Class SVM](http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html) funcationality. In this fashion, future extensions might take a factorial approach, varying between random forest versus One Class SVM and LDA-reduced data versus no dimensionality reduction. \n\nPerhaps the greatest potential for improving accuracy involves re-considering how we go about data transformation and sampling. With the original data transformation, we made the decision to aggregate page views by country code to create approximately 150 geographic predictors. There is no reason to believe that aggregating page views by country should yield greater accuracy than aggregating page pings by country. It is likley worth assessing whether a different set of geographic predictors might yield dividends. \n\n### References\n* Alexandr, A., Indyk, P. \u201cNear-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions\u201d, Foundations of Computer Science, 2006. FOCS \u201806. 47th Annual IEEE Symposium.\n* Bawa, M., Condie, T., Ganesan, P. \u201cLSH Forest: Self-Tuning Indexes for Similarity Search\u201d, WWW \u201805 Proceedings of the 14th international conference on World Wide Web Pages 651-660.\n* Bishop, Christopher M. Pattern Recognition and Machine Learning, Chapter 4.3.4.\n* Martinez, A., Avinash C. Kak. \"PCA versus LDA,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(2), 2001: doi:10.1.1.144.5303.\n* Pedregosa et al. \"Scikit-learn: Machine Learning in Python,\" JMLR 12, pp. 2825-2830, 2011.\n* Wainer, Jacques. \"Comparison of 14 Different Families of Classification Algorithms on 115 Binary Datasets,\" June 6, 2016. Retrieved from  https://arxiv.org/pdf/1606.00930v1.pdf.\n* Saito T., Rehmsmeier M. \"The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets,\" PLoS ONE 10(3), 2015: doi:10.1371.\n* Schmidt, Mark and Nicolas Le Roux and Francis Bach: Minimizing Finite Sums with the Stochastic Average Gradient.\n* Smola, A., Bernhard Sch\u00f6lkopf \u201cA Tutorial on Support Vector Regression\u201d, Statistics and Computing archive Volume 14 Issue 3, August 2004, p. 199-222.\n"}, {"repo": "laptc/magento2-customer-attributes", "language": "PHP", "readme_contents": "# magento2-customer-attributes\n## Free Magento 2 Customer Attributes Management\n\n- This is a free module to manage customer and address attribute for Magento 2. \n- Some simple attribute types are supported such as: Text, Text Area, Date, Yes/No, Multiple Select, Dropdown.\n\nBuild on: Magento 2.3.1, PHP 7.1\n\n## Installation\n\n### Manually\nRECOMMEND: get assistance from developer.\n- Download source code\n- Copy/upload all the files and folder to the following path on your Magento root directory:\n```text\napp/code/Mvn/Cam/\n```\n- Run the upgrade command line:\n```bash\nphp bin/magento setup:upgrade\n```\n- You may need to run deploy if your site is in production mode.\n\n# Contact\n- My info: https://laptc.dev\n- If you have any questions, just email me at: info@laptc.dev\n"}, {"repo": "tarasrng/customers-notification", "language": "Java", "readme_contents": "# customers-notification\nCustomers notification service is an example of using DI in a Vert.x based project. \n\nSee the details in my article \"Vert.x + Micronaut \u2014 Do We Need Dependency Injection in the Microservices World?\" \n\nIn English: https://medium.com/@taraskohut/vert-x-micronaut-do-we-need-dependency-injection-in-the-microservices-world-84e43b3b228e\n\nIn Ukrainian: https://dou.ua/lenta/articles/dependency-injection-for-microservices/\n"}, {"repo": "serhatyazicioglu/RFM-Analysis-for-Customer-Segmentation", "language": "Python", "readme_contents": "# RFM-Analysis-for-Customer-Segmentation\n\nI segmented the customers according to their behavior and thus more comfortable marketing strategies can be made.\n\nFor those who want to access the data set: https://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n"}, {"repo": "Yash-ai/Kaggle-Telecom-Customer-Churn-Prediction", "language": "Jupyter Notebook", "readme_contents": "Customer attrition, also known as customer churn, customer turnover, or customer defection, is the loss of clients or customers.\n\nTelephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer attrition analysis and customer attrition rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.\n\nCompanies usually make a distinction between voluntary churn and involuntary churn. Voluntary churn occurs due to a decision by the customer to switch to another company or service provider, involuntary churn occurs due to circumstances such as a customer's relocation to a long-term care facility, death, or the relocation to a distant location. In most applications, involuntary reasons for churn are excluded from the analytical models. Analysts tend to concentrate on voluntary churn, because it typically occurs due to factors of the company-customer relationship which companies control, such as how billing interactions are handled or how after-sales help is provided.\n\npredictive analytics use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn.\n"}, {"repo": "SubeyteT/Customer-Lifetime-Value-Prediction-Online-Retail-Dataset", "language": "Python", "readme_contents": "# Customer-Lifetime-Value-Prediction-Online-Retail-Dataset\n\nCLTV is a method for predicting a how much a customer is going to create value for a firm in a specific time. \n\nCLTV method consists of two main parts: \n  1. BG/NGD (Beta Geometric/ Negative Binomial Distributions) Submodel: Calculation of conditional expected number of transactions,\n  2. Gamma Gamma Submodel: Calculation of conditional expected average profit.\n\nBetaGeoFitter is used for CLTV model. \n\nWhile the predict functions' of submodels are timed for a week, BeteGeoFitter functions time variable is calculated monthly.\n\nOnline Retail II Dataset Information:\n      (https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)\n\nDataset inludes transactions non-store online retail between 01/12/2009 and 09/12/2011.\nThe company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n\nDataset variables:\n\nInvoiceNo: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\nStockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\nDescription: Product (item) name. Nominal.\nQuantity: The quantities of each product (item) per transaction. Numeric.\nInvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\nUnitPrice: Unit price. Numeric. Product price per unit in sterling (\u00c2\u00a3).\nCustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\nCountry: Country name. Nominal. The name of the country where a customer resides.\n\n\n\nFor better resoruces about the topic, visit: (https://www.veribilimiokulu.com/)\n"}, {"repo": "BlitzkriegSoftware/CustomerAPI", "language": "C#", "readme_contents": "# ASP.NET Core WebAPI w. Swashbuckle/Swagger #\n\nThis little demo app is to show how to:\n\n* Implement an ASP.NET Core Web API project\n* Add logging\n* Add health checks\n* Customize it with a global error handler\n* Add and customize swagger\n* demo parallel version\n\n## Versions ## \n\n<a href=\"dotnet2%2FREADME.md\" target=\"_blank\">DotNet Core 2.x</a>\n\n<a href=\"dotnet3%2FREADME.md\" target=\"_blank\">DotNet Core 3.1 LTS</a>\n\n\n## About ##\n\nStuart Williams\nCloud/DevOps Practice Lead\n\u00a0\nMagenic Technologies Inc.\nOffice of the CTO\n\u00a0\n<a href=\"mailto:stuartw@magenic.com\" target=\"_blank\">stuartw@magenic.com</a> (e-mail)\n\u00a0\nBlog: <a href=\"http://blitzkriegsoftware.net/Blog\" target=\"_blank\">http://blitzkriegsoftware.net/Blog</a> \nLinkedIn: <a href=\"http://lnkd.in/P35kVT\" target=\"_blank\">http://lnkd.in/P35kVT</a> \nYouTube: <a href=\"https://www.youtube.com/channel/UCO88zFRJMTrAZZbYzhvAlMg\" target=\"_blank\">https://www.youtube.com/channel/UCO88zFRJMTrAZZbYzhvAlMg</a> \n"}, {"repo": "terraform-aws-modules/terraform-aws-customer-gateway", "language": "HCL", "readme_contents": "# AWS Customer Gateway Terraform module\n\nTerraform module which creates AWS Customer Gateway resources on AWS.\n\nThis module has been extracted from the [VPC](https://github.com/terraform-aws-modules/terraform-aws-vpc) module, because sometimes it makes sense to reuse Customer Gateways across multiple VPC resources. Check out other related modules - [VPC](https://github.com/terraform-aws-modules/terraform-aws-vpc), [VPN Gateway](https://github.com/terraform-aws-modules/terraform-aws-vpn-gateway) and [Transit Gateway](https://github.com/terraform-aws-modules/terraform-aws-transit-gateway) for more details.\n\n## Usage\n\n```hcl\nmodule \"cgw\" {\n  source  = \"terraform-aws-modules/customer-gateway/aws\"\n  version = \"~> 1.0\"\n\n  name = \"test-cgw\"\n\n  customer_gateways = {\n    IP1 = {\n      bgp_asn    = 65112\n      ip_address = \"49.33.1.162\"\n    },\n    IP2 = {\n      bgp_asn    = 65112\n      ip_address = \"85.38.42.93\"\n    }\n  }\n\n  tags = {\n    Test = \"maybe\"\n  }\n}\n```\n\n## Examples\n\n- [Complete example](https://github.com/terraform-aws-modules/terraform-aws-customer-gateway/tree/master/examples/complete) creates 2 Customer Gateways, a VPC and creates 2 VPN connections between them.\n\n## Conditional creation\n\nSometimes you need to have a way to create Customer Gateway conditionally but Terraform does not allow to use `count` inside `module` block, so the solution is to specify argument `create`.\n\n```hcl\n# This CGW will not be created\nmodule \"cgw\" {\n  source  = \"terraform-aws-modules/customer-gateway/aws\"\n  version = \"~> 1.0\"\n\n  create = false\n  # ... omitted\n}\n```\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Requirements\n\n| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >= 0.12.26 |\n| <a name=\"requirement_aws\"></a> [aws](#requirement\\_aws) | >= 2.23 |\n\n## Providers\n\n| Name | Version |\n|------|---------|\n| <a name=\"provider_aws\"></a> [aws](#provider\\_aws) | >= 2.23 |\n\n## Modules\n\nNo modules.\n\n## Resources\n\n| Name | Type |\n|------|------|\n| [aws_customer_gateway.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/customer_gateway) | resource |\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|------|---------|:--------:|\n| <a name=\"input_create\"></a> [create](#input\\_create) | Whether to create Customer Gateway resources | `bool` | `true` | no |\n| <a name=\"input_customer_gateways\"></a> [customer\\_gateways](#input\\_customer\\_gateways) | Maps of Customer Gateway's attributes (BGP ASN and Gateway's Internet-routable external IP address) | `map(map(any))` | `{}` | no |\n| <a name=\"input_name\"></a> [name](#input\\_name) | Name to be used on all the resources as identifier | `string` | `\"\"` | no |\n| <a name=\"input_tags\"></a> [tags](#input\\_tags) | A mapping of tags to assign to all resources | `map(string)` | `{}` | no |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| <a name=\"output_customer_gateway\"></a> [customer\\_gateway](#output\\_customer\\_gateway) | Map of Customer Gateway attributes |\n| <a name=\"output_ids\"></a> [ids](#output\\_ids) | List of IDs of Customer Gateway |\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n\n## Authors\n\nModule is maintained by [Anton Babenko](https://github.com/antonbabenko) with help from [these awesome contributors](https://github.com/terraform-aws-modules/terraform-aws-customer-gateway/graphs/contributors).\n\n## License\n\nApache 2 Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-customer-gateway/tree/master/LICENSE) for full details.\n"}, {"repo": "astorm/CustomerPage", "language": "PHP", "readme_contents": "CustomerPage\n============\n\nA Magento module that demonstrates how to add a new page to the customer My Account section.\n\nOriginal Blog Post: http://alanstorm.com/magento_create_customer_page\n\n###Build Instructions\n\nThe `build_customer_page.bash` file is a bash script that will create a simple tar archive of the extension files. \n\n    $ ./build_customer_page.bash\n    \nThis script assumes the existence of a `var` folder.    "}, {"repo": "newrelic/nr1-customer-journey", "language": "JavaScript", "readme_contents": "[![Community Project header](https://github.com/newrelic/opensource-website/raw/master/src/images/categories/Community_Project.png)](https://opensource.newrelic.com/oss-category/#community-project)\n\n# New Relic One Customer Journey (nr1-customer-journey)\n[![All Contributors](https://img.shields.io/badge/all_contributors-4-orange.svg?style=flat-square)](#contributors)\n\n![CI](https://github.com/newrelic/nr1-customer-journey/workflows/CI/badge.svg) ![GitHub release (latest SemVer including pre-releases)](https://img.shields.io/github/v/release/newrelic/nr1-customer-journey?include_prereleases&sort=semver) [![Snyk](https://snyk.io/test/github/newrelic/nr1-customer-journey/badge.svg)](https://snyk.io/test/github/newrelic/nr1-customer-journey)\n\n\n## Overview\n\n`nr1-customer-journey` is like a [NRQL funnel query](https://docs.newrelic.com/docs/query-data/nrql-new-relic-query-language/nrql-query-examples/funnels-evaluate-data-series-events) on steroids. The Nerdpack is designed to allow devops teams, product manager, and digital marketers to examine a number of pre-defined measures in a side-by-side comparison between cohorts (columns) of users through a set of steps (rows).\n\n* Rows = Steps\n* Columns = Series\n* Measures = Stats\n\nThe application does this through a few visualizations.\n\n### Multi-series Funnel\n\nIt leverages a [`nr1-funnel-component`](https://github.com/newrelic/nr1-funnel-component). This visualization combines a 3rd party funnel library with the NR1 `NerdGraphQuery` component to execute three New Relic `NRQL` queries in the same GraphQL request and align the results into one data set that is delivered to the funnel visualization.\n\n### Data panels\n\nFor each row (Step) and column (Series), the Nerdpack renders a set of measurements (Stat). Those calculations can apply KPI thresholds to color-code the information. Each panel is also `clickable`, leading to a more detailed overview.\n\n![Overview](catalog/screenshots/nr1-customer-journey-1.png)\n\n### Detail Nerdlet\n\nFor any `Journey`, `Series`, and `Step`, there's a Nerdlet that will display the more detailed KPI's as well as timeseries view of the the given `Stats` for the selected row and column.\n\n![Details](catalog/screenshots/nr1-customer-journey-2.png)\n\n## Usage\n\nCurrently, the application is driven by a relatively complex JSON config file. The documentation for that configuration file is available [here](CONFIG.md).\n\n## Open Source License\n\nThis project is distributed under the [Apache 2 license](LICENSE).\n\n## What do you need to make this work?\n\n1. Access to [New Relic One](https://newrelic.com/platform).\n2. Configure Journey with built-in form.\n\n### Configuring new Journey\n\nIn `nr1-customer-journey` you can use wizard form to configure Journeys which will be saved in `AccountStorage`. See [CONFIG.md](CONFIG.md) for detailed instructions on the capabilities of each portion of the configuration. Also look at the [examples](examples) directory for inspiration.\n\n## Getting started\n\nFirst, ensure that you have [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) and [NPM](https://www.npmjs.com/get-npm) installed. If you're unsure whether you have one or both of them installed, run the following command(s) (If you have them installed these commands will return a version number, if not, the commands won't be recognized):\n\n```bash\ngit --version\nnpm -v\n```\n\nNext, install the [NR1 CLI](https://one.newrelic.com/launcher/developer-center.launcher) by going to [this link](https://one.newrelic.com/launcher/developer-center.launcher) and following the instructions (5 minutes or less) to install and setup your New Relic development environment.\n\nNext, to clone this repository and run the code locally against your New Relic data, execute the following command:\n\n```bash\nnr1 nerdpack:clone -r https://github.com/newrelic/nr1-customer-journey.git\ncd nr1-customer-journey\nnr1 nerdpack:serve\n```\n\nVisit [https://one.newrelic.com/?nerdpacks=local](https://one.newrelic.com/?nerdpacks=local), navigate to the Nerdpack, and :sparkles:\n\n## Deploying this Nerdpack\n\nOpen a command prompt in the nerdpack's directory and run the following commands.\n\n```bash\n# If you need to create a new uuid for the account to which you're deploying this Nerdpack, use the following\n# nr1 nerdpack:uuid -g [--profile=your_profile_name]\n# to see a list of APIkeys / profiles available in your development environment, run nr1 credentials:list\nnr1 nerdpack:publish [--profile=your_profile_name]\nnr1 nerdpack:deploy [-c [DEV|BETA|STABLE]] [--profile=your_profile_name]\nnr1 nerdpack:subscribe [-c [DEV|BETA|STABLE]] [--profile=your_profile_name]\n```\n\nVisit [https://one.newrelic.com](https://one.newrelic.com), navigate to the Nerdpack, and :sparkles:\n\n## Support\n\nNew Relic has open-sourced this project. This project is provided AS-IS WITHOUT WARRANTY OR SUPPORT, although you can report issues and contribute to the project here on GitHub.\n\n_Please do not report issues with this software to New Relic Global Technical Support._\n\n### Community\n\nNew Relic hosts and moderates an online forum where customers can interact with New Relic employees as well as other customers to get help and share best practices. Like all official New Relic open source projects, there's a related Community topic in the New Relic Explorers Hub. You can find this project's topic/threads here:\n\nhttps://discuss.newrelic.com/t/customer-journey-nerdpack/83270\n*(Note: URL subject to change before GA)*\n\n### Issues / Enhancement Requests\n\nIssues and enhancement requests can be submitted in the [Issues tab of this repository](../../issues). Please search for and review the existing open issues before submitting a new issue.\n\n## Contributing\n\nContributions are welcome (and if you submit a Enhancement Request, expect to be invited to contribute it yourself :grin:). Please review our [Contributors Guide](CONTRIBUTING.md).\n\nKeep in mind that when you submit your pull request, you'll need to sign the CLA via the click-through using CLA-Assistant. If you'd like to execute our corporate CLA, or if you have any questions, please drop us an email at opensource@newrelic.com.\n\n## Contributors \u2728\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore -->\n<table>\n  <tr>\n    <td align=\"center\"><a href=\"http://joelworrall.com\"><img src=\"https://avatars0.githubusercontent.com/u/929261?v=4\" width=\"100px;\" alt=\"Joel Worrall\"/><br /><sub><b>Joel Worrall</b></sub></a><br /><a href=\"https://github.com/newrelic/nr1-customer-journey/commits?author=tangollama\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#ideas-tangollama\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n    <td align=\"center\"><a href=\"https://twitter.com/dangolden1\"><img src=\"https://avatars2.githubusercontent.com/u/812989?v=4\" width=\"100px;\" alt=\"Daniel Golden\"/><br /><sub><b>Daniel Golden</b></sub></a><br /><a href=\"#ideas-danielgolden\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a> <a href=\"https://github.com/newrelic/nr1-customer-journey/commits?author=danielgolden\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#design-danielgolden\" title=\"Design\">\ud83c\udfa8</a></td>\n    <td align=\"center\"><a href=\"https://github.com/wesleyradcliffe\"><img src=\"https://avatars1.githubusercontent.com/u/1923933?v=4\" width=\"100px;\" alt=\"Wesley D. Radcliffe\"/><br /><sub><b>Wesley D. Radcliffe</b></sub></a><br /><a href=\"#ideas-wesleyradcliffe\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a> <a href=\"https://github.com/newrelic/nr1-customer-journey/commits?author=wesleyradcliffe\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://devfreddy.com\"><img src=\"https://avatars1.githubusercontent.com/u/197140?v=4\" width=\"100px;\" alt=\"Michael Frederick\"/><br /><sub><b>Michael Frederick</b></sub></a><br /><a href=\"#tool-devfreddy\" title=\"Tools\">\ud83d\udd27</a> <a href=\"https://github.com/newrelic/nr1-customer-journey/commits?author=devfreddy\" title=\"Code\">\ud83d\udcbb</a></td>\n  </tr>\n</table>\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\n"}, {"repo": "DanielGallo/CustomerApp", "language": "JavaScript", "readme_contents": ""}, {"repo": "php-cuong/magento2-customer-ajax-login-and-registration", "language": "PHP", "readme_contents": "# magento2-customer-ajax-login-and-registration\nMagento 2 Customer Ajax Login and Registration\n\n# See the video about this tutorial\n- Youtube: https://www.youtube.com/watch?v=UZTJw51m3Xo&index=41&list=PL98CDCbI3TNvPczWSOnpaMoyxVISLVzYQ\n- Facebook: https://www.facebook.com/giaphugroupcom/videos/1997540213689513/\n\n# Snapshot\n\n## The popup customer login form\n\n![ScreenShot](https://raw.githubusercontent.com/php-cuong/magento2-customer-ajax-login-and-registration/master/Snapshot/login-form.png)\n\n## The popup customer registration form\n\n![ScreenShot](https://raw.githubusercontent.com/php-cuong/magento2-customer-ajax-login-and-registration/master/Snapshot/registration-form.png)\n"}, {"repo": "sbodak/magento2-b2b-disable-customer-registration", "language": "PHP", "readme_contents": "# Magento 2 - Disable customer registration\n \n## Overview\nThis module enable possibility to disable the customer registration.\nIt can be useful for B2B customers, if we don't want to allow them register by themselves. \nExtension will remove link to register page and registration form from login page.\n\n## Compatibility\n- Magento 2.1.x - 2.3.x\n\n## Installation details\n1. Run `composer require sbodak/magento2-b2b-disable-customer-registration`\n2. Run `php bin/magento module:enable Bodak_DisableRegistration`\n3. Run `bin/magento setup:upgrade`\n4. Run `bin/magento clean:cache`\n\n## Configuration details\n1. Go to Magento admin panel\n2. Find option in `Stores > Configuration > Customers > Customer configuration`\n3. Under the `Create New Account Options` tab you will find the `Disable frontend customer registration` option\n4. `Enable` this option to activate the plugin\n\n\n### Module configuration - administration panel\n![Module configuration - administration panel](docs/customer_registration_disabled_configuration.png)\n\n### Remove registration form - frontend view\n![Remove registration form](docs/customer_registration_disabled.png)\n\n\n## Uninstall\n1. Run `composer remove sbodak/magento2-b2b-disable-customer-registration`\n\n## License\n[MIT License](LICENSE)\n"}, {"repo": "rowanfuchs/PrestaShop-login-as-a-customer", "language": "PHP", "readme_contents": ""}, {"repo": "Fizzzzer/Android_Fizzer_CustomerCamera", "language": "Java", "readme_contents": "# Android_Fizzer_CustomerCamera  \n\n\u8fd9\u91cc\u662f\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u76f8\u673a\u529f\u80fd\uff0c\u53ef\u4ee5\u6839\u636e\u81ea\u5df1\u7684\u9879\u76ee\u9700\u6c42\uff0c\u66f4\u6539\u76f8\u673a\u62cd\u7167\u7684\u754c\u9762view\uff0c\u6548\u679c\u56fe\u5982\u4e0b\n![image](https://github.com/Fizzzzer/Android_Fizzer_CustomerCamera/blob/master/test.gif)\n\n###\u4f7f\u7528\n\u5728`OnActivityResult`\u4e2d\u53ef\u4ee5\u63a5\u53d7\u62cd\u7167\u8fd4\u56de\u6765\u7684\u56fe\u7247\n```java\nString path = data.getStringExtra(\"data\");\n```\n\u8fd9\u91cc\u9762\u7684path\u5c31\u662f\u62cd\u7167\u8fd4\u56de\u7684\u56fe\u7247\u8def\u5f84\uff0c\u62ff\u5230\u8fd9\u4e2a\u56fe\u7247\u8def\u5f84\uff0c\u60f3\u5e72\u5565\u5c31\u53ef\u4ee5\u5e72\u5565\u4e86    \n\u540e\u9762\u7684\u5c31\u4e0d\u5728\u591a\u8bf4\u4e86\uff0c\u5728\u4ee3\u7801\u4e2d\u90fd\u53ef\u4ee5\u770b\uff0c\u5982\u679c\u6709\u5199\u7684\u4e0d\u597d\u7684\u5730\u65b9\uff0c\u671b\u65a7\u6b63\n"}, {"repo": "smtdeveloper/CustomerManagementSystem", "language": "Java", "readme_contents": "\n# CustomerManagementSystem\n\n<h2> Customer Management System - m\u00fc\u015fteri y\u00f6netim sistemi / Mernis ile TC Do\u011frulama </h2>\n\n<h4> SMTcoder :   Projeye y\u0131ld\u0131z Vermeyi Unutmay\u0131n  \ud83d\ude80 Te\u015fekk\u00fcrler! \u2764\ufe0f  </h4> \n\n<br>\n\n\nMERN\u0130S ( Kimlik Do\u011frulama Servisi ) Projeye Ekleme\n\u00dcst men\u00fcden, \"File - New - Project\"\n\n![1](https://user-images.githubusercontent.com/74311713/117274321-a0f0d600-ae65-11eb-857e-b1397cf27b6f.png)\n \n\nWizards: k\u0131sm\u0131nda \"Java Project\" yaz\u0131p, alt taraftan \"Java Project\" yazan\u0131 se\u00e7ip \"Next\" butonuna bas\u0131yoruz.\n![2](https://user-images.githubusercontent.com/74311713/117274599-e44b4480-ae65-11eb-85b6-8e0f087e8e04.png)!\n\n\n\n\n\n\n\nProject name \"MernisTest\" olarak a\u00e7\u0131yorum, \"Use an execution environment JRE:\" k\u0131sm\u0131ndan JavaSE-1.8 se\u00e7iyoruz. (Mernis servisi i\u00e7in stabil s\u00fcr\u00fcm) ve \"Finish\" diyoruz.\n![3](https://user-images.githubusercontent.com/74311713/117274730-01801300-ae66-11eb-92ce-95b8481c1cea.png)\n\nModule name \"MernisTest\" olarak belirledim. \"Don't Create\" butonuna bas\u0131yorum ve projemi olu\u015fturuyorum.\n\n\n![4](https://user-images.githubusercontent.com/74311713/117274638-ead9bc00-ae65-11eb-9322-de7d6bb4db91.png)\n\nOlu\u015fan projeme sa\u011f t\u0131klay\u0131p, \"New - Other\" se\u00e7iyorum.\n\n![5](https://user-images.githubusercontent.com/74311713/117274639-eb725280-ae65-11eb-9223-d452c4b3da33.png)\n\nWizards: k\u0131sm\u0131na \"Web Service Client\" yaz\u0131p, \"Web Services alt\u0131nda ki Web Service Client\" olan\u0131 se\u00e7iyorum ve \"Next\" butonuna bas\u0131yorum.\n![6](https://user-images.githubusercontent.com/74311713/117274640-eb725280-ae65-11eb-9966-ca80f6998ec0.png)\n\nService definition: k\u0131sm\u0131na \"https://tckimlik.nvi.gov.tr/Service/KPSPublic.asmx?WSDL\" adresini yap\u0131\u015ft\u0131r\u0131yorum ve \"Finish\" butonuna bas\u0131yorum.\n\n![7](https://user-images.githubusercontent.com/74311713/117274643-ec0ae900-ae65-11eb-97ec-bde8092ae4ad.png)\n\nYine projemde \"src\" klas\u00f6r\u00fcne sa\u011f t\u0131klayarak \"New - Class\" se\u00e7iyorum.\n\n![8](https://user-images.githubusercontent.com/74311713/117274644-ec0ae900-ae65-11eb-9ba9-d754a5cbac20.png)\n\nPackage: k\u0131sm\u0131n\u0131 bo\u015f b\u0131rak\u0131yorum, Name: k\u0131sm\u0131na \"Main\" yaz\u0131yorum, [ ] public static void main. yazan k\u0131sm\u0131 se\u00e7iyorum ve \"Finish\" butonuna bas\u0131yorum.\n\n\n![9](https://user-images.githubusercontent.com/74311713/117274646-eca37f80-ae65-11eb-86b6-94f69bce23c4.png)\nK\u0131rm\u0131z\u0131 \u00e7er\u00e7eveye ald\u0131\u011f\u0131m her \u015feyi ayn\u0131 \u015fekilde yazarak projenizi mernis kimlik kontrol\u00fc ile birlikte \u00e7al\u0131\u015ft\u0131rabilirsiniz.\n\n\n![10](https://user-images.githubusercontent.com/74311713/117274648-eca37f80-ae65-11eb-90ad-c77bbd841354.png)\n\n<h3> <a href=\"https://sametakca.com/\">  web sitem </a> </h3> \n \n<br> <br>\nSosyal Medya Hesaplar\u0131m \ud83d\ude1b\n<br>\n\n<a href=\"https://www.instagram.com/smtcoder/\">\ninstagram\n</a>\n<br>\n\n<a href=\"https://www.linkedin.com/in/samet-akca-2a4bbb1a8/\">\nlinkedin\n</a>\n<br>\n\n<a href=\"https://www.youtube.com/channel/UCZXmqpZJ3ax5Uzm0pXeVqMg\">\nyoutube\n</a>\n\n<br>\n\n<a href=\"https://play.google.com/store/apps/developer?id=Samet+Akca&gl=TR\">\nGoogle Play uygulamalar\u0131m\n</a>\n\n<br>\n<br>\n\n\n\n\nProjeye y\u0131ld\u0131z Vermeyi Unutmay\u0131n  \ud83d\ude80\nTe\u015fekk\u00fcrler! \u2764\ufe0f\n\n\n\n"}, {"repo": "gaiterjones/magento-GetCustomerFeedback", "language": "PHP", "readme_contents": "## Magento Get Customer Feedback\n***\n\n### Synopsis\nThis Magento 1.X CE module sends a Get Customer Feedback email to customers who have placed an order.\n\n### Version\n***\n\t@version\t\t30052017\n\t@author\t\t\tgaiterjones\n\t@documentation\t[blog.gaiterjones.com](http://blog.gaiterjones.com)\n\t\n### Requirements\n\n* PHP5.4+\n\n* Magento CE 1.3-1.9\n\n### Installation\n\nTo install the extension, copy the contents of the extension app folder to your magento store app folder. Make sure the extension cache folder is writeable by your www user group. Refresh your magento cache. Logout from admin, and login again.\n\nmodman clone https://github.com/gaiterjones/magento-GetCustomerFeedback.git\n \n\n## License\n\nThe MIT License (MIT)\nCopyright (c) 2013 Peter Jones\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."}, {"repo": "smit5490/CustomerChurn", "language": "Jupyter Notebook", "readme_contents": "# Understanding Customer Churn\n*Note: This repository was completely updated using survival analysis techniques and used as the first project in \nUdacity's Data Scientist Nanodegree program.*\n \n### Project Setup:\nThe Python version used was 3.8.1. After creating a clean Python/Conda virtual environment all of the project's dependencies are in the requirements.txt \nfile which can be installed using:\n\n`pip install -r requirements.txt`\n\nAll of the survival modeling was done using the [lifelines](https://lifelines.readthedocs.io/en/latest/) package.\n\n### Motivation: \nThere are two main challenges when running a subscription-based business, getting customers and retaining them. This \nproject focuses on the latter by analyzing customer enrollment characteristics and their impact on churn rate for a \nsubscription-based personal finance business.\n\n### Summary of Analysis\nCustomers in the silver tier had the lowest churn and longest tenure of all three tiers.\nCustomers that were referred to the company's software versus through partnerships or were organically generated also \nexperienced the longest tenure. Customers based in Canada and the Netherlands had the longest tenure amongst all \ncountries.\n \nTwo survival models were explored to forecast enrollments and understand the impact of a customer's enrollment characteristics on \ntheir tenure:\n\n**Modified Cox Proportional Hazard Model:** A modified version of the Cox Proportional Hazards model where the baseline \nhazard includes cubic spline terms. This model allows for extrapolation, but its predictions vary widely depending on \nthe number of splines used (while still reporting very similar concordance indices).\n\n**Weibull Accelerated Failure Time Model:** A parameteric survival analysis model based on the Weibull distribution.\n\nIn an effort to balance accuracy and interpretability, the Weibull model was used for forecasting.\n\n### Data:\n\nIn the *data* folder, all data is in one flat file called *9mo_pull.csv*, which contains data for all members who \nsubscribed to a personal finance SaaS exactly 9 months ago. It does not contain data for members who subscribed since \nthen. In other words, each member in the dataset has the same start date. As a result, this data is considered to be \n\"right censored\". \n\n**Data Dictionary:**  \n*member_id* - Unique ID of the user.  \n*tier* - Price tier (Silver, Gold, or Platinum).  \n*country* - Member country.  \n*source* - Original acquisition channel.  \n*tenure* - Number of cycles billed. Min is 1. Max is 9.  \n*active* - Is the subscription still active?  \n\n### Code:\nThere is a single notebook in the *code* folder that contains the entire end-to-end analysis and modeling of the data set. \nThere are some old notebooks in the *old notebooks* folder that explore other packages/approaches, but are not directly \nrelevant to the Udacity project.\n"}, {"repo": "VirtoCommerce/vc-module-customer", "language": "C#", "readme_contents": "# Overview\n\n[![CI status](https://github.com/VirtoCommerce/vc-module-customer/workflows/Module%20CI/badge.svg?branch=dev)](https://github.com/VirtoCommerce/vc-module-customer/actions?query=workflow%3A\"Module+CI\") [![Quality gate](https://sonarcloud.io/api/project_badges/measure?project=VirtoCommerce_vc-module-customer&metric=alert_status&branch=dev)](https://sonarcloud.io/dashboard?id=VirtoCommerce_vc-module-customer) [![Reliability rating](https://sonarcloud.io/api/project_badges/measure?project=VirtoCommerce_vc-module-customer&metric=reliability_rating&branch=dev)](https://sonarcloud.io/dashboard?id=VirtoCommerce_vc-module-customer) [![Security rating](https://sonarcloud.io/api/project_badges/measure?project=VirtoCommerce_vc-module-customer&metric=security_rating&branch=dev)](https://sonarcloud.io/dashboard?id=VirtoCommerce_vc-module-customer) [![Sqale rating](https://sonarcloud.io/api/project_badges/measure?project=VirtoCommerce_vc-module-customer&metric=sqale_rating&branch=dev)](https://sonarcloud.io/dashboard?id=VirtoCommerce_vc-module-customer)\n\nVirtoCommerce.Customer module represents contacts management system. The main purpose of this functionality is to keep the users contact information. The VC Customer Module helps to view, search and edit contact information.\n\n## Key features:\n\n1. \u0421ontacts arrangement in hierarchical structure;\n1. Module extensibility with custom contact types;\n1. \"Organization\", \"Employee\", \"Customer\" and \"Vendor\" contact types supported out of the box.\n\n## Documentation\n\n1. [Customer Module Document](/docs/index.md)\n1. [View on Github](https://github.com/VirtoCommerce/vc-module-customer)\n\n1. Developer guide: <a href=\"https://virtocommerce.com/docs/vc2devguide/extending-commerce/extending-members-domain-types\" target=\"_blank\">Extending Members domain types</a>\n\n## Installation\n\n1. Automatically: in VC Manager go to More -> Modules -> Customer management module -> Install;\n\n1. Manually: download module zip package from https://github.com/VirtoCommerce/vc-module-customer/releases. In VC Manager go to More -> Modules -> Advanced -> upload module package -> Install.\n\n## References\n\n1. Deploy: https://virtocommerce.com/docs/latest/developer-guide/deploy-module-from-source-code/\n1. Installation: https://www.virtocommerce.com/docs/latest/user-guide/modules/\n1. Home: https://virtocommerce.com\n1. Community: https://www.virtocommerce.org\n1. [Download Latest Release](https://github.com/VirtoCommerce/vc-module-customer/releases/tag/3.2.0)\n\n## Available resources\n\n1. Module related service implementations as a <a href=\"https://www.nuget.org/packages/VirtoCommerce.CustomerModule.Data\" target=\"_blank\">NuGet package</a>\n1. API client as a <a href=\"https://www.nuget.org/packages/VirtoCommerce.CustomerModule.Client\" target=\"_blank\">NuGet package</a>\n1. API client documentation http://demo.virtocommerce.com/admin/docs/ui/index#!/Customer_management_module\n\n## License\n\nCopyright (c) Virto Solutions LTD.  All rights reserved.\n\nLicensed under the Virto Commerce Open Software License (the \"License\"); you\nmay not use this file except in compliance with the License. You may\nobtain a copy of the License at\n\nhttp://virtocommerce.com/opensourcelicense\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied.\n"}, {"repo": "SonarSoftware/customer_portal", "language": "PHP", "readme_contents": "# End of Life\nThis portal is no longer supported and will not receive further updates. Please check out our [brand new portal](https://github.com/sonarsoftwareinc/customer_portal)!\n\n# Sonar Customer Portal\nThis provides a prebuilt customer portal for [Sonar](https://sonar.software). This requires Sonar 0.6 or greater. Please check the [wiki](https://github.com/SonarSoftware/customer_portal/wiki) for installation instructions.\n\n\n## Help and other resources\n[Installation, upgrade, and configuration instructions](https://github.com/SonarSoftware/customer_portal/wiki)\n\n[Need help installing or configuring your customer portal?](https://sonar.software/contact)\n\n[Want to build your own customer portal?](https://packagist.org/packages/sonarsoftware/customer_portal_framework)\n"}, {"repo": "oroinc/customer-portal", "language": "PHP", "readme_contents": "# customer-portal\n\nPackage that contains bundles related to the Customer management.\n\nResources\n---------\n\n  * [OroCommerce Documentation](https://doc.oroinc.com)\n  * [Contributing](https://doc.oroinc.com/community/contribute/)\n"}, {"repo": "microsoft/customer-scripts", "language": "PowerShell", "readme_contents": "# customer-scripts\nRepository of scripts provided frequently to our customers.\n\nWe frequently provide customers with steps to do certain tasks to sometimes help us debug issues and let them make progress. We will maintain such steps as scripts here for quick and easy access.\nFeel free to contribute with your own\n"}, {"repo": "correooke/example-customers-app", "language": "JavaScript", "readme_contents": "## Segunda aplicaci\u00f3n de ejemplo\n\nFocalizado en utilizaci\u00f3n de conceptos integradores de todo lo visto en la primer parte del curso\n\nPrimero, antes de ejecutar \"npm start\", se debe levantar el servidor json-server\n\njson-server --watch db.json --port 3001"}, {"repo": "darkdreamingdan/mlnd-customer-segments", "language": "HTML", "readme_contents": "# Content: Unsupervised Learning\n## Project: Creating Customer Segments\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute a [Jupyter Notebook](http://ipython.org/notebook.html)\n\nIf you do not have Python installed yet, it is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. \n\n### Code\n\nTemplate code is provided in the `customer_segments.ipynb` notebook file. You will also be required to use the included `visuals.py` Python file and the `housing.csv` dataset file to complete your work. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project. Note that the code included in `visuals.py` is meant to be used out-of-the-box and not intended for students to manipulate. If you are interested in how the visualizations are created in the notebook, please feel free to explore this Python file.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `customer_segments/` (that contains this README) and run one of the following commands:\n\n```bash\nipython notebook customer_segments.ipynb\n```  \nor\n```bash\njupyter notebook customer_segments.ipynb\n```\n\nThis will open the Jupyter Notebook software and project file in your browser.\n\n## Data\n\nThe customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal. More information can be found on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers).\n\nNote (m.u.) is shorthand for *monetary units*.\n\n**Features**\n1) `Fresh`: annual spending (m.u.) on fresh products (Continuous); \n2) `Milk`: annual spending (m.u.) on milk products (Continuous); \n3) `Grocery`: annual spending (m.u.) on grocery products (Continuous); \n4) `Frozen`: annual spending (m.u.) on frozen products (Continuous);\n5) `Detergents_Paper`: annual spending (m.u.) on detergents and paper products (Continuous);\n6) `Delicatessen`: annual spending (m.u.) on and delicatessen products (Continuous); \n7) `Channel`: {Hotel/Restaurant/Cafe - 1, Retail - 2} (Nominal)\n8) `Region`: {Lisnon - 1, Oporto - 2, or Other - 3} (Nominal) "}, {"repo": "emilybache/CustomerSync-Refactoring-Kata", "language": "Java", "readme_contents": "Customer Sync Refactoring Kata \n==============================\n\nThis code is inspired by some actual code I saw once. The business layer logic is not well separated from the data layer, amongst other things. Concentrate on the \"CustomerSync\" class. The purpose of the 'syncWithDataLayer' method is to take a ExternalCustomer instance, which has been updated in an external system, and see whether there is a matching Customer in our database. If there is not, create a new Customer to match the incoming ExternalCustomer. If there is one, update it. If there are several matching Customers in our database, update them all (slightly differently).\n\nThere is a unit test there to start you off. It uses Mockito to mock access to the database, and check the stored customer is synchronized correctly with the external customer.\n\nThe change you need to make\n---------------------------\n\nAs ever, you have a goal with your refactoring. The scenario is that you have been asked to synchronize an additional field from the ExternalCustomer to the Customer. The field is 'bonusPointsBalance' and is an integer. Only private people have bonus points, not companies. Add the field and ensure that if the ExternalCustomer has a different number of points from the Customer, the balance is updated in our database.\n\nBranch with_tests\n-----------------\n\nThe branch 'with_tests' is an alternative starting point where there are good unit tests available, and you can get started refactoring straight away. These tests do not use Mockito, they replace the database with a Fake and use an Approval Testing approach to check the stored customer is synchronized correctly with the external customer. The code coverage is not quite 100%, I believe this is due to unreachable code. Another way to use this code is to read and understand the approval testing techniques used, or to re-write the tests in another style.\n"}, {"repo": "bestkao/customer_segments", "language": "HTML", "readme_contents": "\n# Machine Learning Engineer Nanodegree\n## Unsupervised Learning\n## Project: Creating Customer Segments\n\nWelcome to the third project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `'TODO'` statement. Please be sure to read the instructions carefully!\n\nIn addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n\n>**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.\n\n## Getting Started\n\nIn this project, you will analyze a dataset containing data on various customers' annual spending amounts (reported in *monetary units*) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.\n\nThe dataset for this project can be found on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers). For the purposes of this project, the features `'Channel'` and `'Region'` will be excluded in the analysis \u2014 with focus instead on the six product categories recorded for customers.\n\nRun the code block below to load the wholesale customers dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported.\n\n\n```python\n# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\n# Import supplementary visualizations code visuals.py\nimport visuals as vs\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Load the wholesale customers dataset\ntry:\n    data = pd.read_csv(\"customers.csv\")\n    data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n    print \"Wholesale customers dataset has {} samples with {} features each.\".format(*data.shape)\nexcept:\n    print \"Dataset could not be loaded. Is the dataset missing?\"\n```\n\n    Wholesale customers dataset has 440 samples with 6 features each.\n\n\n## Data Exploration\nIn this section, you will begin exploring the data through visualizations and code to understand how each feature is related to the others. You will observe a statistical description of the dataset, consider the relevance of each feature, and select a few sample data points from the dataset which you will track through the course of this project.\n\nRun the code block below to observe a statistical description of the dataset. Note that the dataset is composed of six important product categories: **'Fresh'**, **'Milk'**, **'Grocery'**, **'Frozen'**, **'Detergents_Paper'**, and **'Delicatessen'**. Consider what each category represents in terms of products you could purchase.\n\n\n```python\n# Display a description of the dataset\ndisplay(data.describe())\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>440.000000</td>\n      <td>440.000000</td>\n      <td>440.000000</td>\n      <td>440.000000</td>\n      <td>440.000000</td>\n      <td>440.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>12000.297727</td>\n      <td>5796.265909</td>\n      <td>7951.277273</td>\n      <td>3071.931818</td>\n      <td>2881.493182</td>\n      <td>1524.870455</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>12647.328865</td>\n      <td>7380.377175</td>\n      <td>9503.162829</td>\n      <td>4854.673333</td>\n      <td>4767.854448</td>\n      <td>2820.105937</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.000000</td>\n      <td>55.000000</td>\n      <td>3.000000</td>\n      <td>25.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3127.750000</td>\n      <td>1533.000000</td>\n      <td>2153.000000</td>\n      <td>742.250000</td>\n      <td>256.750000</td>\n      <td>408.250000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>8504.000000</td>\n      <td>3627.000000</td>\n      <td>4755.500000</td>\n      <td>1526.000000</td>\n      <td>816.500000</td>\n      <td>965.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>16933.750000</td>\n      <td>7190.250000</td>\n      <td>10655.750000</td>\n      <td>3554.250000</td>\n      <td>3922.000000</td>\n      <td>1820.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>112151.000000</td>\n      <td>73498.000000</td>\n      <td>92780.000000</td>\n      <td>60869.000000</td>\n      <td>40827.000000</td>\n      <td>47943.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Implementation: Selecting Samples\nTo get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, add **three** indices of your choice to the `indices` list which will represent the customers to track. It is suggested to try different sets of samples until you obtain customers that vary significantly from one another.\n\n\n```python\n# Select three indices of your choice you wish to sample from the dataset\nindices = [39,56,71]\n\n# Create a DataFrame of the chosen samples\nsamples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)\nprint \"Chosen samples of wholesale customers dataset:\"\ndisplay(samples)\n```\n\n    Chosen samples of wholesale customers dataset:\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>56159</td>\n      <td>555</td>\n      <td>902</td>\n      <td>10002</td>\n      <td>212</td>\n      <td>2916</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4098</td>\n      <td>29892</td>\n      <td>26866</td>\n      <td>2616</td>\n      <td>17740</td>\n      <td>1340</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18291</td>\n      <td>1266</td>\n      <td>21042</td>\n      <td>5373</td>\n      <td>4173</td>\n      <td>14472</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Question 1\nConsider the total purchase cost of each product category and the statistical description of the dataset above for your sample customers.\n*What kind of establishment (customer) could each of the three samples you've chosen represent?*\n**Hint:** Examples of establishments include places like markets, cafes, and retailers, among many others. Avoid using names for establishments, such as saying *\"McDonalds\"* when describing a sample customer as a restaurant.\n\n\n```python\nprint 'Total purchase costs for each category:'\nprint data.sum()\nprint 'Average purchase costs for each category:'\nprint data.sum() / 440\n\n# Visualize samples\nimport seaborn as sns\nsamples_bar = samples.append(data.describe().loc['mean'])\nsamples_bar.index = indices + ['mean']\n_ = samples_bar.plot(kind='bar', figsize=(14,6))\n```\n\n    Total purchase costs for each category:\n    Fresh               5280131\n    Milk                2550357\n    Grocery             3498562\n    Frozen              1351650\n    Detergents_Paper    1267857\n    Delicatessen         670943\n    dtype: int64\n    Average purchase costs for each category:\n    Fresh               12000.297727\n    Milk                 5796.265909\n    Grocery              7951.277273\n    Frozen               3071.931818\n    Detergents_Paper     2881.493182\n    Delicatessen         1524.870455\n    dtype: float64\n\n\n\n![png](output/output_9_1.png)\n\n\n**Answer:**\n\nEstablishment 1 is your typical fresh produce market, with a small selection of frozen foods. Compared to the mean, it needs significantly more `Fresh` foods (56159 vs 12000) and `Frozen` foods (10002 vs 3072).\n\nEstablishment 2 is your typical cafe or restaurant with a need for a variety of food staples, basic ingredients, and supplies for supporting customers that dine in or take out. A cafe might particularly use a lot more milk since they'll serve coffee. Compared to the mean, it needs significantly more `Milk` (29892 vs 5796), `Grocery` foods (26866 vs 7951), and `Detergents_Paper` (17740 vs 2881).\n\nEstablishment 3 is your typical deli, with a great diversty of food offerings, perhaps even a selection ethnic or unusual foods. Compared to the mean, it needs significantly more `Delicatessen` (14472 vs 1525) and `Grocery` foods (21042 vs 7951), and somewhat more `Fresh` foods (18291 vs 12000) than other establishments.\n\n### Implementation: Feature Relevance\nOne interesting thought to consider is if one (or more) of the six product categories is actually relevant for understanding customer purchasing. That is to say, is it possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.\n\nIn the code block below, you will need to implement the following:\n - Assign `new_data` a copy of the data by removing a feature of your choice using the `DataFrame.drop` function.\n - Use `sklearn.cross_validation.train_test_split` to split the dataset into training and testing sets.\n   - Use the removed feature as your target label. Set a `test_size` of `0.25` and set a `random_state`.\n - Import a decision tree regressor, set a `random_state`, and fit the learner to the training data.\n - Report the prediction score of the testing set using the regressor's `score` function.\n\n\n```python\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Make a copy of the DataFrame, using the 'drop' function to drop the given feature\nnew_data = data.copy().drop('Detergents_Paper', 1)\n\n# Split the data into training and testing sets using the given feature as the target\nX_train, X_test, y_train, y_test = train_test_split(new_data, data['Detergents_Paper'], test_size=0.25, random_state=0)\n\n# Create a decision tree regressor and fit it to the training set\nregressor = DecisionTreeRegressor(random_state=0)\nregressor.fit(X_train, y_train)\n\n# Report the score of the prediction using the testing set\nscore = regressor.score(X_test, y_test)\nprint \"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score)\n```\n\n    Model has a coefficient of determination, R^2, of 0.729.\n\n\n### Question 2\n*Which feature did you attempt to predict? What was the reported prediction score? Is this feature is necessary for identifying customers' spending habits?*\n**Hint:** The coefficient of determination, `R^2`, is scored between 0 and 1, with 1 being a perfect fit. A negative `R^2` implies the model fails to fit the data.\n\n**Answer:**\n\nI tried to predict annual spending on detergents and paper products and got a R^2 score of 0.729. This model performs predictions on this feature quite well, so it's not absolutely necessary that we need this feature to identify customers' spending habits.\n\n### Visualize Feature Distributions\nTo get a better understanding of the dataset, we can construct a scatter matrix of each of the six product features present in the data. If you found that the feature you attempted to predict above is relevant for identifying a specific customer, then the scatter matrix below may not show any correlation between that feature and the others. Conversely, if you believe that feature is not relevant for identifying a specific customer, the scatter matrix might show a correlation between that feature and another feature in the data. Run the code block below to produce a scatter matrix.\n\n\n```python\n# Produce a scatter matrix for each pair of features in the data\npd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n```\n\n\n![png](output/output_16_0.png)\n\n\n### Question 3\n*Are there any pairs of features which exhibit some degree of correlation? Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict? How is the data for those features distributed?*\n**Hint:** Is the data normally distributed? Where do most of the data points lie?\n\n**Answer:**\n\n`Detergents_Paper` appears to be somewhat correlated with `Milk` and rather highly correlated with `Grocery`. This confirms my suspicions that it's not too relevant for identifying a specific customer with our model. All features appear to have a heavily positively skewed distribution.\n\n## Data Preprocessing\nIn this section, you will preprocess the data to create a better representation of customers by performing a scaling on the data and detecting (and optionally removing) outliers. Preprocessing data is often times a critical step in assuring that results you obtain from your analysis are significant and meaningful.\n\n### Implementation: Feature Scaling\nIf data is not normally distributed, especially if the mean and median vary significantly (indicating a large skew), it is most [often appropriate](http://econbrowser.com/archives/2014/02/use-of-logarithms-in-economics) to apply a non-linear scaling \u2014 particularly for financial data. One way to achieve this scaling is by using a [Box-Cox test](http://scipy.github.io/devdocs/generated/scipy.stats.boxcox.html), which calculates the best power transformation of the data that reduces skewness. A simpler approach which can work in most cases would be applying the natural logarithm.\n\nIn the code block below, you will need to implement the following:\n - Assign a copy of the data to `log_data` after applying logarithmic scaling. Use the `np.log` function for this.\n - Assign a copy of the sample data to `log_samples` after applying logarithmic scaling. Again, use `np.log`.\n\n\n```python\n# Scale the data using the natural logarithm\nlog_data = np.log(data.copy())\n\n# Scale the sample data using the natural logarithm\nlog_samples = np.log(samples)\n\n# Produce a scatter matrix for each pair of newly-transformed features\npd.scatter_matrix(log_data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n```\n\n\n![png](output/output_21_0.png)\n\n\n### Observation\nAfter applying a natural logarithm scaling to the data, the distribution of each feature should appear much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before).\n\nRun the code below to see how the sample data has changed after having the natural logarithm applied to it.\n\n\n```python\n# Display the log-transformed sample data\ndisplay(log_samples)\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.935942</td>\n      <td>6.318968</td>\n      <td>6.804615</td>\n      <td>9.210540</td>\n      <td>5.356586</td>\n      <td>7.977968</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.318254</td>\n      <td>10.305346</td>\n      <td>10.198617</td>\n      <td>7.869402</td>\n      <td>9.783577</td>\n      <td>7.200425</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9.814164</td>\n      <td>7.143618</td>\n      <td>9.954276</td>\n      <td>8.589142</td>\n      <td>8.336390</td>\n      <td>9.579971</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Implementation: Outlier Detection\nDetecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use [Tukey's Method for identfying outliers](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/): An *outlier step* is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal.\n\nIn the code block below, you will need to implement the following:\n - Assign the value of the 25th percentile for the given feature to `Q1`. Use `np.percentile` for this.\n - Assign the value of the 75th percentile for the given feature to `Q3`. Again, use `np.percentile`.\n - Assign the calculation of an outlier step for the given feature to `step`.\n - Optionally remove data points from the dataset by adding indices to the `outliers` list.\n\n**NOTE:** If you choose to remove any outliers, ensure that the sample data does not contain any of these points!\nOnce you have performed this implementation, the dataset will be stored in the variable `good_data`.\n\n\n```python\n# For each feature find the data points with extreme high or low values\nfor feature in log_data.keys():\n\n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data[feature], 25)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(log_data[feature], 75)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = 1.5 * (Q3 - Q1)\n\n    # Display the outliers\n    print \"Data points considered outliers for the feature '{}':\".format(feature)\n    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n\n# OPTIONAL: Select the indices for data points you wish to remove\noutliers  = [65, 66, 75, 128, 154]\n\n# Remove the outliers, if any were specified\ngood_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)\n```\n\n    Data points considered outliers for the feature 'Fresh':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>65</th>\n      <td>4.442651</td>\n      <td>9.950323</td>\n      <td>10.732651</td>\n      <td>3.583519</td>\n      <td>10.095388</td>\n      <td>7.260523</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>2.197225</td>\n      <td>7.335634</td>\n      <td>8.911530</td>\n      <td>5.164786</td>\n      <td>8.151333</td>\n      <td>3.295837</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>5.389072</td>\n      <td>9.163249</td>\n      <td>9.575192</td>\n      <td>5.645447</td>\n      <td>8.964184</td>\n      <td>5.049856</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>1.098612</td>\n      <td>7.979339</td>\n      <td>8.740657</td>\n      <td>6.086775</td>\n      <td>5.407172</td>\n      <td>6.563856</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>3.135494</td>\n      <td>7.869402</td>\n      <td>9.001839</td>\n      <td>4.976734</td>\n      <td>8.262043</td>\n      <td>5.379897</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>4.941642</td>\n      <td>9.087834</td>\n      <td>8.248791</td>\n      <td>4.955827</td>\n      <td>6.967909</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>5.298317</td>\n      <td>10.160530</td>\n      <td>9.894245</td>\n      <td>6.478510</td>\n      <td>9.079434</td>\n      <td>8.740337</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>5.192957</td>\n      <td>8.156223</td>\n      <td>9.917982</td>\n      <td>6.865891</td>\n      <td>8.633731</td>\n      <td>6.501290</td>\n    </tr>\n    <tr>\n      <th>218</th>\n      <td>2.890372</td>\n      <td>8.923191</td>\n      <td>9.629380</td>\n      <td>7.158514</td>\n      <td>8.475746</td>\n      <td>8.759669</td>\n    </tr>\n    <tr>\n      <th>304</th>\n      <td>5.081404</td>\n      <td>8.917311</td>\n      <td>10.117510</td>\n      <td>6.424869</td>\n      <td>9.374413</td>\n      <td>7.787382</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>5.493061</td>\n      <td>9.468001</td>\n      <td>9.088399</td>\n      <td>6.683361</td>\n      <td>8.271037</td>\n      <td>5.351858</td>\n    </tr>\n    <tr>\n      <th>338</th>\n      <td>1.098612</td>\n      <td>5.808142</td>\n      <td>8.856661</td>\n      <td>9.655090</td>\n      <td>2.708050</td>\n      <td>6.309918</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>4.762174</td>\n      <td>8.742574</td>\n      <td>9.961898</td>\n      <td>5.429346</td>\n      <td>9.069007</td>\n      <td>7.013016</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>5.247024</td>\n      <td>6.588926</td>\n      <td>7.606885</td>\n      <td>5.501258</td>\n      <td>5.214936</td>\n      <td>4.844187</td>\n    </tr>\n    <tr>\n      <th>357</th>\n      <td>3.610918</td>\n      <td>7.150701</td>\n      <td>10.011086</td>\n      <td>4.919981</td>\n      <td>8.816853</td>\n      <td>4.700480</td>\n    </tr>\n    <tr>\n      <th>412</th>\n      <td>4.574711</td>\n      <td>8.190077</td>\n      <td>9.425452</td>\n      <td>4.584967</td>\n      <td>7.996317</td>\n      <td>4.127134</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Milk':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>86</th>\n      <td>10.039983</td>\n      <td>11.205013</td>\n      <td>10.377047</td>\n      <td>6.894670</td>\n      <td>9.906981</td>\n      <td>6.805723</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>6.220590</td>\n      <td>4.718499</td>\n      <td>6.656727</td>\n      <td>6.796824</td>\n      <td>4.025352</td>\n      <td>4.882802</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>6.432940</td>\n      <td>4.007333</td>\n      <td>4.919981</td>\n      <td>4.317488</td>\n      <td>1.945910</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>356</th>\n      <td>10.029503</td>\n      <td>4.897840</td>\n      <td>5.384495</td>\n      <td>8.057377</td>\n      <td>2.197225</td>\n      <td>6.306275</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Grocery':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>75</th>\n      <td>9.923192</td>\n      <td>7.036148</td>\n      <td>1.098612</td>\n      <td>8.390949</td>\n      <td>1.098612</td>\n      <td>6.882437</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>6.432940</td>\n      <td>4.007333</td>\n      <td>4.919981</td>\n      <td>4.317488</td>\n      <td>1.945910</td>\n      <td>2.079442</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Frozen':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>38</th>\n      <td>8.431853</td>\n      <td>9.663261</td>\n      <td>9.723703</td>\n      <td>3.496508</td>\n      <td>8.847360</td>\n      <td>6.070738</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>8.597297</td>\n      <td>9.203618</td>\n      <td>9.257892</td>\n      <td>3.637586</td>\n      <td>8.932213</td>\n      <td>7.156177</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>4.442651</td>\n      <td>9.950323</td>\n      <td>10.732651</td>\n      <td>3.583519</td>\n      <td>10.095388</td>\n      <td>7.260523</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>10.000569</td>\n      <td>9.034080</td>\n      <td>10.457143</td>\n      <td>3.737670</td>\n      <td>9.440738</td>\n      <td>8.396155</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>7.759187</td>\n      <td>8.967632</td>\n      <td>9.382106</td>\n      <td>3.951244</td>\n      <td>8.341887</td>\n      <td>7.436617</td>\n    </tr>\n    <tr>\n      <th>264</th>\n      <td>6.978214</td>\n      <td>9.177714</td>\n      <td>9.645041</td>\n      <td>4.110874</td>\n      <td>8.696176</td>\n      <td>7.142827</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>10.395650</td>\n      <td>9.728181</td>\n      <td>9.519735</td>\n      <td>11.016479</td>\n      <td>7.148346</td>\n      <td>8.632128</td>\n    </tr>\n    <tr>\n      <th>420</th>\n      <td>8.402007</td>\n      <td>8.569026</td>\n      <td>9.490015</td>\n      <td>3.218876</td>\n      <td>8.827321</td>\n      <td>7.239215</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>9.060331</td>\n      <td>7.467371</td>\n      <td>8.183118</td>\n      <td>3.850148</td>\n      <td>4.430817</td>\n      <td>7.824446</td>\n    </tr>\n    <tr>\n      <th>439</th>\n      <td>7.932721</td>\n      <td>7.437206</td>\n      <td>7.828038</td>\n      <td>4.174387</td>\n      <td>6.167516</td>\n      <td>3.951244</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Detergents_Paper':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>75</th>\n      <td>9.923192</td>\n      <td>7.036148</td>\n      <td>1.098612</td>\n      <td>8.390949</td>\n      <td>1.098612</td>\n      <td>6.882437</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>9.428190</td>\n      <td>6.291569</td>\n      <td>5.645447</td>\n      <td>6.995766</td>\n      <td>1.098612</td>\n      <td>7.711101</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Delicatessen':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>66</th>\n      <td>2.197225</td>\n      <td>7.335634</td>\n      <td>8.911530</td>\n      <td>5.164786</td>\n      <td>8.151333</td>\n      <td>3.295837</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>7.248504</td>\n      <td>9.724899</td>\n      <td>10.274568</td>\n      <td>6.511745</td>\n      <td>6.728629</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>4.941642</td>\n      <td>9.087834</td>\n      <td>8.248791</td>\n      <td>4.955827</td>\n      <td>6.967909</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>8.034955</td>\n      <td>8.997147</td>\n      <td>9.021840</td>\n      <td>6.493754</td>\n      <td>6.580639</td>\n      <td>3.583519</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>10.519646</td>\n      <td>8.875147</td>\n      <td>9.018332</td>\n      <td>8.004700</td>\n      <td>2.995732</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>6.432940</td>\n      <td>4.007333</td>\n      <td>4.919981</td>\n      <td>4.317488</td>\n      <td>1.945910</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>10.514529</td>\n      <td>10.690808</td>\n      <td>9.911952</td>\n      <td>10.505999</td>\n      <td>5.476464</td>\n      <td>10.777768</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>5.789960</td>\n      <td>6.822197</td>\n      <td>8.457443</td>\n      <td>4.304065</td>\n      <td>5.811141</td>\n      <td>2.397895</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>7.798933</td>\n      <td>8.987447</td>\n      <td>9.192075</td>\n      <td>8.743372</td>\n      <td>8.148735</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>6.368187</td>\n      <td>6.529419</td>\n      <td>7.703459</td>\n      <td>6.150603</td>\n      <td>6.860664</td>\n      <td>2.890372</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>6.871091</td>\n      <td>8.513988</td>\n      <td>8.106515</td>\n      <td>6.842683</td>\n      <td>6.013715</td>\n      <td>1.945910</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>10.602965</td>\n      <td>6.461468</td>\n      <td>8.188689</td>\n      <td>6.948897</td>\n      <td>6.077642</td>\n      <td>2.890372</td>\n    </tr>\n    <tr>\n      <th>289</th>\n      <td>10.663966</td>\n      <td>5.655992</td>\n      <td>6.154858</td>\n      <td>7.235619</td>\n      <td>3.465736</td>\n      <td>3.091042</td>\n    </tr>\n    <tr>\n      <th>343</th>\n      <td>7.431892</td>\n      <td>8.848509</td>\n      <td>10.177932</td>\n      <td>7.283448</td>\n      <td>9.646593</td>\n      <td>3.610918</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Question 4\n*Are there any data points considered outliers for more than one feature based on the definition above? Should these data points be removed from the dataset? If any data points were added to the `outliers` list to be removed, explain why.*\n\n**Answer:**\n\nData points considered outliers in multiple features:\n- `65` in `Milk` and `Frozen`\n- `75` in `Grocery` and `Detergents_Paper`\n- `154` in `Milk`, `Grocery`, and `Delicatessen`\n- both `66` and `128` were outliers in `Fresh` and `Delicatessen`\n\nI opted to remove these multi-outlier data points from the dataset, since being an extreme spender or saver in a multiple product categories is poorly representative of typical customer's spending habits. I could've opted to remove every data point with an outliers in any single product category, but there were `42` of those, which represents `9.54%` of the data. Since we're unsure about the model's ability to accurately determine what is or is not \"good data\", we can't be very confident about removing so many outliers.\n\n## Feature Transformation\nIn this section you will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.\n\n### Implementation: PCA\n\nNow that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the `good_data` to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the *explained variance ratio* of each dimension \u2014 how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.\n\nIn the code block below, you will need to implement the following:\n - Import `sklearn.decomposition.PCA` and assign the results of fitting PCA in six dimensions with `good_data` to `pca`.\n - Apply a PCA transformation of `log_samples` using `pca.transform`, and assign the results to `pca_samples`.\n\n\n```python\nfrom sklearn.decomposition import PCA\n\n# Apply PCA by fitting the good data with the same number of dimensions as features\npca = PCA(n_components=6)\npca.fit(good_data)\n\n# Transform log_samples using the PCA fit above\npca_samples = pca.transform(log_samples)\n\n# Generate PCA results plot\npca_results = vs.pca_results(good_data, pca)\n```\n\n\n![png](output/output_30_0.png)\n\n\n### Question 5\n*How much variance in the data is explained* ***in total*** *by the first and second principal component? What about the first four principal components? Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending.*\n**Hint:** A positive increase in a specific dimension corresponds with an *increase* of the *positive-weighted* features and a *decrease* of the *negative-weighted* features. The rate of increase or decrease is based on the indivdual feature weights.\n\n\n```python\n# Display cumulative sums of the explained variance ratios\npca_results.cumsum()\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Explained Variance</th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Dimension 1</th>\n      <td>0.4430</td>\n      <td>0.1675</td>\n      <td>-0.4014</td>\n      <td>-0.4381</td>\n      <td>0.1782</td>\n      <td>-0.7514</td>\n      <td>-0.1499</td>\n    </tr>\n    <tr>\n      <th>Dimension 2</th>\n      <td>0.7068</td>\n      <td>-0.5184</td>\n      <td>-0.5686</td>\n      <td>-0.5088</td>\n      <td>-0.3223</td>\n      <td>-0.7938</td>\n      <td>-0.6440</td>\n    </tr>\n    <tr>\n      <th>Dimension 3</th>\n      <td>0.8299</td>\n      <td>-1.1958</td>\n      <td>-0.5284</td>\n      <td>-0.5283</td>\n      <td>-0.0073</td>\n      <td>-1.0055</td>\n      <td>-0.0154</td>\n    </tr>\n    <tr>\n      <th>Dimension 4</th>\n      <td>0.9311</td>\n      <td>-1.4001</td>\n      <td>-0.5156</td>\n      <td>-0.4726</td>\n      <td>0.7781</td>\n      <td>-0.7959</td>\n      <td>-0.5577</td>\n    </tr>\n    <tr>\n      <th>Dimension 5</th>\n      <td>0.9796</td>\n      <td>-1.4027</td>\n      <td>0.2036</td>\n      <td>-0.1172</td>\n      <td>0.7450</td>\n      <td>-1.3541</td>\n      <td>-0.7669</td>\n    </tr>\n    <tr>\n      <th>Dimension 6</th>\n      <td>1.0000</td>\n      <td>-1.3735</td>\n      <td>-0.3366</td>\n      <td>0.7033</td>\n      <td>0.7655</td>\n      <td>-1.5365</td>\n      <td>-0.7472</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n**Answer:**\n\nTotal variance explained by first two PCs: `0.7068`\n\nTotal variance explained by first four PCs: `0.9311`\n\nIn `Dimension 1` a significant positive weight is placed on `Detergents_Paper` with meaningful positive weight on `Milk` and `Grocery`. This dimension is best categorized by customer spending on retail goods.\n\nIn `Dimension 2` a significant positive weight is placed on `Fresh` with meaningful positive weight on `Frozen` and `Delicatessen`. This dimension is best categorized by general customer spending on foods.\n\nIn `Dimension 3` a significant positive weight is placed on `Fresh` and a significant negative weight is placed on `Delicatessen`, with meaningful negative weight on `Frozen`. This dimension is best categorized by customer spending on produce and health foods.\n\nIn `Dimension 4` a significant positive weight is placed on `Delicatessen`, and a significant negative weight is placed on `Frozen`. This dimension is best categorized by customer spending on unique and ethnic foods.\n\n### Observation\nRun the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions. Observe the numerical value for the first four dimensions of the sample points. Consider if this is consistent with your initial interpretation of the sample points.\n\n\n```python\n# Display sample log-data after having a PCA transformation applied\ndisplay(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension 1</th>\n      <th>Dimension 2</th>\n      <th>Dimension 3</th>\n      <th>Dimension 4</th>\n      <th>Dimension 5</th>\n      <th>Dimension 6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.0426</td>\n      <td>-2.5854</td>\n      <td>0.1965</td>\n      <td>-0.0757</td>\n      <td>-1.4213</td>\n      <td>0.0093</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-3.9321</td>\n      <td>-0.8289</td>\n      <td>0.2152</td>\n      <td>0.9953</td>\n      <td>0.3950</td>\n      <td>-0.2858</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.4462</td>\n      <td>-2.7836</td>\n      <td>1.1083</td>\n      <td>-0.3929</td>\n      <td>-1.6836</td>\n      <td>1.5910</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Implementation: Dimensionality Reduction\nWhen using principal component analysis, one of the main goals is to reduce the dimensionality of the data \u2014 in effect, reducing the complexity of the problem. Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the *cumulative explained variance ratio* is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards.\n\nIn the code block below, you will need to implement the following:\n - Assign the results of fitting PCA in two dimensions with `good_data` to `pca`.\n - Apply a PCA transformation of `good_data` using `pca.transform`, and assign the results to `reduced_data`.\n - Apply a PCA transformation of `log_samples` using `pca.transform`, and assign the results to `pca_samples`.\n\n\n```python\n# Apply PCA by fitting the good data with only two dimensions\npca = PCA(n_components=2)\npca.fit(good_data)\n\n# Transform the good data using the PCA fit above\nreduced_data = pca.transform(good_data)\n\n# Transform the log_samples using the PCA fit above\npca_samples = pca.transform(log_samples)\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n```\n\n### Observation\nRun the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions. Observe how the values for the first two dimensions remains unchanged when compared to a PCA transformation in six dimensions.\n\n\n```python\n# Display sample log-data after applying PCA transformation in two dimensions\ndisplay(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension 1</th>\n      <th>Dimension 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.0426</td>\n      <td>-2.5854</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-3.9321</td>\n      <td>-0.8289</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.4462</td>\n      <td>-2.7836</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n## Visualizing a Biplot\nA biplot is a scatterplot where each data point is represented by its scores along the principal components. The axes are the principal components (in this case `Dimension 1` and `Dimension 2`). In addition, the biplot shows the projection of the original features along the components. A biplot can help us interpret the reduced dimensions of the data, and discover relationships between the principal components and original features.\n\nRun the code cell below to produce a biplot of the reduced-dimension data.\n\n\n```python\n# Create a biplot\nvs.biplot(good_data, reduced_data, pca)\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x11d8271d0>\n\n\n\n\n![png](output/output_41_1.png)\n\n\n### Observation\n\nOnce we have the original feature projections (in red), it is easier to interpret the relative position of each data point in the scatterplot. For instance, a point the lower right corner of the figure will likely correspond to a customer that spends a lot on `'Milk'`, `'Grocery'` and `'Detergents_Paper'`, but not so much on the other product categories.\n\nFrom the biplot, which of the original features are most strongly correlated with the first component? What about those that are associated with the second component? Do these observations agree with the pca_results plot you obtained earlier?\n\n**Answer:**\n\n`Fresh`, `Frozen`, and `Delicatessen` are the features most strongly correlated with the `Dimension 1`.\n\n`Detergents_Paper`, `Grocery`, and `Milk` are the features most strongly correlated with the `Dimension 2`.\n\n## Clustering\n\nIn this section, you will choose to use either a K-Means clustering algorithm or a Gaussian Mixture Model clustering algorithm to identify the various customer segments hidden in the data. You will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale.\n\n### Question 6\n*What are the advantages to using a K-Means clustering algorithm? What are the advantages to using a Gaussian Mixture Model clustering algorithm? Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why?*\n\n**Answer:**\n\nK-means:\n- Hard assign a data point to one particular cluster on convergence.\n- It makes use of the euclidean norm when optimizing its centroid coordinates.\n- Speed and simplicity\n\nGaussian Mixture Model:\n- Soft assigns a point to clusters (so its give a probability of any point belonging to any centroid).\n- It doesn't depend on the euclidean norm, but is based on the _expectation_, i.e. the probability of the point belonging to a particular cluster. This makes K-means biased towards spherical clusters.\n\nK-means is more useful when we want certainty in which data points belong in which clusters and we want to produce more spherical clusters. GMM gives you probabilities instead to express the uncertainty, providing more information and flexibility than K-means. In fact K-means is a special case of GMM, where probabilities of being in a certain cluster is mapped to 1, with the rest of mapped to 0.\n\nI think our model might have some hidden non-observable parameters within the wholesale customer data, so I'll be using a gaussian mixture model. Plus I like the flexibility and added information from gaussian mixture models \ud83d\ude01\n\n### Implementation: Creating Clusters\nDepending on the problem, the number of clusters that you expect to be in the data may already be known. When the number of clusters is not known *a priori*, there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data \u2014 if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's *silhouette coefficient*. The [silhouette coefficient](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the *mean* silhouette coefficient provides for a simple scoring method of a given clustering.\n\nIn the code block below, you will need to implement the following:\n - Fit a clustering algorithm to the `reduced_data` and assign it to `clusterer`.\n - Predict the cluster for each data point in `reduced_data` using `clusterer.predict` and assign them to `preds`.\n - Find the cluster centers using the algorithm's respective attribute and assign them to `centers`.\n - Predict the cluster for each sample data point in `pca_samples` and assign them `sample_preds`.\n - Import `sklearn.metrics.silhouette_score` and calculate the silhouette score of `reduced_data` against `preds`.\n   - Assign the silhouette score to `score` and print the result.\n\n\n```python\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\nbest_num_clusters = 0\nbest_score = 0\n\n# Apply your clustering algorithm of choice to the reduced data\nclusterer = GaussianMixture(n_components=2, covariance_type='full')\nclusterer.fit(reduced_data)\n\n# Predict the cluster for each data point\npreds = clusterer.predict(reduced_data)\n\n# Find the cluster centers\ncenters = clusterer.means_\n\n# Predict the cluster for each transformed sample data point\nsample_preds = clusterer.predict(pca_samples)\n\n# Calculate the mean silhouette coefficient for the number of clusters chosen\nscore = silhouette_score(reduced_data, preds)\nprint \"Silhouette coefficient for {} clusters: {:.3f}\".format(2, score)\n```\n\n    Silhouette coefficient for 2 clusters: 0.422\n\n\n### Question 7\n*Report the silhouette score for several cluster numbers you tried. Of these, which number of clusters has the best silhouette score?*\n\n**Answer:**\n\nSilhouette coefficient for `2` clusters: `0.422`\n\nSilhouette coefficient for `3` clusters: `0.318`\n\nSilhouette coefficient for `4` clusters: `0.276`\n\nSilhouette coefficient for `5` clusters: `0.284`\n\nSilhouette coefficient for `6` clusters: `0.328`\n\nSilhouette coefficient for `7` clusters: `0.296`\n\nSilhouette coefficient for `8` clusters: `0.226`\n\nSilhouette coefficient for `9` clusters: `0.323`\n\nThe best number of clusters of the ones I've tried is `2` with a silhousette coefficient of `0.422`\n\n### Cluster Visualization\nOnce you've chosen the optimal number of clusters for your clustering algorithm using the scoring metric above, you can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters.\n\n\n```python\n# Display the results of the clustering from implementation\nvs.cluster_results(reduced_data, preds, centers, pca_samples)\n```\n\n\n![png](output/output_52_0.png)\n\n\n### Implementation: Data Recovery\nEach cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the *averages* of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to *the average customer of that segment*. Since the data is currently reduced in dimension and scaled by a logarithm, we can recover the representative customer spending from these data points by applying the inverse transformations.\n\nIn the code block below, you will need to implement the following:\n - Apply the inverse transform to `centers` using `pca.inverse_transform` and assign the new centers to `log_centers`.\n - Apply the inverse function of `np.log` to `log_centers` using `np.exp` and assign the true centers to `true_centers`.\n\n\n\n```python\n# Inverse transform the centers\nlog_centers = pca.inverse_transform(centers)\n\n# Exponentiate the centers\ntrue_centers = np.exp(log_centers)\n\n# Display the true centers\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Segment 0</th>\n      <td>8953.0</td>\n      <td>2114.0</td>\n      <td>2765.0</td>\n      <td>2075.0</td>\n      <td>353.0</td>\n      <td>732.0</td>\n    </tr>\n    <tr>\n      <th>Segment 1</th>\n      <td>3552.0</td>\n      <td>7837.0</td>\n      <td>12219.0</td>\n      <td>870.0</td>\n      <td>4696.0</td>\n      <td>962.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Question 8\nConsider the total purchase cost of each product category for the representative data points above, and reference the statistical description of the dataset at the beginning of this project. *What set of establishments could each of the customer segments represent?*\n**Hint:** A customer who is assigned to `'Cluster X'` should best identify with the establishments represented by the feature set of `'Segment X'`.\n\n\n```python\nprint 'Average purchase costs for each category:'\nprint data.sum() / 440\n\n# Visualize samples\nimport seaborn as sns\ntrue_centers_bar = true_centers.append(data.describe().loc['mean'])\n_ = true_centers_bar.plot(kind='bar', figsize=(14,6))\n```\n\n    Average purchase costs for each category:\n    Fresh               12000.297727\n    Milk                 5796.265909\n    Grocery              7951.277273\n    Frozen               3071.931818\n    Detergents_Paper     2881.493182\n    Delicatessen         1524.870455\n    dtype: float64\n\n\n\n![png](output/output_56_1.png)\n\n\n**Answer:**\n\nA customer assigned to `Cluster 0` should best itentify with cafes and restaurants. They have nearly identical needs as my `Establishment 2` example, with a need for `Milk` and `Grocery` foods to prepare their foods, and a supply of  `Detergents_Paper` to clean dishes, supply napkins, and generally support their dining experience. Compared to the mean, they'll typically need more `Milk` (7837 vs 5796), `Grocery` foods (12219 vs 7951), and `Detergents_Paper` (4696 vs 2881).\n\nA customer assigned to `Cluster 1` should best identify with the small fresh produce markets. They have nearly identical needs as my `Establishment 1` example, with `Fresh` foods as their primary product, and a small selection of `Frozen` foods, but with reduced volume. Compared to the mean, they'll typically need less of everything across the board, but will have more similar needs of `Fresh` foods compared to the mean (8953 vs 12000) than the rest of the product categories.\n\n### Question 9\n*For each sample point, which customer segment from* ***Question 8*** *best represents it? Are the predictions for each sample point consistent with this?*\n\nRun the code block below to find which cluster each sample point is predicted to be.\n\n\n```python\n# Display the predictions\nfor i, pred in enumerate(sample_preds):\n    print \"Sample point\", i, \"predicted to be in Cluster\", pred\n```\n\n    Sample point 0 predicted to be in Cluster 0\n    Sample point 1 predicted to be in Cluster 1\n    Sample point 2 predicted to be in Cluster 1\n\n\n**Answer:**\n\nMy initial prediction for `Sample point 0` is completely consistent with it's predicted classification of `Cluster 1`. It has a very similar distribution of needs as `Cluster 1`, where your typical fresh produce market has `Fresh` foods as their primary product, and a small selection of `Frozen` foods.\n\nMy initial prediction for `Sample point 1` is completely consistent with it's predicted classification of `Cluster 0`. It has a very similar distribution of needs as `Cluster 0`, where your typical cafe or restaurant needs a good supply `Milk` and `Grocery` ingredients, and a good supply of `Detergents_Paper` to clean & sanitize the place and provide paper napkins for their customers.\n\nIt's hard to say whether my initial prediction for `Sample point 2` is consistent with it's predicted classification of `Cluster 0`. Your typical deli inherently needs a lot of `Delicatessen` foods and some need for `Grocery` and `Fresh` foods. Neither clusters put too much importance on `Delicatessen` foods, and while it's `Grocery` needs matches `Cluster 0`, it's `Fresh` foods needs better matches `Cluster 1`. We can see it's on the border between the two clusters in the cluster visualization (after Question 7), so we can't have much confidence in which cluster it belongs to either way.\n\n## Conclusion\n\nIn this final section, you will investigate ways that you can make use of the clustered data. First, you will consider how the different groups of customers, the ***customer segments***, may be affected differently by a specific delivery scheme. Next, you will consider how giving a label to each customer (which *segment* that customer belongs to) can provide for additional features about the customer data. Finally, you will compare the ***customer segments*** to a hidden variable present in the data, to see whether the clustering identified certain relationships.\n\n### Question 10\nCompanies will often run [A/B tests](https://en.wikipedia.org/wiki/A/B_testing) when making small changes to their products or services to determine whether making that change will affect its customers positively or negatively. The wholesale distributor is considering changing its delivery service from currently 5 days a week to 3 days a week. However, the distributor will only make this change in delivery service for customers that react positively. *How can the wholesale distributor use the customer segments to determine which customers, if any, would react positively to the change in delivery service?*\n**Hint:** Can we assume the change affects all customers equally? How can we determine which group of customers it affects the most?\n\n**Answer:**\n\nWe can run an A/B test by changing the delivery schedule on a sample of each segment individually. Once you get some responses from each sample, you can then identify how much this schedule change affects customers from this customer segment.\n\nIf you find that a cluster's customers respond positively, you can roll out this schedule change to the remaining customers in the cluster. Conversely if you find that a cluster's customers respond negatively, you can rule option that out for that cluster cluster of customers.\n\n### Question 11\nAdditional structure is derived from originally unlabeled data when using clustering techniques. Since each customer has a ***customer segment*** it best identifies with (depending on the clustering algorithm applied), we can consider *'customer segment'* as an **engineered feature** for the data. Assume the wholesale distributor recently acquired ten new customers and each provided estimates for anticipated annual spending of each product category. Knowing these estimates, the wholesale distributor wants to classify each new customer to a ***customer segment*** to determine the most appropriate delivery service.\n*How can the wholesale distributor label the new customers using only their estimated product spending and the* ***customer segment*** *data?*\n**Hint:** A supervised learner could be used to train on the original customers. What would be the target variable?\n\n**Answer:**\n\nYou could use a supervised learner to train on the existing customer base, using annual spending of each product category as it's features and labelling each customer with their respective the customer segment. With this model, you can try to predict which customer segment each of those ten new customers belong to using their estimated product spending and suggest the most appropriate delivery service for them.\n\nWe can actually still use the supervised learner from this exercise to classify these ten new customers to their respective customer segments. In addition, we can always use the predicted labels as an engineered input feature for another supervised learning model to predict something else.\n\n### Visualizing Underlying Distributions\n\nAt the beginning of this project, it was discussed that the `'Channel'` and `'Region'` features would be excluded from the dataset so that the customer product categories were emphasized in the analysis. By reintroducing the `'Channel'` feature to the dataset, an interesting structure emerges when considering the same PCA dimensionality reduction applied earlier to the original dataset.\n\nRun the code block below to see how each data point is labeled either `'HoReCa'` (Hotel/Restaurant/Cafe) or `'Retail'` the reduced space. In addition, you will find the sample points are circled in the plot, which will identify their labeling.\n\n\n```python\n# Display the clustering results based on 'Channel' data\nvs.channel_results(reduced_data, outliers, pca_samples)\n```\n\n\n![png](output/output_68_0.png)\n\n\n### Question 12\n*How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers? Are there customer segments that would be classified as purely 'Retailers' or 'Hotels/Restaurants/Cafes' by this distribution? Would you consider these classifications as consistent with your previous definition of the customer segments?*\n\n**Answer:**\n\nMy Gaussian mixture model with two clusters has quite a similar distribution to the `Hotel/Restaurant/Cafe` & `Retailer` distribution. Along the `Detergents_Paper`, `Grocery`, `Milk` dimension, the more extreme parts of my customer segments would confidently classify as purely `Retailer` or `Hotel/Restaurant/Cafe` by this distribution.\n\nI'd say these classifications are somewhat consistent with my previous definitions of customer segments. `Hotel/Restaurant/Cafe`'s have similar food needs as my `Cafe/Restaurant` definition for `Cluster 1`. And although `Retailer`'s don't might not intuitively fit how I defined `Cluster 0` as a typical \"small fresh produce market\", they `Retailer`s are still kind of a blanket statement to classify things as `Other` and my small fresh produce market definition had low enough volume to not pull in any direction either way.\n"}, {"repo": "devmentors/Pacco.Services.Customers", "language": "C#", "readme_contents": "![Pacco](https://raw.githubusercontent.com/devmentors/Pacco/master/assets/pacco_logo.png)\n\n**What is Pacco?**\n----------------\n\nPacco is an open source project using microservices architecture written in .NET Core 3.1 and the domain tackles the exclusive parcels delivery which revolves around the general concept of limited resources availability. To read more about this project [click here](https://github.com/devmentors/Pacco).\n\n**What is Pacco.Services.Customers?**\n----------------\n\nPacco.Services.Customers is the microservice being part of [Pacco](https://github.com/devmentors/Pacco) solution.\n\n|Branch             |Build status                                                  \n|-------------------|-----------------------------------------------------\n|master             |[![master branch build status](https://api.travis-ci.org/devmentors/Pacco.Services.Customers.svg?branch=master)](https://travis-ci.org/devmentors/Pacco.Services.Customers)\n|develop            |[![develop branch build status](https://api.travis-ci.org/devmentors/Pacco.Services.Customers.svg?branch=develop)](https://travis-ci.org/devmentors/Pacco.Services.Customers/branches)\n\n**How to start the application?**\n----------------\n\nService can be started locally via `dotnet run` command (executed in the `/src/Pacco.Services.Customers` directory) or by running `./scripts/start.sh` shell script in the root folder of repository.\n\nBy default, the service will be available under http://localhost:5002.\n\nYou can also start the service via Docker, either by building a local Dockerfile: \n\n`docker build -t pacco.services.customers .` \n\nor using the official one: \n\n`docker pull devmentors/pacco.services.customers`\n\n**What HTTP requests can be sent to the microservice API?**\n----------------\n\nYou can find the list of all HTTP requests in [Pacco.Services.Customers.rest](https://github.com/devmentors/Pacco.Services.Customers/blob/master/Pacco.Services.Customers.rest) file placed in the root folder of the repository.\nThis file is compatible with [REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client) plugin for [Visual Studio Code](https://code.visualstudio.com). "}, {"repo": "toshi-k/kaggle-santander-customer-satisfaction", "language": "Lua", "readme_contents": "Kaggle Santander Customer Satisfaction\n====\n\nMy solution in this Kaggle competition [\"Santander Customer Satisfaction\"](https://www.kaggle.com/c/santander-customer-satisfaction), 44th place.\n\n![solution](https://raw.githubusercontent.com/toshi-k/kaggle-santander-customer-satisfaction/master/img/solution.png)\n\n"}, {"repo": "cometchat-pro-tutorials/react-customer-support-live-widget", "language": "JavaScript", "readme_contents": "# Build a Customer Support Live Chat Widget with React\n\n\nRead the full tutorial here:\n\n[**>> Build a Customer Support Live Chat Widget with React**](https://www.cometchat.com/tutorials/build-a-customer-support-live-chat-widget-with-react/?utm_source=github&utm_medium=example-code-readme)\n\nThis demo app shows how to build an Android group chat app with React:\n\n![Client](screenshot/screenshot_1.png)\n![Agent](screenshot/screenshot_2.png)\n\n\n## Running the demo\n\nTo run the demo first setup CometChat:\n\n1. Head to [CometChat Pro](https://cometchat.com/pro?utm_source=github&utm_medium=example-code-readme) and create an account\n2. From the [dashboard](https://app.cometchat.com/#/apps?utm_source=github&utm_medium=example-code-readme), create a new app called \"React chat widget\"\n3. One created, click **Explore**\n4. Go to the **API Keys** tab and click **Create API Key**\n5. Create an API key called \"React chat widget key\" with **Full Access**\n6. Go to the **Users** tab and click **Create User**\n7. Create a user with the name \"Agent\" and the UID \"Agent\"\n6. Download the repository [here](https://github.com/cometchat-pro-samples/react-customer-support-live-widget/archive/master.zip) or by running `git clone https://github.com/cometchat-pro-samples/react-customer-support-live-widget.git`\n\nSetup the server:\n\n1. In the root directory run `npm install`\n2. Open [sever.js](https://github.com/cometchat-pro-samples/react-customer-support-live-widget/blog/master/sever.js) and update [`appID`](https://github.com/cometchat-pro-samples/react-customer-support-live-widget/blob/master/server.js#L5), [`apiKey`](https://github.com/cometchat-pro-samples/react-customer-support-live-widget/blob/master/server.js#L6) to use your own credentials\n3. Set [`agentUID`](https://github.com/cometchat-pro-samples/react-customer-support-live-widget/blob/master/server.js#L7) to \"Agent\"\n3. Run the server by running `node server.js`\n\nSetup the client:\n\n1. Go to the `client` directory\n2. Run `npm install` there too\n3. Update [config.js](https://github.com/cometchat-pro-samples/react-customer-support-live-widget/blob/master/client/src/config.js) with your credentials too\n4. In another terminal run `npm start` to start the client\n\nQuestions about running the demo? [Open an issue](https://github.com/cometchat-pro-samples/react-customer-support-live-widget/issues). We're here to help \u270c\ud83c\udffb\n\n\n## Useful links\n\n- \ud83c\udfe0 [CometChat Homepage](https://cometchat.com/pro?utm_source=github&utm_medium=example-code-readme)\n- \ud83d\ude80 [Create your free account](https://app.cometchat.com?utm_source=github&utm_medium=example-code-readme)\n- \ud83d\udcda [Documentation](https://prodocs.cometchat.com/docs?utm_source=github&utm_medium=example-code-readme)\n- \ud83d\udc7e [GitHub](https://github.com/CometChat-Pro)\n"}, {"repo": "celestinhermez/sparkify_customer_churn", "language": "HTML", "readme_contents": "# Predicting Customer Churn\n\nThe goal of this repository is to show how we can predict customer churn with Spark.\nAlthough we are using Spark in local mode here, and technically the data could be \nanalyzed on a single machine, we process the data and build the model with Spark in order\nto create an extensible framework. The analysis conducted here could scale to much\nbigger datasets, provided the code be deployed on a cluster capable of handling the \ncomputations necessary. \n\nSparkify is an imaginary music app company, and we use a small subset of their log\ndata to try and predict churn. More context on this analysis and interpretation of\nthe results can be found in [this blog post on Medium](https://medium.com/@celestinhermez/predicting-customer-churn-with-spark-4d093907b2dc).\n\n## File Structure\n\n* **mini_sparkify_event_data.json**: this is the log data that we use in this example.\nIt contains information about 226 distinct users, with actions as detailed as giving\na thumbs up to a song or changing the settings of the account\n* **Sparkify.ipynb**: this is the Spark notebook which contains all the code for this \nanalysis. In order to run it, `pyspark`, `pandas`, `matplotlib`, `seaborn` and\n`datetime` have to be installed\n* **Sparkify.html**: an HTML version of the notebook\n* **images**: the three png files are the graphs included in the Medium blog post\n\n## Analysis\n\nAfter loading and cleaning the data, we create features, both related to the nature of the \naccount (paid vs. free, state, registration date) and to behaviors taken on platform\n(thumbs up, creation of playlists, number of songs per session etc.). We engineer\nthese features with Spark and leverage the `Pipeline` class to efficiently process the data.\nOne processing step which is particularly important is to upsample the positive class\nsince the proportion of users who churned is small and this could bias the accuracy \nof our model. To do so, we sample with replacement from the population of users who\nchurned.\n\nWe then test out various classification models (`LogisticRegression`, `RandomForestClassifier`,\n`GBTClassifier`) and compare their accuracy and F1 score on the test set. From there,\nwe choose to tune a logistic regression model further through a `CrossValidator` \nperforming the GridSearch algorithm on 3 folds, with the accuracy as the optimization\nmetric. We choose to tune the logistic regression because of the interpretability of \nits coefficients. \n\nAfter hyperparameter tuning, we have a model which has 73% accuracy on the test set,\nwith a F1 score of 0.7. This means we are not disproportionately predicting one of the classes and\nhave a good balance between precision and recall. Interestingly, there is no improvement\nafter hyperparameter tuning through Grid Search, most likely due to the small size\nof our train set (191 users). \nExamining feature importance, we conclude that both static features (such as the state\nor the registration month) as well as on-platform behaviors (adding a friend) matter\nwhen making a prediction. Getting this insight is crucial for Sparkify, because they can\ntarget these users at risk with special offers (discounts, personalized messages) to try  \nand mitigate the churn which is likely to happen. Internally, this could be automated\nby running the model regularly (every day/week) and flagging users at risk.\n\n## Possible Improvements\n\nThis analysis would gain from leveraging a larger dataset and being deployed on a cluster.\nGrid search is a particularly computationally expensive operation, but with larger resources and more time\na more extensive search over a larger dataset and hyperparameter space\ncould be conducted to further tune the model and likely improve overall accuracy.\n\nMoreover, this model should not be static but run somewhat regularly as user behaviors\nand the consumer base evolve. It is important not to rely on an outdated model for such\nan important aspect of the business.\n\nFinally, some A/B tests could be set up to examine the insights of this model, \nin particular the resulting mitigating actions taken. One possibility would be \nto find users identified as potential churners, split them into\na control and treatment group, assign some \"churn-mitigating\" treatment and compare their \nchurn rates through statistical hypothesis testing. This approach would be a rigorous\nfollow-up to this model. "}, {"repo": "Ifihan/customer-management-app", "language": "Python", "readme_contents": "# Customer Management App\n\n## Framework\nDjango"}, {"repo": "skioo/django-customer-billing", "language": "Python", "readme_contents": "django-customer-billing\n============\n\n[![Build Status](https://travis-ci.org/skioo/django-customer-billing.svg?branch=master)](https://travis-ci.org/skioo/django-customer-billing)\n[![PyPI version](https://badge.fury.io/py/django-customer-billing.svg)](https://badge.fury.io/py/django-customer-billing)\n[![Requirements Status](https://requires.io/github/skioo/django-customer-billing/requirements.svg?branch=master)](https://requires.io/github/skioo/django-customer-billing/requirements/?branch=master)\n[![Coverage Status](https://coveralls.io/repos/github/skioo/django-customer-billing/badge.svg?branch=master)](https://coveralls.io/github/skioo/django-customer-billing?branch=master)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n\n\nRequirements\n------------\n\n* Python: 3.6 and over\n* Django: 2.2 and over\n\nInstallation\n------------\n\n```\npip install django-customer-billing\n```\n\nUsage\n-----\n\nAdd billing to your `INSTALLED_APPS`:\n\n    INSTALLED_APPS = (\n        ...\n        'billing.apps.BillingConfig',\n        'import_export',\n        ...\n    )\n\n\nRun the migrations: \n\n    ./manage.py migrate\n\n\nDevelopment\n-----------\n\nTo install all dependencies:\n\n    pip install -e .\n    \nTo run unit tests:\n\n    pip install pytest-django\n    pytest\n\nTo lint, typecheck, unit test:\n\n    tox\n\nTo generate a diagram representing the state-machines:\n\n    pip install graphviz\n    ./manage.py graph_transitions -o docs/state-machines.png\n\n\nTo install the version being developed into another django project:\n\n    pip install -e <path-to-this-directory>\n"}, {"repo": "anqi666/Customer-Data-Analysis", "language": "Jupyter Notebook", "readme_contents": "# Customer Data Analysis\n\nThis project is about my reflections and practices on analyzing customer data to learn their behaviors and preferences for making strategic and tactical business decisions.\n\nDone:\n\n- [Sales Prediction of Time-Series Data](/time-series-sales-prediction)\n- [Promotion Marketing Data Analysis](/promotion-marketing-prediction)\n- [Insights About World-wide Food Producers](/food-producers-insights)\n- [Black Friday Regression Analysis](/black-friday-regression)\n\nWorking in Progress:\n\n- More...\n\nPlanning:\n\n- More...\n"}, {"repo": "mutedblues/Customer-Success-Management", "language": null, "readme_contents": "<img src=\"https://raw.githubusercontent.com/mutedblues/Customer-Success-Management/master/csm-logo.png\">\n\n# Customer Success Management\n## About\nA curated list of customer success management resources. The goal is to list information that's helpful for defining the role and building your career, understanding the industry, as well as resources for articles, interviews, presentations, books, software, and case studies. This is just the beginning, so I'll continue to add more info here as the project takes shape.\n\nIf you're interested in contributing, please see the guide here [coming soon].\n\nCredit for inspiration goes to the awesome [Open Product Management](https://github.com/ProductHired/open-product-management) repo created by Nicholas Ivanecky.\n\n## Content\n\n- [Core 1: Customer Success Management](#core-1-customer-success-management)\n    - [What is customer success management?](#what-is-customer-success-management)\n    - [Becoming a Customer Success Manager](#becoming-a-customer-success-manager)\n    - [Getting the job done](#getting-the-job-done)\n    - [Measuring customer success](#measuring-customer-success)\n        - [Articles](#general-articles)\n        - [Presentations](#general-presentations)\n        - [ABF](#abf)\n        - [CLV](#clv)\n        - [NPS](#nps)\n        - [RFM](#rfm)\n    - [Organizing your team](#organizing-your-team)\n    - [Customer fit](#customer-fit)\n    - [Mapping the customer journey](#mapping-the-customer-journey)\n    - [Onboarding](#onboarding)\n- [Core 2: Resources](#core-2-resources)\n    - [Dedicated software](#dedicated-software)\n    - [Books](#books)\n    - [Newsletters](#newsletters)\n    - [Blogs](#blogs)\n    - [Podcasts](#podcasts)\n    - [Training](#training)\n- [Core 3: Real life customer success](#core-3-real-life-customer-success)\n    - [Interviews](#interviews)\n    - [Case studies](#case-studies)\n    - [Communities to join](#communities-to-join)\n        - [Associations](#associations)\n        - [Groups](#groups)\n        - [Conferences](#conferences)\n    - [Thought leaders](#thought-leaders)\n- [Core 4: Joining the team](#core-4-joining-the-team)\n    - [Interview prep](#interview-prep)\n        \n# Core 1: Customer Success Management\n## What is customer success management?\n- [ ] [The 8 Elements of Customer Success Management](https://sixteenventures.com/elements-customer-success-management) - Lincoln Murphy\n> Customer success is when your customers achieve their desired outcome through their interactions with your company.\n- [ ] [The Definition of Customer Success](https://www.customersuccessassociation.com/library/the-definition-of-customer-success/) - The Customer Success Association\n> Customer Success is a long-term, scientifically engineered, and professionally directed strategy for maximizing customer and company sustainable proven value.\n- [ ] [The Essential Guide to Customer Success](https://www.gainsight.com/guides/the-essential-guide-to-customer-success/) - Gainsight\n> The business methodology of ensuring customers achieve their desired outcomes while using your product or service. Customer Success is relationship-focused client management, that aligns client and vendor goals for mutually beneficial outcomes. Effective Customer Success strategy typically results in decreased customer churn and increased upsell opportunities.\n- [ ] [What is Customer Success?](https://mindtouch.com/resources/what-is-customer-success) - The Mindtouch Digital Team\n> Customer success encompasses the ongoing efforts of an organization to continue delivering value to its customers. A good customer success program aims to deliver value throughout every step of the customer journey, from pre-purchase to post-sale and beyond. This can include (but isn\u2019t limited to) onboarding, product training, customer service and support.\n- [ ] [Wikipedia: Customer Success](https://en.wikipedia.org/wiki/Customer_success)\n> Customer success is the function at a company responsible for managing the relationship between a vendor and its customers. The goal of customer success is to make the customer as successful as possible, which in turn, improves customer lifetime value (CLTV) for the company.\n\n## Becoming a Customer Success Manager\n- [ ] [The Essential Guide to Landing Your First Customer Success Job](https://www.saleshacker.com/customer-success-job/) - Todd Eby.\n- [ ] [First 90 days in customer success management](https://blog.natero.com/first-90-days-in-customer-success) - Brooke Goodbary.\n- [ ] [How to Start a Customer Success Program from Scratch](https://www.wootric.com/blog/how-to-start-a-customer-success-program-from-scratch/) - Nichole Elizabeth DeMer\u00e9.\n- [ ] [Keys to become a successful customer success manager](https://www.clicdata.com/blog/keys-to-become-a-successful-customer-success-manager/) - Shree Neve.\n- [ ] [Understanding the customer success manager role](http://brooke.land/customer-success-manager/) - Brooke Goodbary.\n- [ ] [What Is a Customer Success Manager?](https://hbr.org/2019/11/what-is-a-customer-success-manager) - Andris A. Zoltners, PK Sinha, and Sally E. Lorimer.\n\n## Getting the job done\n- [ ] [9 Things Customer Success is Not](https://sixteenventures.com/customer-success-is-not) - Lincoln Murphy.\n- [ ] [Customer Success: Nearly everything you need to know](https://www.typeform.com/blog/guides/customer-success/) - Eric Johnson.\n- [ ] [How a Single-Page Success Plan Produced Big Customer Value](https://successcoaching.co/blog/2019/1/19/single-page-success-plan) - Chad Jasmin.\n- [ ] [Managing Customer Success to Reduce Churn](https://www.forentrepreneurs.com/customer-success/) - David Skok at [for Entrepreneurs](https://www.forentrepreneurs.com/).\n- [ ] [What Does a Wildly Successful CSM Look Like?](https://www.linkedin.com/pulse/what-does-wildly-successful-csm-look-like-brett-andersen/) - Brett Andersen, the fourth article in a five-part series that helps organizations define the Customer Success Manager role.\n\n## Measuring customer success\n### General articles\n- [ ] [10 Customer Success KPIs every SaaS company should track](https://customersuccessbox.com/blog/10-customer-success-kpis/) - Nilesh Surana.\n\n### General presentations\n- [ ] [The Secret to Subscription Success](https://access.gainsight.com/20150225-saas-metrics) - Gainsight webinar, discusses churn rate, LTV, CAC, and more.\n\n### ABF\n- [ ] [The Account Behaviour Formula](https://www.csinfocus.com/account-behaviour-formula) - Presentation by Ben Winn at the 2018 [CS100 Conference](http://cs100.clientsuccess.com/) on measuring customer health to improve the churn rate.\n\n### CLV\n- [ ] [The Only Metric That Matters: Customer Lifetime Value](https://hub.appirio.com/cloud-powered-blog/the-only-metric-that-matters-customer-lifetime-value) - Michael Collins.\n\n### NPS\n- [ ] [4 NPS Challenges for Customer Success Teams](https://www.clientsuccess.com/blog/4-nps-challenges-for-customer-success-teams/)\n- [ ] [How SaaS Companies Do NPS: Learning from Customer Success at HelloSign and Optimizely](https://www.wootric.com/blog/how-saas-companies-do-nps-learning-from-customer-success-at-hellosign-and-optimizely/)\n- [ ] [How to leverage NPS to build advocacy](https://www.youtube.com/watch?v=CDTXdRPpgjg&feature=youtu.be) - Irit Eizips, Chief Customer Officer & CEO of CSM Practice speaks at SparrowCast.\n- [ ] [A modern approach to measuring and boosting customer loyalty with NPS](https://www.natero.com/nps-customer-success-webinar/) - Webinar presentation by Lisa Abbott at Wootric, goes into detail on [high-touch, medium-touch, and low-touch strategies](http://cloud.skyver.ge/98c49c5c8a8a).\n- [ ] [NPS in SaaS: What You Need To Know About Net Promoter Score](https://useriq.com/blogs/nps-best-practices-improve-saas-customer-success/)\n\n### RFM\n- [ ] [Bringing a Paradigm Shift to Customer Lifecycle Management with RFM](https://clevertap.com/blog/bringing-a-paradigm-shift-to-customer-lifecycle-management-with-rfm/) - Sunil Thomas.\n\n## Organizing your team\n- [ ] [SaaS Role Definitions](https://winningbydesign.com/saas-role-definitions/) - Sari Eisendrath at Winning by Design, proposes a \"bow-tie\" model for developing roles within the marketing, sales, and customer success teams.\n\n## Customer fit\n- [ ] [A B2B Startup\u2019s Guide to Creating an Ideal Customer Profile (ICP)](https://www.propellercrm.com/blog/ideal-customer-profile-icp)\n- [ ] [How to Identify, Attract, and Nurture Your Ideal Customer](https://mindtouch.com/resources/how-customer-centric-companies-attract-the-perfect-customer) - Brad Smith.\n\n## Onboarding\n- [ ] [5 Actionable Tactics to Create Onboarding Emails That Convert](https://www.getcloudapp.com/blog/convert-onboarding-emails) - Irina Maltseva\n\n# Core 2: Resources\n## Dedicated software\n- [ ] [Akita](https://www.akitaapp.com/)\n- [ ] [Amity](https://getamity.com/customer-success/)\n- [ ] [ChurnZero](https://churnzero.net/)\n- [ ] [ClientSuccess](https://www.clientsuccess.com/)\n- [ ] [CustomerSuccessBox](https://customersuccessbox.com/)\n- [ ] [Gainsight](https://www.gainsight.com/)\n- [ ] [Natero](https://www.natero.com/)\n- [ ] [ProfitWell](https://www.profitwell.com/)\n- [ ] [Strikedeck](https://strikedeck.com/)\n- [ ] [Totango](https://www.totango.com/)\n- [ ] [UserIQ](https://useriq.com/platform/)\n- [ ] [Vitaly](https://vitally.io/)\n\n## Books\n- [ ] [Chief Customer Officer 2.0: How to Build Your Customer-Driven Growth Engine](https://www.amazon.com/Chief-Customer-Officer-2-0-Customer-Driven/dp/1119047609/) - Jeanne Bliss\n- [ ] [A Complaint Is a Gift: Recovering Customer Loyalty When Things Go Wrong](https://www.amazon.com/Complaint-Gift-Recovering-Customer-Loyalty-ebook/dp/B005M0CZV0/) - Janelle Barlow and Claus M\u00f8ller\n- [ ] [Customer Experience 3.0: High-Profit Strategies in the Age of Techno Service](https://www.amazon.com/Customer-Experience-3-0-High-Profit-Strategies/dp/081443388X/) - John Goodman\n- [ ] [Customer Success: How Innovative Companies Are Reducing Churn and Growing Recurring Revenue](https://www.amazon.com/Customer-Success-Innovative-Companies-Recurring/dp/1119167965) - Nick Mehta (CEO of Gainsight), Dan Steinman, and Lincoln Murphy\n- [ ] [Delivering Happiness: A Path to Profits, Passion, and Purpose](https://www.amazon.com/gp/product/0446576220/) - Tony Hsieh, CEO of Zappos.com\n- [ ] [The Effortless Experience: Conquering the New Battleground for Customer Loyalty](https://www.amazon.com/Effortless-Experience-Conquering-Battleground-Customer/dp/1591845815) - Matthew Dixon, Nick Toman, Rick DeLisi\n- [ ] [Farm Don\u2019t Hunt: The Definitive Guide to Customer Success](https://www.amazon.com/Farm-Dont-Hunt-Definitive-Customer/dp/0692620931) - Guy Nirpaz, Fernando Pizarro\n- [ ] [From Impossible to Inevitable](https://www.amazon.com/Impossible-Inevitable-Hyper-Growth-Companies-Predictable/dp/1119166713) - Aaron Ross and Jason Lemkin\n- [ ] [HYPERGROWTH: How the Customer-Driven Model is Revolutionizing the Way Businesses Build Products, Teams, & Brands](https://www.amazon.com/HYPERGROWTH-Customer-Driven-Revolutionizing-Businesses-Products/dp/1520743777) - David Cancel\n- [ ] [Never Lose a Customer Again: Turn Any Sale into Lifelong Loyalty in 100 Days](https://www.amazon.com/Never-Lose-Customer-Again-Lifelong/dp/0735220034) - Joey Coleman\n- [ ] [Practical Customer Success Management: A Best Practice Framework for Rapid Generation of Customer Success](https://www.amazon.com/Practical-Customer-Success-Management-Generation/dp/0367182769/) - Rick Adams\n- [ ] [The Relationship Economy: Building Stronger Customer Connections in the Digital Age](https://www.amazon.com/Relationship-Economy-Building-Stronger-Connections-ebook/dp/B07WNYCDJJ/) - John R. DiJulius III\n- [ ] [The Startup's Guide to Customer Success: How to Champion the Customer at Your Company](https://www.amazon.com/Startups-Guide-Customer-Success-Champion/dp/1641371889/) - Jennifer Chiang\n- [ ] [Subscribed: Why the Subscription Model Will Be Your Company's Future - and What to Do About It](https://www.amazon.com/Subscribed-Subscription-Model-Companys-Future/dp/0525536469/) - Tien Tzuo, Gabe Weisert\n- [ ] [Switch: How to Change Things When Change Is Hard](https://www.amazon.com/Switch-Change-Things-When-Hard/dp/B0038NLX9S/) - Dan Heath and Chip Heath\n- [ ] [The Ten Principles Behind Great Customer Experiences](https://www.amazon.com/Principles-Behind-Customer-Experiences-Financial-ebook/dp/B00BFSMXGU/) - Matt Watkinson\n- [ ] [To Sell Is Human: The Surprising Truth About Moving Others](https://www.amazon.com/Sell-Human-Surprising-Moving-Others/dp/1594631905/) - Daniel H. Pink\n- [ ] [The Trusted Advisor](https://www.amazon.com/Trusted-Advisor-David-H-Maister/dp/0743212347) - David H. Maister, Charles H. Green, and Robert M. Galford\n- [ ] [What Customers Crave: How To Create Relevant And Memorable Experiences At Every Touchpoint](https://www.amazon.com/What-Customers-Crave-Experiences-Touchpoint-ebook/dp/B01GW9XH7K/) - Nicholas J. Webb\n- [ ] [Why Customers Leave (and How to Win Them Back): (24 Reasons People are Leaving You for Competitors, and How to Win Them Back*)](https://www.amazon.com/Why-Customers-Leave-Them-Back-ebook/dp/B07JCMQ4MD) - David Avrin\n\n### Professional development\n- [ ] [The Fast Forward MBA in Project Management](https://www.amazon.com/Fast-Forward-MBA-Project-Management/dp/1119086574) - Eric Verzuh\n\n## Newsletters\n- [ ] [Akita's Customer Success Newsletter](https://www.akitaapp.com/newsletter)\n- [ ] [ClientSuccess Newsletter](https://www.clientsuccess.com/blog/)\n- [ ] [The Customer Success Newsletter](https://alex-newsletter-8.ongoodbits.com/) - Curated by Alex Bakula-Davis, Director of Customer Success at [Copper Inc.](https://www.copper.com/)\n- [ ] [Gainsight's Newsletter](https://www.gainsight.com/customer-success-management/)\n- [ ] [Strikedeck's Newsletter](https://strikedeck.com/customer-success-blogs/)\n- [ ] [Totango's Newsletter](https://blog.totango.com/)\n\n## Blogs\n- [ ] [CSM Practice](https://www.csmpractice.com/blog/)\n- [ ] [Customer Bliss](https://www.customerbliss.com/blog/) - Insights from Jeanne Bliss, writer of [Would you do that to your mother?](https://www.customerbliss.com/would-you-do-that-to-your-mother-the-book-experience/)\n- [ ] [CustomerThink](http://customerthink.com/) - \"The world's largest online community dedicated to customer-centric business strategy\".\n- [ ] [Enlightened Customers](https://enlightenedcustomers.com/)\n- [ ] [Natero's Blog](https://blog.natero.com/)\n- [ ] [Sixteen Ventures](https://sixteenventures.com/)\n\n## Podcasts\n- [ ] [CHURN.FM](https://www.churn.fm/) - \"The podcast for subscription economy pros to learn how the world's fastest growing companies are tackling churn and using retention to fuel their growth.\" New episodes every Wednesday.\n- [ ] [Customer Strategy Podcast](https://podcast.glideconsultingllc.com/) - Weekly podcast hosted by Nils Vinje, founder of Glide Consulting.\n- [ ] [Customer Success Conversations](https://www.customersuccessmanager.com/cs-conversations.html) - Hosted by the founder of CustomerSuccessManager.com, Adam Joseph, he interviews a Customer Success executive in each episode on how to be successful, overcome challenges, and trends influencing the industry.\n\n## Training\n- [ ] [CSM Bootcamp](https://www.csmpractice.com/customer-success-bootcamp/) - Offered by CSM Practice.\n- [ ] [Tri Tuns](https://trituns.com/customer-success-training/) - Offers training and certification for early career, leaders, and teams.\n\n## Thought leaders\n- [ ] [Peter Armaly](https://www.linkedin.com/in/peterarmaly/detail/recent-activity/) - Senior Director, Customer Success at Oracle.\n- [ ] [Alex Bakula-Davis](https://twitter.com/AlexBDavis) - Director of Customer Success at Copper Inc.\n- [ ] [Catherine Blackmore](https://www.linkedin.com/in/catherineblackmore/detail/recent-activity/) - Head of SaaS Customer Success at Oracle.\n- [ ] [Mikael Blaisdell](https://www.linkedin.com/in/mikaelblaisdell/detail/recent-activity/) - Executive Director: The Customer Success Association. \n- [ ] [Julie Devaney Hogan](https://twitter.com/jalicedev) - VP Customer Success & Services at Drift.\n- [ ] [Todd Eby](https://twitter.com/toddceby) - Co-Founder of SuccessHACKER.\n- [ ] [Irit Eizips](https://twitter.com/iriteizips) - Chief Customer Officer & CEO of CSM Practice.\n- [ ] [Brooke Goodbary](https://twitter.com/bgbary) - Customer Success consultant, writer, and expert.\n- [ ] [Kristen Hayer](https://www.linkedin.com/in/kristenhayer/detail/recent-activity/)\n- [ ] [Lincoln Murphy](https://twitter.com/lincolnmurphy) -  Founder of Sixteen Ventures, a SaaS Growth Consultancy.\n- [ ] [Guy Nirpaz](https://twitter.com/guynirpaz) - Founder and CEO of Totango.\n- [ ] [Donna Weber](https://www.linkedin.com/in/donnaweb/) - President of Springboard Solutions Consulting.\n\n\n# Core 3: Real life customer success\n## Interviews\n- [ ] [An agile manifesto for customer success \u2013 Interview with Todd Eby of Success Hacker](http://customerthink.com/an-agile-manifesto-for-customer-success-interview-with-todd-eby-of-success-hacker/) - A conversation between Adrian Swinscoe, of CustomerThink, and Todd Eby of Success Hacker. \"At it\u2019s heart, customer success is about understanding why your customer hired you, what are they attempting to achieve and then doing all that you can to help them achieve that.\" \n## Communities to join\n### Associations\n- [ ] [The Customer Success Association](https://www.customersuccessassociation.com/) - The free tier provides access to more \"library\" materials, with additional paid tiers available.\n\n### Groups\n- [ ] [The Customer Success Forum](https://www.linkedin.com/groups/1913401/) - Largest online community, with nearly 36k members.\n- [ ] [Customer Success Managers in Action](https://www.linkedin.com/groups/891087/) - LinkedIn group with the goal \"to bring together ideas, strategies and tactics from Customer Success professionals around the globe in order to elevate the profession of Customer Success\".\n- [ ] [OUTCOMES: The Customer Success Community](https://www.customersuccess.community/) - Active community at nearly 7,000 members and growing. Keep up with the latest news and events, as well as interact/share/learn with others in the field.\n- [ ] [r/CustomerSuccess](https://www.reddit.com/r/CustomerSuccess/) - Subreddit for anyone involved in the fields of Customer Success, Customer Support, Customer Experience, Product Management, etc.\n- [ ] [Support Driven](https://www.supportdriven.com/register/) - Primarily support focused, however they have a dedicated CSM channel that is becoming more popular.\n\n### Conferences\n- [ ] [CS100](https://cs100.clientsuccess.com/) - \"The Premier Conference for Customer Success Executives & Leaders\".\n- [ ] [Customer SuccessCon](https://www.customersuccessassociation.com/customer-successcon-schedule/) - Annual one-day events held in Seattle, Boston, London, Oakland, and Denver.\n- [ ] [Pulse](https://www.gainsight.com/pulse/) - Annual events held in North America and Europe.\n\n# Core 4: Joining the team\n## Interview prep\n- [ ] [The Most Important Traits & Skills To Look For When Hiring a Customer Success Manager](https://customer-success.getamity.com/amity-blog/the-most-important-traits-skills-to-look-for-when-hiring-a-customer-success-manager)\n\n"}, {"repo": "ho-nl/magento1-Ho_Customer", "language": "PHP", "readme_contents": "# Ho_Customer\n\n### Features:\n- Automatically creates customers from guest orders. This also links guest orders to customers if the email address matches with an account.\n- Adds better support for the customer increment_id (the customer/create_account/generate_human_friendly_id setting)\n\n### Convert all existing orders to customers:\nThere is a non-scheduled cron job which can be scheduled to convert all existing guest orders. You may also do this\nthrough the supplied shell script, optionally only for orders after a given entity ID (use the -start option for this).\n"}, {"repo": "aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship", "language": "Jupyter Notebook", "readme_contents": "# Datalogy-Customer-Segmentation-Data-Science-Internship\n\n## About\n\nTo be added.\n\n## \ud83d\udcd6 Table Of Contents\n\n| Notebook | Link To Notebook |\n|--------|--------|\n|HAC + KMeans Clustering|<a href=\"https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/HAC_Kmeans_Clustering.ipynb\">Click To View</a>|\n\n| Dataset | Link To Dataset |\n|--------|--------|\n|Credit Card Customer Data|<a href=\"https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/Credit%20Card%20Customer%20Data.xlsx\">Click To View</a>|\n\n## \ud83d\udcd6 Resources\n\n|Resource Name|Link To Resource|\n|----------|----------|\n|Heirarchical Clustering Notes|<a href=\"https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/Resources/10.1%20-%20Hierarchical%20Clustering%20_%20STAT%20555.pdf\">Click To View</a>|\n|Examples Of Heirarchical Clustering Notes|<a href=\"https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/Resources/10.2%20-%20Example_%20Agglomerative%20Hierarchical%20Clustering%20_%20STAT%20555.pdf\">Click To View</a>|\n|PowerPoint Presentation On Clustering|<a href=\"https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/Resources/Clustering.pptx\">Click to View PPT</a>|\n\n## \ud83c\udf93 Achievement + Internship Certificate\n<a href=\"https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/assets/Datalogy-Certificate%20Of%20Completion%5BArya%20Shah%5D.pdf\">Click to View Internship Completion Certificate</a>\n\nThe mode of selection for the internship was a scholarship quiz where I ranked 2nd Place and joint full score winner.\n \n![Internship Selection Result](https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/assets/Internship%20Selection%20Result.png)\n\n![Recommendation Letter](https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/assets/Recommendation%20Letter%20-%20Arya%20Shah.jpg)\n\n![certificate](https://github.com/aryashah2k/Datalogy-Customer-Segmentation-Data-Science-Internship/blob/main/assets/Certificate%20Image.png)\n\n\t\n|Donate/Support|View My Work(In Progress!)|Follow / Connect With Me|Follow / Connect With Me|\n|-----|-----|-----|-----|\n|<a href=\"https://www.patreon.com/bePatron?u=45451225\"><img align=\"left\" alt=\"Arya Shah - Patreon\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/patreon.svg\" /></a><a href=\"https://ko-fi.com/aryashah\"><img align=\"left\" alt=\"Arya Shah - Ko-Fi\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/ko-fi.svg\" /></a>|<a href=\"https://aryashah.hashnode.dev\"><img align=\"left\" alt=\"Arya's Hashnode Blog\" width=\"30px\" src=\"https://github.com/aryashah2k/aryashah2k/blob/main/assets/hashnode.svg\" /></a><a href=\"https://www.kaggle.com/aryashah2k\"><img align=\"left\" alt=\"Arya's Kaggle\" width=\"30px\" src=\"https://github.com/aryashah2k/aryashah2k/blob/main/assets/kaggle-icon.svg\" /></a><a href=\"https://dockship.io/author/aryash-095\"><img align=\"left\" alt=\"Arya's Dockship\" width=\"80px\" src=\"https://github.com/aryashah2k/aryashah2k/blob/main/assets/dockship-logo.png\" /></a>|<a href=\"https://twitter.com/aryashah2k\"><img align=\"left\" alt=\"Arya Shah - Twitter\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/twitter.svg\" /></a><a href=\"https://www.instagram.com/arya_shah_00/\"><img align=\"left\" alt=\"Arya's Instagram\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/instagram.svg\" /></a><a href=\"https://www.reddit.com/user/aryashah2k/\"><img align=\"left\" alt=\"Arya's Reddit\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/reddit.svg\" /></a><a href=\"mailto:aryashah2k@gmail.com\"><img align=\"left\" alt=\"Arya's Person Email\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/gmail.svg\" /></a>|<a href=\"https://www.linkedin.com/in/arya--shah/\"><img align=\"left\" alt=\"Arya's LinkedIn\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/linkedin.svg\" /></a><a href=\"https://stackoverflow.com/users/13949231/aryashah2k\"><img align=\"left\" alt=\"Arya's Stackoverlfow\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/stackoverflow.svg\"/></a><a href=\"mailto:arya.shah82@nmims.edu.in\"><img align=\"left\" alt=\"Arya's Institute Email\" width=\"30px\" src=\"https://github.com/edent/SuperTinyIcons/blob/master/images/svg/gmail.svg\" /></a>|"}, {"repo": "Horizon733/customer-care-chatbot", "language": "Python", "readme_contents": "<p align=\"center\"><img src=\"./assets/bot.png\" width=\"10%\"></p>\n<h1 align=\"center\">Customer Care Bot</h1>\n<p align=\"center\">Customer care bot for ecomm company which can solve faq and chitchat with users, can contact directly to team.</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/github/pipenv/locked/python-version/horizon733/customer-care-chatbot\">\n  <img src=\"https://img.shields.io/github/pipenv/locked/dependency-version/horizon733/customer-care-chatbot/rasa?color=blueviolet&label=Rasa\">\n</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/github/repo-size/horizon733/customer-care-chatbot\">\n</p>\n\n## \ud83d\udee0 Features\n- [x] Basic E-commerce FAQ\n- [x] Basic chitchats\n- [x] Out of Scope\n- [x] Contact us form\n- [x] Send Emails\n\n## \u26a1 Quick Setup\n- Initialize a virtual environment via:\n- Conda:\n```bash\nconda create --name rasaenv python=3.7\n```\n- virtualenv\n```bash\nvirtualenv -p python3.7 rasaenv\n```\n- use pipenv\n```\ncd /customer-care-chatbot\npipenv install\n```\n\n## \ud83e\uddea Testing\n- Train bot\n```\nrasa train\n```\n- Test bot on shell\n```\nrasa shell\n```\n- start `rasa` server\n```bash\nrasa run --enable-api --cors \"*\" --debug[Optional] -p {PORT}[optional]\n```\n- start `actions` server\n```\nrasa run actions -p {PORT}[Optional]\n```\n\n## Tutorial links:\n- [Build customer care chatbot from scratch](https://youtu.be/u6xOgR3jEMU)\n- [Send email from Rasa chatbot](https://youtu.be/UcbNmZA65pw)\n"}, {"repo": "dserradji/reactive-customer-service", "language": "Java", "readme_contents": "# Reactive Restful service with Spring 5 and Spring Boot 2\n[![Build Status](https://travis-ci.org/dserradji/reactive-customer-service.svg?branch=master)](https://travis-ci.org/dserradji/reactive-customer-service)\n\nThis project is about building a small Reactive RESTful service with Spring 5 And Spring Boot 2, the DB used is MongoDB and security is managed with Spring Security OAuth2 and SSL.\n\nMore details can be found here: https://dserradji.wordpress.com/\n\nThe \"non-reactive\" version is also available: [customer-service](https://github.com/dserradji/customer-service)\n\n## Docker\n\nAll docker support files are located in the *docker* directory\n\n- *Dockerfile*: this file describes the docker image of the service\n- *docker-compose.yml*: this file manages 2 containers, a container based on the image created with *Dockerfile* file and a container based on the official MongoDB image.\n- *docker-compose up --build -d*: build, start and link the 2 containers, this command is called from *start.sh*\n- *./start.sh*: builds a jar file without the Embedded MondoDB dependency and calls *docker-compose up --build -d*\n- *curl -f -s http://localhost:8081/application/health | jq '.status'*: can be used to check if the service is up or down\n- *docker-compose down*: stop the containers and remove them\n- *./stop.sh*: is identical to *docker-compose down*\n\n**_Important_**\n- This configuration has been tested with: Ubuntu Linux *16.04*, Docker CE *17.06.1-ce* and docker-compose *1.15.0*\n- If *jq* is not installed on your system you can simply remove *| jq '.status'* from the *curl* command\n- If you receive an error message from *docker-compose* about an unsupported version of *Dockerfile* please update your *docker-compose* as described [here](https://github.com/docker/compose/releases)\n"}, {"repo": "Ashniu123/nestjs-customer-order-eventsourcing-cqrs", "language": "TypeScript", "readme_contents": "<p align=\"center\">\n  <a href=\"http://nestjs.com/\" target=\"blank\"><img src=\"https://nestjs.com/img/logo_text.svg\" width=\"320\" alt=\"Nest Logo\" /></a>\n</p>\n\n## Description\n\n[Nest](https://github.com/nestjs/nest) framework TypeScript example for event-sourcing. This is a monorepo.\n\nThere are 2 main parts of the application:\n\n- _customer_: where a person is registered with minimum details and a initial balance amount\n- _order_: where a customerId is used to order items for an amount. If the amount is less than balance then it is ACCEPTED otherwise REJECTED\n\nNo authentication, just plain and simple APIs.\n\n## Installation\n\n```bash\n$ npm install\n```\n\n## Running the app\n\n### Prereqs\n\n- Kafka. Configurable through `KAFKA_BROKERS` env variable.\n- MongoDB. Configurable through `CUSTOMERS_VIEW_SVC_MONGO_URI` and `ORDERS_VIEW_SVC_MONGO_URI` env variables.\n\n```bash\n# development\n$ npm run start <service>\n\n# watch mode\n$ npm run start:dev <service>\n\n# production mode\n$ npm run start:prod <service>\n```\n\nThe services are under `app/`, each folder is a separate service.\n\n## Test\n\n```bash\n# unit tests\n$ npm run test\n\n# e2e tests\n$ npm run test:e2e\n\n# test coverage\n$ npm run test:cov\n```\n\n## Support\n\nNest is an MIT-licensed open source project. It can grow thanks to the sponsors and support by the amazing backers. If you'd like to join them, please [read more here](https://docs.nestjs.com/support).\n\n## License\n\nNest is [MIT licensed](LICENSE).\n"}, {"repo": "M-R-K-Development/bc-customer-plugin", "language": "JavaScript", "readme_contents": "bc-customer-plugin\n==================\n\nThis is a plugin for Business Catalyst that allows the management of Customers\n"}, {"repo": "melihbodr/Sentiment_analysis_to_customer_comments", "language": "Jupyter Notebook", "readme_contents": "## You can watch the details of the project on my youtube channel\n\n[<img alt=\"Youtube\" src=\"https://img.shields.io/badge/Youtube%20-%23FF0000.svg?&style=for-the-badge&logo=YouTube&logoColor=white\"/>](https://youtu.be/7KGAqNbDjX4)\n\nI extracted the data of 892 phones and customer comments from the shopping site. I did this by using the BeautifulSoup library. I extracted an average of 40 50 comments for each product. I conducted a sentiment analysis for these comments and I have identified the products that customers are most satisfied with and the least satisfied products\n"}, {"repo": "nd009/creating_customer_segments", "language": "Jupyter Notebook", "readme_contents": "# \u9879\u76ee 3: \u975e\u76d1\u7763\u5b66\u4e60\n## \u521b\u5efa\u7528\u6237\u7ec6\u5206\n\n### \u5b89\u88c5\n\n\u8fd9\u4e2a\u9879\u76ee\u8981\u6c42\u4f7f\u7528 **Python 2.7** \u5e76\u4e14\u9700\u8981\u5b89\u88c5\u4e0b\u9762\u8fd9\u4e9bpython\u5305\uff1a\n\n- [NumPy](http\uff1a//www.numpy.org/)\n- [Pandas](http\uff1a//pandas.pydata.org)\n- [scikit-learn](http\uff1a//scikit-learn.org/stable/)\n\n\u4f60\u540c\u6837\u9700\u8981\u5b89\u88c5\u597d\u76f8\u5e94\u8f6f\u4ef6\u4f7f\u4e4b\u80fd\u591f\u8fd0\u884c[Jupyter Notebook](http://jupyter.org/)\u3002\n\n\u4f18\u8fbe\u5b66\u57ce\u63a8\u8350\u5b66\u751f\u5b89\u88c5 [Anaconda](https\uff1a//www.continuum.io/downloads), \u8fd9\u662f\u4e00\u4e2a\u5df2\u7ecf\u6253\u5305\u597d\u7684python\u53d1\u884c\u7248\uff0c\u5b83\u5305\u542b\u4e86\u6211\u4eec\u8fd9\u4e2a\u9879\u76ee\u9700\u8981\u7684\u6240\u6709\u7684\u5e93\u548c\u8f6f\u4ef6\u3002\n\n### \u4ee3\u7801\n\n\u521d\u59cb\u4ee3\u7801\u5305\u542b\u5728 `customer_segments.ipynb` \u8fd9\u4e2anotebook\u6587\u4ef6\u4e2d\u3002\u8fd9\u91cc\u9762\u6709\u4e00\u4e9b\u4ee3\u7801\u5df2\u7ecf\u5b9e\u73b0\u597d\u6765\u5e2e\u52a9\u4f60\u5f00\u59cb\u9879\u76ee\uff0c\u4f46\u662f\u4e3a\u4e86\u5b8c\u6210\u9879\u76ee\uff0c\u4f60\u8fd8\u9700\u8981\u5b9e\u73b0\u9644\u52a0\u7684\u529f\u80fd\u3002\n\n### \u8fd0\u884c\n\n\u5728\u547d\u4ee4\u884c\u4e2d\uff0c\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e3a `customer_segments.ipynb` \u6587\u4ef6\u5939\u7684\u6700\u9876\u5c42\uff08\u76ee\u5f55\u5305\u542b\u672c README \u6587\u4ef6\uff09\uff0c\u8fd0\u884c\u4e0b\u5217\u547d\u4ee4\uff1a\n\n```jupyter notebook customer_segments.ipynb```\n\n\u200b\u8fd9\u4f1a\u542f\u52a8 Jupyter Notebook \u5e76\u628a\u9879\u76ee\u6587\u4ef6\u6253\u5f00\u5728\u4f60\u7684\u6d4f\u89c8\u5668\u4e2d\u3002\n\n## \u6570\u636e\n\n\u200b\u8fd9\u4e2a\u9879\u76ee\u7684\u6570\u636e\u5305\u542b\u5728 `customers.csv` \u6587\u4ef6\u4e2d\u3002\u4f60\u80fd\u5728[UCI \u673a\u5668\u5b66\u4e60\u4fe1\u606f\u5e93](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers)\u9875\u9762\u4e2d\u627e\u5230\u66f4\u591a\u4fe1\u606f\u3002\n"}, {"repo": "digitalcreations/CustomerIOSharp", "language": "C#", "readme_contents": "# Customer.io API C# implementation\n\nImplementation of [Customer.io](https://www.customer.io)'s write-only API for .NET.\n\n## Installation\n\nUsing Nuget:\n\n    Install-Package CustomerIOSharp\n\n## Usage\n\nWe recommend you implement the `ICustomerFactory` interface yourself. Here is a suggested implementation used in one of my projects:\n\n```cs\n    public class CustomerFactory : ICustomerFactory\n    {\n        public ICustomerDetails GetCustomerDetails()\n        {\n            var user = MembershipHelper.GetCurrentUser();\n            return user == null \n                ? null \n                : user.AsCustomer();\n        }\n\n        public string GetCustomerId()\n        {\n            var user = MembershipHelper.GetCurrentUser();\n            return user == null\n                ? null\n                : user.UserId.ToString();\n        }\n    }\n```\n\nThen instantiate `CustomerIo` and call `IdentifyAsync()`:\n\n```cs\nvar customerIo = new CustomerIo(\n    \"siteid\", \n    \"apikey\", \n    new CustomerFactory());\nawait customerIo.IdentifyAsync();\n```\n\nYou should most likely only call `IdentifyAsync()` whenever the user logs in or is changed.\n\nIf you do not want to implement `ICustomerFactory`, you can simply provide the required details to each call:\n\n```cs\nawait customerIo.IdentifyAsync(User.AsCustomer());\n```\n\nWhatever you do, you will have to provide customer data implementing `ICustomerDetails`. It has only two required properties, but any properties you supply in your implementation will be forwarded to customer.io:\n\n```cs\npublic class Customer : ICustomerDetails \n{\n    // these two fields are required:\n    public string Id { get; set; }\n    public string Email { get; set; }\n    // these are my custom fields:\n    public string FirstName { get; set; }\n    public string LastName { get; set; }\n}\n```\n\n### Custom events\n\nTrack a custom event by calling `TrackEvent()`. It takes an event name as the first parameter, the second parameter is any serializable object. Note that it automatically posts using the correct customer (using `ICustomerFactory`), but you can also supply your own.\n\n```cs\nawait customerIo.TrackEventAsync(\"signup\", new {\n\tUserGroup = \"trial\",\n\tReferrer = \"email campaign\"\n});\n\n// this event has no data and uses a different customer\nawait customerIo.TrackEventAsync(\"signup\", customerId: \"foo\");\n```\n\nNote that these two variables will be camelcased before being sent to customer.io, so `userGroup` and `referrer` will be available in your transactional campaigns.\n\n## License\n\nDual licensed under the MIT and the GPL license.\n\nYou don\u2019t have to do anything special to choose one license or the other and you don\u2019t have to notify anyone which license you are using.\n\n### MIT license\n\nThe MIT License (MIT)\n\nCopyright (c) 2013-2017 [Digital Creations AS](https://www.digitalcreations.no).\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n### GPL license\n\nCopyright (c) 2013-2017 [Digital Creations AS](https://www.digitalcreations.no).\n\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with this program.  If not, see <https://www.gnu.org/licenses/>.\n"}, {"repo": "fnneves/customer_retention", "language": "Jupyter Notebook", "readme_contents": "# From the medium article about Customer Retention\n\nThe csv file contains the dataset to be used with the Python snippets provided in the article.\n\nThe Jupyter Notebook is the whole code compiled in one single notebook.\n"}, {"repo": "Pranov1984/Prediction-of-customer-propensity-to-churn", "language": "R", "readme_contents": "# Prediction-of-customer-propensity-to-churn\nThe aim of this project is to build a predictive model that will help a telecom company  in devising targeted strategies for retention of customers.\n\nA detailed presentation of the project outcome and Executive summary is provided in my slideshare account. Link to the presentation is\nhttps://www.slideshare.net/pranovmishra/prediction-of-customer-propensity-to-churn-telecom-industry\n"}, {"repo": "magekey/module-customer-restriction", "language": "PHP", "readme_contents": "# Magento 2 Customer Restriction\n\n## Features:\n\nNavigate to **Stores** > **Configuration** > **Customers** section > **Customer Restriction**\n\n- Customer Registration\n  * restriction by email patterns\n  * set error message\n- Customer Login\n  * restriction by email patterns\n  * set error message\n\n## Installing the Extension\n\n    composer require magekey/module-customer-restriction\n\n## Deployment\n\n    php bin/magento maintenance:enable                  #Enable maintenance mode\n    php bin/magento setup:upgrade                       #Updates the Magento software\n    php bin/magento setup:di:compile                    #Compile dependencies\n    php bin/magento setup:static-content:deploy         #Deploys static view files\n    php bin/magento cache:flush                         #Flush cache\n    php bin/magento maintenance:disable                 #Disable maintenance mode\n\n## Versions tested\n> 2.1.12\n\n> 2.2.3\n\n> 2.3.1\n"}, {"repo": "typesafehub/customers", "language": "Scala", "readme_contents": "This is your new Play application\n=================================\n\nThis file will be packaged with your application, when using `activator dist`.\n"}, {"repo": "aws-samples/build-a-360-degree-customer-view-with-aws", "language": "Python", "readme_contents": "## Build a 360-degree customer view in AWS using a powerful set of analytics tools\n\nThis git demonstrated how to bring data from different data systems as a set of customer dimensions and build a 360-degree customer view as a baseline for all customer analytics initiatives.\n\n## Dimensions for a 360-degree customer view\n\nIn this git we will explore a hypothetic financial services company, as there are common dimensions for this industry and some dimensions that are also valid for any service industry, like marketing and communications, customer history or demographic dimension.\n\n## Solution details\n\nWe created three buckets, one for each data purpose: raw data S3 bucket, stage data S3 bucket and analytics data S3 bucket. You can find in our git the deployment guide for the solution.\nAll the data besides Google Analytics sample from Kaggle were synthetic created using lambda functions using random range values. You can use Amazon AppFlow to extract your own data from Google Analytics data as described in this other blog post.\nFor the bank transactions we created a relational database Amazon RDS PostgreSQL.\nTo simulate the API we created Lambda function that responds as a CRM API, you can also use Amazon AppFlow to extract data from your Salesforce environment.\n\nFor the mainframe simulation the lambdas generate flat files on Amazon S3.\n\nWe then used Amazon CloudWatch events schedules, to trigger the Lambda functions.\n\n\n\n\n\n## To deploy the example in your account go to [Deployment](deployment/README.md)\n\n\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n"}, {"repo": "twilio-professional-services/example-customer-end-chat", "language": "JavaScript", "readme_contents": "# Example: Customer End Webchat\n\n\nThis example adds _End Chat_ capability into the customer-facing [Twilio Flex](http://twilio.com/flex) [Webchat UI](https://www.twilio.com/docs/flex/installing-and-using-flex-webchat). It includes code for [Twilio Functions](https://www.twilio.com/docs/runtime/functions) (found in the `serverless` directory) as well as frontend UI code built atop Twilio's [Flex Webchat UI Sample](https://github.com/twilio/flex-webchat-ui-sample) (found in the `flex-webchat-ui` directory).\n\n![customer-end-chat example](https://github.com/twilio-professional-services/example-customer-end-chat/blob/media/customer-end-chat.gif)\n\n_clicking the X closes the window, which ends the chat_\n\n## How it Works\nThe goal here is to create a React Component in flex-webchat-ui that is capable of ending its own chat channel. In order to accomplish this, that component is going to hit a Twilio Function, `endChat`, with the Chat Channel SID.\n\nHowever: The `endChat` Function must _also_ complete the Flex/TaskRouter Task, so it will need its SID. Since there's no easy way to look that up, we start off by populating the taskSid into our Chat Channel's attributes via Studio and the `populateChatChannelWithTaskSid` Twilio Function. Here's how that looks in order of time:\n\n1. Customer types a message, which kicks off the Studio flow\n2. Studio enqueues a new Task into TaskRouter\n3. Once the Task is created, Studio runs the `populateChatChannelWithTaskSid` Function\n4. `populateChatChannelWithTaskSid` adds the taskSid into the chat channel's attributes\n5. When the customer closes their chat window, the new React Component calls the `endChat` Twilio Function\n6. `endChat` sets the Chat Channel to \"INACTIVE\" and completes the Task\n\n\n## Setup\n\n### Prerequisites\nBefore beginning with this Flex plugin, you'll want to make sure that:\n- You have a working [Twilio Flex](https://www.twilio.com/flex) account\n- You have [Node.js](https://nodejs.org) as well as [`npm`](https://npmjs.com) installed\n  - `npm` generally gets installed along with Node.js, but make sure you have it anyway\n- You have the latest [Twilio CLI](https://www.twilio.com/docs/twilio-cli/quickstart) installed\n- Your Twilio CLI is running the latest [Serverless Plugin](https://github.com/twilio-labs/plugin-serverless)\n\n### Configuration\nOver the course of the configuration process, you'll need several values from your Twilio account. The first five can be found right now in the Twilio Console and Flex Admin pages, but the last one will require you to deploy your Twilio Functions to find (Don't worry, we'll cover that!)\n\n- Account SID\n  - Found on the [Twilio Console Homepage](https://www.twilio.com/console)\n  - String starting with \"AC...\"\n- Auth Token\n  - Found on the [Twilio Console Homepage](https://www.twilio.com/console)\n  - Secure string of 32 characters that we'll call \"blah...\" for the sake of communication\n- Twilio Workspace SID\n  - Found in your [TaskRouter Dashboard](https://www.twilio.com/console/taskrouter/dashboard)\n  - String starting with \"WS...\"\n- Flex Chat Service Sid\n  - Found in your [Programmable Chat Dashboard](https://www.twilio.com/console/chat/dashboard)\n  - String starting with \"IS...\"\n- Flex Flow Sid\n  - Found on the Flex Admin [Developer Setup page](https://flex.twilio.com/admin/developers/)\n  - String starting with \"FO...\"\n- Serverless Runtime Domain\n  - We'll grab this after we've deployed our Twilio Functions\n  - A web domain that looks something like \"foobar-xxx-dev.twilio.io\"\n\nWe'll be entering these values into two files, neither of which exist yet:\n- serverless/.env\n- flex-webchat-ui/public/assets/webchat-appConfig.js\n\n\n#### serverless/.env\nTo kick things off, rename the example serverless environment file to remove `.example`, then open it in your editor of choice:\n\n```bash\nmv serverless/.env.example serverless/.env\n\nvim serverless/.env\n```\n\nYou'll notice that this file has temporary string variables for your Account Sid, Auth Token, Twilio Workspace SID, and Flex Chat Service SID. Replace these strings with your actual value.\n\n```\n# Before\nACCOUNT_SID=accountSid\nAUTH_TOKEN=authToken\nTWILIO_WORKSPACE_SID=workspaceSid\n\n# After\nACCOUNT_SID=AC...\nAUTH_TOKEN=blah...\nTWILIO_WORKSPACE_SID=WS...\nFLEX_CHAT_SERVICE_SID=IS...\n```\n\n#### Deploying Functions\n\nBefore we can configure the next file, we'll need to deploy our Twilio Functions and grab the Runtime Domain. To do so, we'll be using the [Twilio CLI](https://www.twilio.com/docs/twilio-cli/quickstart) and the [Serverless Plugin](https://github.com/twilio-labs/plugin-serverless) that you installed as a prerequisiste.\n\nFirst off, make sure that you have authenticated according to the [Twilio CLI documentation](https://www.twilio.com/docs/twilio-cli/quickstart#login-to-your-twilio-account).\n\nThen cd into the Functions directory and deploy them:\n\n```bash\ncd serverless\ntwilio serverless:deploy\n```\n\nOnce everything gets deployed, your response should look something like this:\n\n```bash\nDeployment Details\nDomain: foobar-xxx-dev.twilio.io\nService:\n   plugin-supervisor-capacity-functions (ZS...)\nEnvironment:\n   dev (ZE...)\nBuild SID:\n   ZB...\nView Live Logs:\n   Open the Twilio Console\nFunctions:\n   https://foobar-xxx-dev.twilio.io/endChat\n   https://foobar-xxx-dev.twilio.io/populateChatChannelWithTaskSid\nAssets:\n```\n\nThe value we're looking for comes after `Domain:` \u2013\u00a0that's your Runtime Domain.\n\n#### flex-webchat-ui/public/assets/webchat-appConfig.js\n\nNow we'll populate the UI configuration variables. Start by renaming the example app configuration file to remove `.example`, then open it in your editor of choice\n\n```bash\nmv flex-webchat-ui/public/assets/webchat-appConfig.example.js flex-webchat-ui/public/assets/webchat-appConfig.js\n\nvim flex-webchat-ui/public/assets/webchat-appConfig.js\n```\n\nJust like before, this new file contains temporary strings that you simply have to replace with the actual values:\n\n```javascript\nvar appConfig = {\n    // change to your AccountSid\n    accountSid: \"AC...\",\n    // change to your Flex Flow SID\n    flexFlowSid: \"FO...\",\n    // change to your runtimeDomain\n    runtimeDomain: \"http...\",\n    colorTheme: {\n        overrides: brandedColors\n    }\n}\n```\n\nAnd now the example is fully configured! Now we have one more step: Setting up Studio to use the `populateChatChannelWithTaskSid` Function we deployed earlier.\n\n### Studio Customization\nThis is the crucial step: After the chat has been enqueued into TaskRouter/Flex as a new Task, we have to populate our Chat Channel Attributes with the Task SID. To do this:\n\n1. Open [Studio in your Twilio Console](https://www.twilio.com/console/studio/flows/)\n2. Edit the default Webchat Flow that was automatically created when you installed Flex\n3. Drag a new \"Run Function\" widget onto your flow, below the \"SendMessageToFlex\" widget\n4. Drag a line from the \"SendMessageToFlex\" widget's \"Task Created\" node to the input of your new Run Function widget\n5. Configure your Run Function widget:\n   - Name it something recognizable like \"populateChatChannel\"\n   - Select the Functions service, by default \"customer-end-chat-functions\"\n   - Select your dev environment\n   - Select the `populateChatChannelWithTaskSid` Function\n   - Add two Function Parameters:\n     - key: `taskSid`\n       - value: `{{widgets.SendMessageToAgent.sid}}`\n     - key: `channelSid`\n       - value: `{{trigger.message.ChannelSid}}`\n6. Save the Run Function widget\n7. Publish your changes\n\nThe end result should look something like this:\n![studio customization](https://github.com/twilio-professional-services/example-customer-end-chat/blob/media/studio-flow.png)\n\nWith this studio customization in-place, the entire example should be set up! You can now run it locally to test and customize it.\n\n## Local Development/Deployment\n\nIn order to develop locally, you can use the Webpack Dev Server by running:\n\n```bash\ncd flex-webchat-ui\nnpm install\nnpm start\n```\n\nThis will automatically start up the Webpack Dev Server and open the browser for you. Your app will run on `http://localhost:8081`. If you want to change that you can do this by setting the `PORT` environment variable:\n\n```bash\nPORT=3000 npm start\n```\n\nWhen you make changes to your code, the browser window will be automatically refreshed.\n\n## Disclaimer\nThis software is to be considered \"sample code\", a Type B Deliverable, and is delivered \"as-is\" to the user. Twilio bears no responsibility to support the use or implementation of this software.\n"}, {"repo": "cloudteams/CustomerPlatform", "language": "JavaScript", "readme_contents": "# CustomerPlatform\nThe Customer Platform of CloudTeams\n\n## About the Customer Platform\nThe CloudTeams Customer Platform is the public platform, where customers can join CloudTeams, explore new software projects, contribute and claim rewards\n\n## Installation\nThe CloudTeams Customer Platform has the following dependencies:\n- Python 2.7\n- Django 1.8 and other python packages mentioned in `requirements.txt`\n- PostgreSQL >= 9.5\n- CloudTeams Developer Platform\n\nIf you want to install the CloudTeams platform or use and/or extend parts of the Customer Platform, please contact the software authors:\n\nDimitris Papaspyros, Software Developer & PhD Candidate at National Technical University of Athens (dpap@epu.ntua.gr)\n\nAggelos Arvanitakis, Software Developer & PhD Candidate at National Technical University of Athens (agg.arvanitakis@epu.ntua.gr)\n"}, {"repo": "eventuate-examples/eventuate-micronaut-examples-customers-and-orders", "language": "Java", "readme_contents": "# eventuate-micronaut-examples-customers-and-orders\nMicronaut version of Eventuate event sourcing-based Customers and Orders\n"}, {"repo": "CasiaFan/customer_lifetime_value_prediction", "language": "Python", "readme_contents": ""}, {"repo": "linwt/Intelligent-Customer-Service", "language": "Python", "readme_contents": "# \u6f14\u793a\u89c6\u9891\n\u94fe\u63a5\uff1ahttps://pan.baidu.com/s/1SfywE5AoKXF3e9IyjeECvg   \n\u63d0\u53d6\u7801\uff1ajkor \n# \u6280\u672f\u8def\u7ebf\u56fe\n![](https://github.com/linwt/Intelligent-Customer-Service/blob/master/data/pic/framework.jpg)\n# \u6587\u4ef6\u8bf4\u660e\n* data\uff1a\u5305\u62ec\u722c\u866b\u6570\u636e\u3001\u6269\u5145\u6570\u636e\u3001\u5b98\u65b9\u6570\u636e\n* security\uff1a\u722c\u53d6\u767e\u5ea6\u3001\u767e\u5ea6\u77e5\u9053\u3001\u641c\u72d7\u6570\u636e\n* wiki\uff1a\u83b7\u53d6\u7ef4\u57fa\u767e\u79d1\u6570\u636e\u8fdb\u884c\u5206\u8bcd\u548c\u5206\u5b57\u5904\u7406\uff0c\u5e76\u8bad\u7ec3\u8bcd\u5411\u91cf\u548c\u5b57\u5411\u91cf\u6a21\u578b\n* process\uff1a\u5bf9\u722c\u866b\u6570\u636e\u548c\u5b98\u65b9\u6570\u636e\u8fdb\u884c\u5904\u7406\n* model\uff1a\u5355\u4e2a\u5f3a\u6a21\u578b\uff0c\u5fae\u8c03\u5f97\u5230\u591a\u4e2a\u5f31\u6a21\u578b\uff0c\u6295\u7968\u65b9\u5f0f\u878d\u5408\n# \u83b7\u53d6\u7ef4\u57fa\u767e\u79d1\u6570\u636e\n\u4e00\u3001\u7ef4\u57fa\u767e\u79d1\u6570\u636e\u4e0b\u8f7d\u5730\u5740  \n&emsp; https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2   \n\u4e8c\u3001\u5f00\u6e90\u89e3\u538b\u9879\u76ee  \n&emsp; https://github.com/attardi/wikiextractor   \n&emsp; 1\u3001\u76f4\u63a5\u590d\u5236WikiExtractor.py\u6587\u4ef6\u5373\u53ef  \n&emsp; 2\u3001\u89e3\u538b\u6587\u4ef6E:\\wikiextractor>python WikiExtractor.py -cb 1500M -o extracted E:\\zhwiki-latest-pages-articles.xml.bz2  \n&emsp; 3\u3001\u5f97\u5230E:\\wikiextractor\\extracted\\AA\\wiki_00.bz2\uff0c\u89e3\u538bwiki_00.bz2\u5f97\u5230wiki_00\uff0c\u91cd\u547d\u540d\u4e3awiki.txt  \n\u4e09\u3001\u4e0b\u8f7dopencc  \n&emsp; 1\u3001\u4e0b\u8f7dopencc windows\u7248  \n&emsp; 2\u3001\u5c06bin\u76ee\u5f55\u8def\u5f84\u6dfb\u52a0\u5230\u73af\u5883\u53d8\u91cf  \n\u56db\u3001\u7b80\u7e41\u4f53\u8f6c\u6362  \n&emsp; E:\\wiki\\extracted\\AA> opencc -i wiki.txt -o wiki_jian.txt -c E:\\wiki\\opencc-1.0.4-win32\\opencc-1.0.4\\share\\opencc\\t2s.json  \n\u4e94\u3001\u5206\u8bcd\u3001\u5206\u5b57  \n&emsp; \u5c06wiki_jian.txt\u6309\u7167\u5206\u8bcd\u548c\u5206\u5b57\u4e24\u79cd\u65b9\u6cd5\u8fdb\u884c\u5207\u5206\uff0c\u5e76\u4fdd\u5b58\u5230txt\u6587\u4ef6\u4e2d\n# \u6a21\u578b\u6307\u6807\n\u5355\u6a21\u578b|\u6b63\u786e\u7387|\u53ec\u56de\u7387|F1\u503c\n--|--|--|--|\n\u6a21\u578b1|0.862|0.767|0.812\n\u6a21\u578b2|0.859|0.758|0.805\n\u6a21\u578b3|0.964|0.370|0.535\n\u6a21\u578b4|0.931|0.570|0.707\n\u6a21\u578b5|0.924|0.611|0.735\n\n\u878d\u5408\u6548\u679c|\u6b63\u786e\u7387|\u53ec\u56de\u7387|F1\u503c\n--|--|--|--|\ntop1(sim>0.8)|0.895|0.812|0.851\ntop5(sim>0.6)|0.984|0.962|0.973\n"}, {"repo": "wuzixuan1104/customerService", "language": "PHP", "readme_contents": "# \u5ba2\u670d\u4fe1\u7bb1\n## \u7c21\u4ecb\n\u5927\u591a\u6578\u96fb\u5b50\u5546\u52d9\u7db2\u7ad9\u4e2d\u90fd\u8a2d\u6709\u5ba2\u670d\u4fe1\u7bb1\uff0c\u4f46\u662f\u5c0d\u65bc\u6211\u9019\u7a2e\u5f88\u61f6\u60f0\u53c8\u8a0e\u53ad\u5ba2\u8a34\u7684\u4eba\uff0c\u82e5\u7db2\u7ad9\u4f7f\u7528\u8005\u9ad4\u9a57\u4e0d\u4f73\u9084\u8981\u5148\u4e86\u89e3\u8907\u96dc\u7684\u5ba2\u8a34\u6d41\u7a0b\uff0c\u6211\u5e0c\u671b\u900f\u904e\u5927\u5bb6\u6700\u5e38\u6e9d\u901a\u7684\u7ba1\u9053\"line\"\u4f86\u9032\u884c\u6574\u500b\u5ba2\u8a34\u7684\u6d41\u7a0b\uff0c\u5373\u80fd\u65b9\u4fbf\u7c21\u55ae\u7684\u9054\u5230\u76ee\u7684\u3002\n\n\u672c\u5c08\u6848\u9664\u4e86\u4f7f\u7528lineBot, \u9084\u7d50\u5408\u4e86\u76ee\u524d\u76db\u884c\u7684Trello\u7ba1\u7406\u5c08\u6848\u5de5\u5177\u7576\u4f5c\u5ee0\u5546\u7684\u5f8c\u53f0\u3002\n\n\u76ee\u524d\u554f\u984c\u7a2e\u985e\u7c21\u55ae\u5206\u70ba\uff1a\u6aa2\u8209\u914d\u9001\u54c1\u8cea\u3001\u9000\u8ca8\u554f\u984c\u3001\u8ca8\u5230\u7f3a\u4ef6\u53cd\u61c9\u3001\u4ed8\u6b3e\u554f\u984c\u3001\u63db\u8ca8\u554f\u984c\u3001\u7dad\u4fee\u4fdd\u56fa\u554f\u984c\u3001\u767c\u7968\u554f\u984c\u3001\u6211\u8981\u7533\u8a34\u3002\n\u4e0d\u540c\u7a2e\u985e\u7684\u554f\u984c\u53ef\u4ee5\u5206\u5225\u6307\u6d3e\u7d66\u4e0d\u540c\u7fa4\u7684\u5ba2\u670d\u5c08\u54e1\u56de\u7b54\uff0c\u7576\u6709\u4eba\u63d0\u51fa\u8a72\u5206\u985e\u7684\u554f\u984c\uff0c\u4fbf\u6703\u767c\u9001\u901a\u77e5\u7d66\u8a72\u5c08\u54e1\u56de\u7b54\uff0c\u800c\u6bcf\u500b\u554f\u984c\u90fd\u6703\u6709\u6642\u9593\u63d0\u9192\uff0c\u554f\u984c\u6703\u4f9d\u7167\u6642\u9593\u6709\u4e0d\u540c\u7684\u6a19\u7c64\u984f\u8272\u8b66\u793a\u3002\n\n## \u6280\u8853\n+ Linebot Api: https://developers.line.me/en/docs/messaging-api/overview/\n+ Trello Api: https://developers.trello.com/v1.0/reference#introduction \n\n## \u6a23\u677f\n+ \u81ea\u884c\u8a2d\u8a08\uff0c\u4f7f\u7528Line Flex template : https://developers.line.me/en/docs/messaging-api/flex-message-elements/\n\n## trello\u5ba2\u670d\u5f8c\u53f0\n<img src='assets/img/trello1.png' width='800'>\n\n## Client\u7aef\u958b\u59cb\u64cd\u4f5c\u6a5f\u5668\u4eba\u645f\uff01\n### Step0. \u6253\u958b\u4e0b\u65b9 \"\u66f4\u591a\" \u9078\u55ae\n### Step1. \u9ede\u9078 \"\u4f7f\u7528\u8aaa\u660e\"\uff0c\u8b93\u60a8\u66f4\u65b9\u4fbf\u64cd\u4f5c\n+ \u4f7f\u7528\u8aaa\u660e\u4e5f\u6703\u5728\u4e00\u52a0\u5165\u6a5f\u5668\u4eba\u5f8c\u8df3\u51fa\n### Step2. \u9ede\u9078 \"\u6211\u8981\u554f\u554f\u984c\", \u958b\u59cb\u56de\u8986\u4e26\u9001\u51fa\u8a0a\u606f\u81f3Trello\u5f8c\u53f0\n+ \u63a5\u8457\u9078\u64c7\u554f\u984c\u5206\u985e\u5f8c\u958b\u59cb\u56de\u8986\n+ \u5f8c\u53f0\u6536\u5230\u6a5f\u5668\u4eba\u50b3\u4f86\u7684\u8a0a\u606f\uff0c\u5ba2\u670d\u4eba\u54e1\u4fbf\u6703\u958b\u59cb\u8655\u7406\u4e26\u56de\u8986\uff0c\u82e5\u6b64\u554f\u984c\u5c1a\u672a\u7d50\u675f\u4ecd\u7136\u53ef\u4ee5\u554f\u4e0b\u4e00\u500b\u554f\u984c(\u554f\u984c\u5206\u985e\u4e0d\u62d8)\n\n+ <img src='assets/img/linebot/1.PNG' width='300'>\n\n### Step3. \u7576\u60a8\u4e00\u6b21\u6709\u591a\u500b\u554f\u984c\u6642\uff0c\u60a8\u53ef\u4ee5\u9ede\u9078 \"\u9032\u884c\u4e2d\u7684\u554f\u984c\"\uff0c\u4e26\u5c0d\u5c1a\u672a\u7d50\u675f\u7684\u554f\u984c\u9032\u884c\u5207\u63db\n\n> \u5217\u8868\u6703\u986f\u793a`\u554f\u984c\u9032\u5ea6\u72c0\u614b`\uff0c\u4f9d\u7167`\u5206\u985e`\u505a\u6392\u5e8f\n\n1. \u9032\u884c\u4e2d\u7684\u554f\u984c\u5217\u8868\n\n+ <img src='assets/img/linebot/2.PNG' width='300'>\n\n3. \u9ede\u9078\"\u5207\u63db\", \u5207\u63db\u5217\u8868\u6709\u4ee5\u4e0b\u5169\u500b\u6309\u9215\n\n+ <img src='assets/img/linebot/3.PNG' width='300'>\n\n+ \u9ede\u9078 \"\u6aa2\u8996\u5148\u524d\u7684\u6b77\u53f2\u7d00\u9304\"\n\n+ <img src='assets/img/linebot/4.PNG' width='300'>\n\n+ \u8f38\u5165\u554f\u984c\u5f8c\uff0c\u518d\u9ede\u9078 \"\u56de\u8986\u8a0a\u606f\u5f8c\u6309\u6b64\u9001\u51fa\"\n\n+ \u9001\u51fa\u6210\u529f\n\n+ <img src='assets/img/linebot/5.PNG' width='300'>\n\n+ \u5ba2\u670d\u56de\u8986\u5f8c\u50b3\u9001\u81f3\u6a5f\u5668\u4eba\uff0c\u53ef\u4ee5\u91dd\u5c0d\u56de\u8986\u7e7c\u7e8c\u4e00\u4f86\u4e00\u5f80\u7684\u56de\u7b54\u96d9\u65b9\u554f\u984c\n\n### Step4. \u82e5\u6709\u591a\u500b\u554f\u984c\u9032\u884c\u4e2d\uff0c\u5ba2\u670d\u56de\u8986\u554f\u984c\u5f8c\u50b3\u9001\u7d66\u6a5f\u5668\u4eba\u6703\u932f\u4e82\n1. \u82e5\u7576\u524d\u7684\u554f\u984c\u70ba\"Q1\"\uff0c\u800c\u5ba2\u670d\u90a3\u908a\u56de\u8986\u4e86\u60a8\u5148\u524d\u554f\u7684\u554f\u984c\"Q2\"\uff0c\u6703\u8df3\u51fa\u63d2\u64ad\uff0c\u8a62\u554f\u662f\u5426\u5207\u63db\u554f\u984c\u4e26\u67e5\u770b\u554f\u984c\u56de\u8986\n\n+ <img src='assets/img/linebot/7.PNG' width='300'>\n\n### Step5. \u554f\u984c\u5b8c\u6210\u5f8c\u8acb\u586b\u5beb\u5ba2\u670d\u8a55\u5206\u8868\n+ \u7576\u67d0\u500b\u554f\u984c\u5ba2\u670d\u4eba\u54e1\u65bc\u5f8c\u53f0\u6a19\u6ce8\u5b8c\u6210\uff0c\u5247\u6703\u50b3\u9001\u7d66\u6a5f\u5668\u4eba\u8868\u793a\u8a72\u554f\u984c\u5df2\u5b8c\u6210\uff0c\u4e26\u9001\u51fa\u554f\u984c\u8a55\u5206\u8868\n+ \u6b64\u8a55\u5206\u8868\u6703\u91dd\u5c0d\u6b64\u554f\u984c\u8ddf\u56de\u7b54\u7684\u5ba2\u670d\u4eba\u54e1\u505a\u8a55\u5206\n\n+ <img src='assets/img/linebot/8.PNG' width='300'>\n\n### Step6. \u9ede\u9078 \"\u610f\u898b\u56de\u994b\"\n1. \u56de\u8986\u5f8c\uff0c\u9ede\u9078 \"\u9001\u51fa\u9215\"\n\n+ <img src='assets/img/linebot/6.PNG' width='300'>\n\n"}, {"repo": "Wanghongchao12138/CustomerCollectionView", "language": "Objective-C", "readme_contents": "# CustomerCollectionView\n \u81ea\u5b9a\u4e49\u7011\u5e03\u6d41\uff08\u53ef\u6dfb\u52a0\u81ea\u5b9a\u4e49header\u548cfooter\uff09\n\n![image](https://github.com/Wanghongchao12138/CustomerCollectionView/blob/master/6pssss.gif)   \n \n /*\n    \u8bbe\u7f6ecell  \u7684\u5bbd\u9ad8\n*/\n - (CGFloat)collectionView:(UICollectionView *)collectionView layout:(WHCWaterfallFlowLayout*)collectionViewLayout heightForWidth:(CGFloat)width atIndexPath:(NSIndexPath*)indexPath;\n\n/*\n    \u8bbe\u7f6ecollection \u7684\u5934\u89c6\u56fe\u7684size\n */\n- (CGSize)collectionView:(UICollectionView *)collectionView layout:(WHCWaterfallFlowLayout*)collectionViewLayout referenceSizeForHeaderInSection:(NSInteger)section;\n\n/*\n    \u8bbe\u7f6ecollection footer \u89c6\u56fe\u7684size\n*/\n - (CGSize)collectionView:(UICollectionView *)collectionView layout:(WHCWaterfallFlowLayout*)collectionViewLayout referenceSizeForFooterInSection:(NSInteger)section;\n \n   -- colCount --      \u00a0 \u8bbe\u7f6eCollectionView \u7684cell \u7684\u5217\u6570\n \u00a0 -- sectionInset -- \u00a0 \u00a0\u8bbe\u7f6ecell \u7684\u95f4\u8ddd\n \n\n\n"}, {"repo": "im3x/weCustomer", "language": "JavaScript", "readme_contents": "# \u5fae\u4fe1\u5c0f\u7a0b\u5e8f\u5ba2\u670d\u6d88\u606fUI\u7ec4\u4ef6\n\n> \u61d2\u4eba\u5fc5\u5907\uff0c\u4e00\u952e\u7ed9\u7a0b\u5e8f\u6dfb\u52a0\u591a\u6837\u5316\u7684\u8054\u7cfb\u5ba2\u670d\u529f\u80fd\uff01\n\n## \u7ec4\u4ef6\u5217\u8868\n| \u7ec4\u4ef6\u540d\u79f0 | \u8bf4\u660e |\n| :---: | :---: |\n| floatAction | \u53f3\u4e0b\u89d2\u56fa\u5b9a\u7684\u8054\u7cfb\u5ba2\u670d\u6309\u94ae\u56fe\u6807 |\n| weReadTip | \u6a21\u4eff\u5fae\u4fe1\u8bfb\u4e66\u5f15\u5bfc\u4e0b\u8f7dapp UI |\n\n## \u5b89\u88c5\n\u590d\u5236 SDK \u4ee3\u7801\u5230\u5c0f\u7a0b\u5e8f\u76ee\u5f55\n\n## \u4f7f\u7528\n\u5728\u9700\u8981\u7684\u9875\u9762\uff08\u6bd4\u5982\u9996\u9875\uff09`json`\u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u6309\u9700\u5f15\u5165sdk\u7684\u7ec4\u4ef6\uff1a\n\n``` json\n{\n  \"usingComponents\": {\n    \"floatAction\": \"/components/weCustomer/floatAction/index\",\n    \"weReadTip\": \"/components/weCustomer/weReadTip/index\n  }\n}\n```\n\n> \u63d0\u793a\uff1a\u8def\u5f84\u8bf7\u6839\u636e\u81ea\u8eab\u60c5\u51b5\u4fee\u6539\n\n\u6700\u540e\uff0c\u5728\u9875\u9762\uff08wxml\uff09\u6587\u4ef6\u4e2d\uff0c\u6dfb\u52a0\u6211\u4eec\u7684\u7ec4\u4ef6\u5373\u53ef\uff1a\n\n``` wxml\n<floatAction />\n```\n\n## `floatAction` \u7ec4\u4ef6\u5c5e\u6027\n\n|\u5c5e\u6027\u540d | \u7c7b\u578b | \u9ed8\u8ba4\u503c | \u8bf4\u660e |\n|:--:|:--:|:--:|:--:|\n| bottom | Number | 100 | \u5e95\u90e8\u8ddd\u79bb |\n| right | Number | 20 | \u53f3\u4fa7\u8ddd\u79bb |\n| icon | String | customer.png | \u6309\u94ae\u56fe\u7247\u8d44\u6e90 |\n\n## `weReadTip` \u7ec4\u4ef6\u5c5e\u6027\n\n|\u5c5e\u6027\u540d | \u7c7b\u578b | \u9ed8\u8ba4\u503c | \u8bf4\u660e |\n|:--:|:--:|:--:|:--:|\n| show | Boolean | false | \u662f\u5426\u663e\u793a\u63d0\u793aUI |\n| closeOnTap | Boolean | true | \u662f\u5426\u70b9\u51fb\u540e\u81ea\u52a8\u9690\u85cfUI |\n| title | String | \u9876\u90e8\u6807\u9898\\n\u9700\u8981\u4e24\u884c | \u9876\u90e8\u6807\u9898 |\n| listIcon | String | list.svg | \u5217\u8868\u5de6\u4fa7\u56fe\u6807\u8d44\u6e90 |\n| features | Array | ['\u529f\u80fd1','\u529f\u80fd2'] | \u529f\u80fd\u4ecb\u7ecd\u6587\u5b57\u5217\u8868 |\n| bottomText | String | \u7acb\u5373\u4e0b\u8f7d\u00b7\u5728\u5ba2\u670d\u4f1a\u8bdd\u4e2d\u56de\u590d1 | \u5e95\u90e8\u6309\u94ae\u63d0\u793a\u6587\u5b57 |\n| themeColor | String | #67C23A | \u6574\u4f53\u5f39\u51fa\u5c42\u4e3b\u9898\u989c\u8272 |\n\n## \u5ba2\u670d\u6d88\u606f\u81ea\u52a8\u56de\u590d\u795e\u5668\n\n\u57fa\u7840\u7248\uff08\u514d\u8d39\uff09\uff1ahttps://s.w7.cc/module-25655.html\n\n## \u9884\u89c8\u6548\u679c\n\n![\u5c0f\u7a0b\u5e8f\u5ba2\u670d\u6d88\u606f](https://i.loli.net/2020/05/29/oNdbrcvgt7xETZG.jpg)"}, {"repo": "thomasnield/customer-wait-queue-simulation", "language": "Kotlin", "readme_contents": "# Poisson Optimizer and Simulator\n\nUsing poisson and normal distributions to simulate staffing headcount decisions and impact on wait times. Blog post/video coming soon.\n\n## Abstract\n\nThis is a stochastic (controlled randomness) simulator that accepts probabilistic parameters (number of staff, average customer arrivals per hour, average minutes to process a customer) and animates the outcome over a specified period.\n\nThe animation was built with JavaFX/TornadoFX. \n\n![](animation.gif)\n\n"}, {"repo": "mesudepolat/CUSTOMER_LIFETIME_VALUE", "language": "Python", "readme_contents": "# CUSTOMER LIFETIME VALUE\n\nCustomer lifetime value for a firm is the net profit or loss to the firm from a customer over the entire life of transactions of that customer with the firm\n\n\n## Project Steps\n\n1. Calculation of the expected number of selling number in a specific time period by using BG/NBD.\n2. Calculation of the expected average profit with Gamma Gamma modeling.\n3. By combining them we get the CLTV.\n\n\n## How to Calculate Customer Lifetime Value?\n\n* CLTV = (Customer_Value / Churn_Rate) x Profit_margin\n* Customer_Value = Average_Order_Value * Purchase_Frequency\n* Average_Order_Value = Total_Revenue / Total_Number_of_Orders\n* Purchase_Frequency =  Total_Number_of_Orders / Total_Number_of_Customers\n* Churn_Rate = 1 - Repeat_Rate\n* Profit_margin\n\n\n## Dataset Information\n\nThis Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011. The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n\nhttps://archive.ics.uci.edu/ml/datasets/Online+Retail+II#\n"}, {"repo": "erenonal/K-means_customer_segmentation", "language": "Jupyter Notebook", "readme_contents": "# K-Means Customer Segmentation (due to cradit card behaviour)\n\n# About Porject\n\nThe number of people who do not use credit cards nowadays is very small. But how efficiently do we use it? To us, this question may sound like a social question. But for banks, this question refers to the risk they take or the opportunities they can seize. In order to evaluate these risks and opportunities, they need to analyze the situation of their customers well. That's why customer segmentation is essential.  \n\nOur aim in this project, which examines the status of customers according to credit card behaviors, is to determine which customer we should approach and how. To put it another way, in this case we will  develop a customer segmentation to define marketing strategy. \n\nOur sequence of actions is as follows: Answering questions such as how customers spend, who gets more cash, who buys more expensive, who buys more in installments etc. According to the answers to these questions we asked, we can take actions such as which customer's limit will be increased or decreased, which customers should we inform about the installment opportunities.\n\n# About Implementation\n\nUsing the `K-Means` algorithm, I made a clustering by examining the spending of customers with their credit cards.\n\nThe explanations of the codes are also written as a comment line in detail, but I will write the operations I have done.\n\nAfter defining the libraries we use, we read our file. I defined the reading patterns for the colab environment and for the local environment.\n\nTo familiarize myself with the dataset, I took a look at the variable names and types.\n\nFirst of all, I examined values such as mean value, standard deviation, min max with the `.describe()` function and saw that there may be outliers here.\n\nAfter I filled in the missing values with their mean, I started to detect the outliers.\n\nAs a next step, I plotted graphs between variables of my choice to observe outliers.\n\nI printed out the visibly obvious outliers and their percentages separately for each column with a for loop.\n\nDeleting it would cause a huge data loss as each column had a large amount of outliers. So I made ranges to deal with outliers\n\nAfter standardizing the data with `StandardScaler`, I moved on to the modeling phase.\n\nI observed the optimal clustering number with the `Elbow method`.\n\nI ran the K-Means model with the appropriate hyperparameters and fitted our model.\n\nI decided 6 clusters. Also I decided `n_init` for initialize cluster number, `max_iter` for how many iterations we will do, `random_state` is random state as all you know :)\n\nI interpreted with `FacetGrid` by observing the relations of clusters with variables. \n\nThe classes I clustered are as follows\n\n* Cluster5 People with average to high credit limit who make all type of purchases\n\n* Cluster4 This group has more people with due payments who take advance cash more often\n\n* Cluster3 Less money spenders with average to high credit limits who purchases mostly in installments\n\n* Cluster2 People with high credit limit who take more cash in advance\n\n* Cluster1 High spenders with high credit limit who make expensive purchases\n\n* Cluster0 People who don't spend much money and who have average to high credit limit\n\nBefore using PCA to transform data to 2 dimensions for visualization, first we took cosine distance before PCA because cosine distance kills the magnitude of vectors and focuses on relationships only.\n\nFinally after PCA, I visualized our data reduced to x and y tags, expressing the clusters with different colors, grouping them according to their labels with the `.groupby` function to observe.\n\n\n# Required Libraries \n\n* `pandas`  (load and manipulate data and for One-Hot Encoding)\n\n* `numpy`  (calculate the mean and standard deviation)\n\n* `seaborn` (helping with some visualization techniques)\n\n* `matplotlib.pyplot` (some graphs)\n\n* `matplotlib.ticker`  (for specifying the axes thick format)\n\n* `sklearn.preprocessing` , StandartScaler (scaling)\n\n* `sklearn.cluster` , KMeans (k-means algorithm library)\n\n* `sklearn.decomposition` , PCA (applying principal component analysis)\n\n* `sklearn.metrics.pairwise` , cosine_similarity (normalised dot product between two vectors (preparing for PCA))\n\n* `io` (reading files for all systems)\n\n\n# About dataset\nThe sample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.\n\n\nFollowing is the Data Dictionary for Credit Card dataset :\n\n`CUSTID :` Identification of Credit Card holder (Categorical)\n\n`BALANCE :` Balance amount left in their account to make purchases (\n\n`BALANCEFREQUENCY :` How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n\n`PURCHASES :` Amount of purchases made from account\n\n`ONEOFFPURCHASES :` Maximum purchase amount done in one-go\n\n`INSTALLMENTSPURCHASES :` Amount of purchase done in installment\n\n`CASHADVANCE :` Cash in advance given by the user\n\n`PURCHASESFREQUENCY :` How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n\n`ONEOFFPURCHASESFREQUENCY :` How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n\n`PURCHASESINSTALLMENTSFREQUENCY :` How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n\n`CASHADVANCEFREQUENCY :` How frequently the cash in advance being paid\n\n`CASHADVANCETRX :` Number of Transactions made with \"Cash in Advanced\"\n\n`PURCHASESTRX :` Numbe of purchase transactions made\n\n`CREDITLIMIT :` Limit of Credit Card for user\n\n`PAYMENTS :` Amount of Payment done by user\n\n`MINIMUM_PAYMENTS :` Minimum amount of payments made by user\n\n`PRCFULLPAYMENT :` Percent of full payment paid by user\n\n`TENURE :` Tenure of credit card service for user\n\nDataset link: https://www.kaggle.com/arjunbhasin2013/ccdata\n\n# Conclusion\n\nDifferent actions can be taken according to the 6 clusters we have separated. For example,(for `Cluster3`) we can encourage users who shop in installments by increasing their special installment options or (for `Cluster2`), customers who withdraw and spend cash can be provided with low commission cash via credit card or (for `Cluster5`) we can increase their loyalty with special offers for people with high limits and all kinds of spending.\n\n\n\n\n\n\n"}, {"repo": "meteorhacks/meteor-customer-io", "language": "JavaScript", "readme_contents": "# Customer.Io integration for Meteor\n\nThis is an isopack!<br>\n(Which means, both client and server has the same API)\n\n## Installation\n\n    // for meteor 0.9\n    meteor add meteorhacks:customer.io\n\n    // for < meteor 0.9\n    mrt add customer.io\n\n## Usage\n\n    // initialization\n\n    var siteId = 'site_id';\n    var token = 'the_token'; // only on the server\n    var cio = CustomerIo.init(siteId, token);\n\n    // identifing\n\n    var userId = 'user_id';\n    var email = 'user@email.com';\n    var properties = {name: \"My Name\"};\n    cio.identify(userId, email, properties, function(err, res, body) {\n      // callback is only available on the server\n    });\n\n    // track\n\n    var userId = 'user_id';\n    var event = 'the_event';\n    var properties = {name: \"My Name\"};\n    cio.track(userId, event, properties, function(err, res, body) {\n      // callback is only available on the server\n    });\n"}, {"repo": "latonaio/data-interface-for-salesforce-customer", "language": "Go", "readme_contents": "# data-interface-for-salesforce-customer\n\n## \u6982\u8981\ndata-interface-for-salesforce-customer \u306f\u3001salesforce\u306e\u9867\u5ba2\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u5bfe\u3059\u308b\u5404\u7a2e\u30a2\u30af\u30b7\u30e7\u30f3\u306b\u5fc5\u8981\u306a\u30c7\u30fc\u30bf\u306e\u6574\u5f62\u3001\u304a\u3088\u3073 salesforce \u304b\u3089\u53d7\u3051\u53d6\u3063\u305f response \u306e MySQL \u3078\u306e\u683c\u7d0d\u3092\u884c\u3046\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u3067\u3059\u3002\n\n## \u52d5\u4f5c\u74b0\u5883\ndata-interface-for-salesforce-customer \u306f\u3001aion-core\u306e\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u4e0a\u3067\u306e\u52d5\u4f5c\u3092\u524d\u63d0\u3068\u3057\u3066\u3044\u307e\u3059\u3002  \n\u4f7f\u7528\u3059\u308b\u969b\u306f\u3001\u4e8b\u524d\u306b\u4e0b\u8a18\u306e\u901a\u308aAION\u306e\u52d5\u4f5c\u74b0\u5883\u3092\u7528\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002   \n\n* OS: Linux OS     \n* CPU: ARM/AMD/Intel     \n* Kubernetes     \n* AION \u306e\u30ea\u30bd\u30fc\u30b9     \n\n## \u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n1. \u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3001docker image\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n```\n$ cd /path/to/data-interface-for-salesforce-customer\n$ make docker-build\n```\n\n2. \u672c\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u306f DB \u306b MySQL \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002MySQL \u306b\u95a2\u3059\u308b\u8a2d\u5b9a\u3092\u3001 `data-interface-for-salesforce-customer.yaml` \u306e\u74b0\u5883\u5909\u6570\u306b\u8a18\u8ff0\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n| env_name | description |\n| --- | --- |\n| MYSQL_HOST | \u30db\u30b9\u30c8\u540d |\n| MYSQL_PORT | \u30dd\u30fc\u30c8\u756a\u53f7 |\n| MYSQL_USER | \u30e6\u30fc\u30b6\u30fc\u540d |\n| MYSQL_PASSWORD | \u30d1\u30b9\u30ef\u30fc\u30c9 |\n| MYSQL_DBNAME | \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u540d |\n| MAX_OPEN_CONNECTION | \u6700\u5927\u30b3\u30cd\u30af\u30b7\u30e7\u30f3\u6570 |\n| MAX_IDLE_CONNECTION | \u30a2\u30a4\u30c9\u30eb\u72b6\u614b\u306e\u6700\u5927\u30b3\u30cd\u30af\u30b7\u30e7\u30f3\u6570 |\n| KANBANADDR: | kanban \u306e\u30a2\u30c9\u30ec\u30b9 |\n| TZ | \u30bf\u30a4\u30e0\u30be\u30fc\u30f3 |\n\n## \u8d77\u52d5\u65b9\u6cd5\n\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3001pod\u3092\u7acb\u3061\u4e0a\u3052\u3066\u304f\u3060\u3055\u3044\u3002\n```\n$ cd /path/to/data-interface-for-salesforce-customer\n$ kubectl apply -f data-interface-for-salesforce-customer.yaml\n```\n\n## kanban \u3068\u306e\u901a\u4fe1\n### kanban \u304b\u3089\u53d7\u4fe1\u3059\u308b\u30c7\u30fc\u30bf\nkanban \u304b\u3089\u53d7\u4fe1\u3059\u308b metadata \u306b\u4e0b\u8a18\u306e\u60c5\u5831\u3092\u542b\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n| key | value |\n| --- | --- |\n| connection_type | \u6587\u5b57\u5217 \"request\" |\n| method | \u4f7f\u7528\u3059\u308b http method |\n| object | Customer |\n| id | \u9867\u5ba2 ID |\n| account_id | \u9867\u5ba2 ID |\n| body | salesforce api \u306b\u9001\u4fe1\u3059\u308b\u6d41\u30af\u30a8\u30b9\u30c8\u306e body |\n\n\u5177\u4f53\u4f8b 1: \n```example\n# metadata (map[string]interface{}) \u306e\u4e2d\u8eab\n\n\"connection_type\": \"request\"\n\"method\": \"get\"\n\"object\": \"Account\"\n\"id\": \"xxxx\"\n```\n\n\u5177\u4f53\u4f8b 2: \n```example\n# metadata (map[string]interface{}) \u306e\u4e2d\u8eab\n\n\"connection_type\": \"request\"\n\"method\": \"get\"\n\"object\": \"Account\"\n\"account_id\": \"xxxx\"\n```\n\n### kanban \u306b\u9001\u4fe1\u3059\u308b\u30c7\u30fc\u30bf\nkanban \u306b\u9001\u4fe1\u3059\u308b metadata \u306f\u4e0b\u8a18\u306e\u60c5\u5831\u3092\u542b\u307f\u307e\u3059\u3002\n\n| key | type | description |\n| --- | --- | --- |\n| method | string | \u4f7f\u7528\u3059\u308b http method |\n| object | string | \u6587\u5b57\u5217 \"Account\" \u3092\u6307\u5b9a |\n| path_param | string | \u9867\u5ba2 ID \u3092\u6307\u5b9a |\n| connection_key | string | \u6587\u5b57\u5217 customer \u3068\u3001\u4f7f\u7528\u3059\u308b http \u30e1\u30bd\u30c3\u30c9\u3092(_)\u3067\u7d50\u5408\u3057\u305f\u6587\u5b57\u5217 (\u4f8b: customer_get)|\n\n\n\u5177\u4f53\u4f8b: \n```example\n# metadata (map[string]interface{}) \u306e\u4e2d\u8eab\n\n\"method\": \"get\"\n\"object\": \"Account\"\n\"path_param\": \"xxxx\"\n\"connection_key\": \"customer_get\"\n```\n\n## kanban(salesforce-api-kube) \u304b\u3089\u53d7\u4fe1\u3059\u308b\u30c7\u30fc\u30bf\nkanban \u304b\u3089\u306e\u53d7\u4fe1\u53ef\u80fd\u30c7\u30fc\u30bf\u306f\u4e0b\u8a18\u306e\u5f62\u5f0f\u3067\u3059\n\n| key | value |\n| --- | --- |\n| key | \u6587\u5b57\u5217 \"Account\" |\n| content | Contract \u306e\u8a73\u7d30\u60c5\u5831\u3092\u542b\u3080 JSON \u914d\u5217 |\n| connection_type | \u6587\u5b57\u5217 \"response\" |\n\n\u5177\u4f53\u4f8b:\n```example\n# metadata (map[string]interface{}) \u306e\u4e2d\u8eab\n\n\"key\": \"Account\"\n\"content\": \"[{xxxxxxxxxxx}]\"\n\"connection_type\": \"response\"\n```"}, {"repo": "apache/fineract-cn-customer", "language": "Java", "readme_contents": "# Apache Fineract CN Customer Management [![Build Status](https://api.travis-ci.com/apache/fineract-cn-customer.svg?branch=develop)](https://travis-ci.com/apache/fineract-cn-customer) [![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/apache/fineract-cn-customer)](https://hub.docker.com/r/apache/fineract-cn-customer/builds)\n\nThis service covers simple functionality around CRM and KYC. [Read more](https://cwiki.apache.org/confluence/display/FINERACT/Fineract+CN+Project+Structure#FineractCNProjectStructure-customer).\n\n## Versioning\nThe version numbers follow the [Semantic Versioning](http://semver.org/) scheme.\n\nIn addition to MAJOR.MINOR.PATCH the following postfixes are used to indicate the development state.\n\n* BUILD-SNAPSHOT - A release currently in development. \n* M - A _milestone_ release include specific sets of functions and are released as soon as the functionality is complete.\n* RC - A _release candidate_ is a version with potential to be a final product, considered _code complete_.\n* RELEASE - _General availability_ indicates that this release is the best available version and is recommended for all usage.\n\nThe versioning layout is {MAJOR}.{MINOR}.{PATCH}-{INDICATOR}[.{PATCH}]. Only milestones and release candidates can  have patch versions. Some examples:\n\n1.2.3.BUILD-SNAPSHOT  \n1.3.5.M.1  \n1.5.7.RC.2  \n2.0.0.RELEASE\n\n## License\nSee [LICENSE](LICENSE) file.\n"}, {"repo": "watson-developer-cloud/customer-engagement-nodejs", "language": "JavaScript", "readme_contents": "<h1 align=\"center\" style=\"border-bottom: none;\">\ud83d\ude80 Customer Engagement Demo</h1>\n<p align=\"center\">\n  <a href=\"http://travis-ci.org/watson-developer-cloud/customer-engagement-nodejs\">\n    <img alt=\"Travis\" src=\"https://travis-ci.org/watson-developer-cloud/customer-engagement-nodejs.svg?branch=master\">\n  </a>\n  <a href=\"#badge\">\n    <img alt=\"semantic-release\" src=\"https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--release-e10079.svg\">\n  </a>\n</p>\n</p>\n\n## Prerequisites\n\n1. Sign up for an [IBM Cloud account](https://console.bluemix.net/registration/).\n1. Download the [IBM Cloud CLI](https://console.bluemix.net/docs/cli/index.html#overview).\n1. Create an instance of the Tone Analyzer service and get your credentials:\n   - Go to the [Tone Analyzer](https://console.bluemix.net/catalog/services/tone-analyzer) page in the IBM Cloud Catalog.\n   - Log in to your IBM Cloud account.\n   - Click **Create**.\n   - Click **Show** to view the service credentials.\n   - Copy the `apikey` value.\n   - Copy the `url` value.\n\n## Configuring the application\n\n1. In the application folder, copy the _.env.example_ file and create a file called _.env_\n\n   ```\n   cp .env.example .env\n   ```\n\n2. Open the _.env_ file and add the service credentials that you obtained in the previous step.\n\n   Example _.env_ file that configures the `apikey` and `url` for a Tone Analyzer service instance hosted in the US East region:\n\n   ```\n   TONE_ANALYZER_IAM_APIKEY=X4rbi8vwZmKpXfowaS3GAsA7vdy17Qh7km5D6EzKLHL2\n   TONE_ANALYZER_URL=https://gateway.watsonplatform.net/tone-analyzer/api\n   ```\n\n## Running locally\n\n1. Install the dependencies\n\n   ```\n   npm install\n   ```\n\n1. Run the application\n\n   ```\n   npm start\n   ```\n\n1. View the application in a browser at `localhost:3000`\n\n## Deploying to IBM Cloud as a Cloud Foundry Application\n\n1. Login to IBM Cloud with the [IBM Cloud CLI](https://console.bluemix.net/docs/cli/index.html#overview)\n\n   ```\n   ibmcloud login\n   ```\n\n1. Target a Cloud Foundry organization and space.\n\n   ```\n   ibmcloud target --cf\n   ```\n\n1. Edit the _manifest.yml_ file. Change the **name** field to something unique.  \n   For example, `- name: my-app-name`.\n1. Deploy the application\n\n   ```\n   ibmcloud app push\n   ```\n\n1. View the application online at the app URL.  \n   For example: https://my-app-name.mybluemix.net\n\n---\n\n### Directory structure\n\n```none\n.\n\u251c\u2500\u2500 app.js                      // express routes\n\u251c\u2500\u2500 config                      // express configuration\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 error-handler.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 express.js\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 security.js\n\u251c\u2500\u2500 manifest.yml\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 public                      // static resources\n\u251c\u2500\u2500 server.js                   // entry point\n\u251c\u2500\u2500 test                        // unit tests\n\u2514\u2500\u2500 views                       // react components\n```\n\n## License\n\nThis sample code is licensed under Apache 2.0.\n\n## Contributing\n\nSee [CONTRIBUTING](.github/CONTRIBUTING.md).\n\n## Open Source @ IBM\n\nFind more open source projects on the [IBM Github Page](http://ibm.github.io/)\n"}, {"repo": "szhua/CustomerServiceAssistant", "language": "Java", "readme_contents": "#\u5fae\u4fe1\u6570\u636e\u7684\u83b7\u53d6\n\n\u4ece\u5fae\u4fe1\u4e2d\u83b7\u53d6\u804a\u5929\u8bb0\u5f55\uff0c\u662f\u4e00\u4e2a\u4e0d\u9519\u7684\u60f3\u6cd5\u3002\u7136\u540e\u5c06\u5fae\u4fe1\u804a\u5929\u8bb0\u5f55\u4e0d\u95f4\u65ad\u7684\u4e0a\u4f20\uff0c\u9700\u8981\u4e00\u4e2a\u5408\u7406\u7684\u903b\u8f91\u53bb\u5904\u7406\u8fd9\u4ef6\u4e8b\u60c5\u3002\u7ecf\u8fc7\u4e00\u4e2a\u591a\u6708\u7684\u6478\u7d22\uff0c\u73b0\u5728\u5df2\u7ecf\u80fd\u591f\u8fbe\u5230\u57fa\u672c\u7684\u9700\u6c42\uff0c\u5931\u8bef\u7387\u5f88\u4f4e\u3002\u4e0d\u5408\u7406\u7684\u5730\u65b9\u8fd8\u6709\u5f88\u591a\uff0c\u4ee5\u540e\u518d\u8fdb\u884c\u4fee\u6539\u3002\u81ea\u4ece\u8fd9\u4e2a\u9879\u76ee\u5f00\u59cb\u5230\u73b0\u5728gitHub\u4e0a\u9762\u5df2\u7ecf\u66f4\u65b0\u4e86\u597d\u591a\u7684\u7248\u672c\uff0c\u4e0d\u65ad\u7684\u8fdb\u884c\u4fee\u6539\u3002\n\n![\u57fa\u672c\u6846\u67b6\u7ed3\u6784](https://github.com/szhua/CustomerServiceAssistant/blob/master/\u89e3\u5bc6\u5fae\u4fe1.png)\n\n\u8fd9\u4e2a\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u89e3\u5bc6\u5fae\u4fe1\u6570\u636e\u5e93\u7684\u6d41\u7a0b\uff0c\u7f51\u4e0a\u7684\u8d44\u6599\u5f88\u591a\uff0c\u4f46\u662fdemo\u5374\u5f88\u5c11\u3002\u6211\u662f\u4f7f\u7528\u7684sqlChiper\u8fdb\u884c\u7684\u89e3\u5bc6\u3002\nhttps://github.com/sqlcipher/android-database-sqlcipher;\n\n\u6d41\u7a0b\u4e2d\u5177\u4f53\u7684\u6b65\u9aa4\uff1a\n\n1.  \u83b7\u5f97\u624b\u673a\u7684IMEI\u53ea\u9700\u83b7\u5f97\u624b\u673a\u6743\u9650\u5c31\u80fd\u591f\u8f7b\u677e\u7684\u83b7\u5f97\u3002\n2.  \u8fd9\u4e2a\u6b65\u9aa4\u7f51\u4e0a\u7684\u8d44\u6599\u5f88\u5c11\uff0c\u901a\u8fc7\u89c2\u5bdf\u5fae\u4fe1\u7684\u6587\u4ef6\u7ed3\u6784\u83b7\u5f97\u7684\u3002\u6587\u4ef6\u8def\u5f84\u4e3a\uff1a\"/data/data/com.tencent.mm/shared_prefs/system_config_prefs.xml\" \u662f\u4e00\u4e2axml\u6587\u4ef6\uff0c\u5f53\u524d\u767b\u5f55\u7684\u5fae\u4fe1uin\u7684key\u4e3adefault_uin ,\u901a\u8fc7android\u4e2d\u7684XmlParser\u7c7b\u5c31\u80fd\u591f\u8fdb\u884c\u89e3\u6790\uff0c\u82e5\u662f\u5f53\u524d\u6ca1\u6709\u767b\u5f55\u5fae\u4fe1\u8d26\u53f7\u7684\u8bdd\uff0cdefault_uin\u5bf9\u5e94\u7684\u503c\u4e3a0 \u3002\n3.  IME+uin\u7684\u610f\u601d\u662f\u5c06\u4e24\u4e2a\u5b57\u7b26\u4e32\u8fdb\u884c\u62fc\u63a5\uff0c\u7136\u540e\u8fdb\u884cMD5\u52a0\u5bc6\uff0c\u83b7\u5f97\u52a0\u5bc6\u540e\u5b57\u7b26\u4e32\u7684\u524d7\u4f4d\uff0c\u5f97\u5230\u5f53\u524d\u767b\u5f55\u5fae\u4fe1\u53f7\u7684enMicroMsg.db\u7684\u5bc6\u7801(\u5f53\u524d\u767b\u5f55\u5fae\u4fe1enMicroMsg.db\u5bc6\u7801)\u3002\n4.  \u8fd9\u4e2a\u6b65\u9aa4\u4e0d\u662f\u5f88\u5bb9\u6613\uff0c\u7f51\u4e0a\u4e5f\u6ca1\u6709\u8d44\u6599\uff1a\u4e0b\u9762\u662f\u5fae\u4fe1\u7684\u804a\u5929\u8bb0\u5f55\u6587\u4ef6\u76ee\u5f55\u3002\n![\u6587\u4ef6\u76ee\u5f55](https://github.com/szhua/CustomerServiceAssistant/blob/master/20141202102707406.png)\n\n\u8fd9\u4e2a\u56fe\u662f\u4ece\u7f51\u4e0acopy\u4e0b\u6765\u7684\uff0c\u5b9e\u9645\u4e2d\u7684\u6587\u4ef6\u76ee\u5f55\u548c\u8fd9\u4e2a\u5927\u81f4\u76f8\u540c\uff0c\u4e0d\u540c\u7684\u662f\u4f1a\u6709\u597d\u51e0\u4e2a\u548c\u5212\u7ea2\u90e8\u5206\u76f8\u540c\u7684\u6587\u4ef6\u5939\uff0c\u8fd9\u4e2a\u6587\u4ef6\u5939\u91cc\u9762\u4e0d\u77e5\u9053\u54ea\u4e2a\u6709\u6211\u4eec\u9700\u8981\u7684enMicroMsg.db\u6587\u4ef6\uff0c\u5373\u4f7f\u6709enMicroMsg.db\u6587\u4ef6\n\u4e5f\u4e0d\u4e00\u5b9a\u662f\u6211\u4eec\u9700\u8981\u7684\u90a3\u4e2a\u6570\u636e\u5e93\u6587\u4ef6\u3002\u8003\u8651\u5230\u4e00\u4e2a\u673a\u5668\u4e0d\u4f1a\u6709\u5f88\u591a\u7684\u5fae\u4fe1\u53f7\uff0c\u672c\u4eba\u611a\u7b28\u4f7f\u7528\u4e86\u4e0b\u9762\u7684\u65b9\u5f0f\u8fdb\u884c\u89e3\u6790\u3002\n\u4e0b\u9762\u662f\u89e3\u5bc6\u6b65\u9aa4\uff1a\n![](https://github.com/szhua/CustomerServiceAssistant/blob/master/\u89e3\u5bc6.png)\nMIcroMsg\u6587\u4ef6\u5939\u7684\u8def\u5f84\u662f\u56fa\u5b9a\u7684\uff0c\u76f4\u63a5\u8fdb\u884c\u62fc\u63a5\u5c31\u53ef\u4ee5\u4e86\u3002\u7136\u540e\u6211\u4eec\u901a\u8fc7\u904d\u5386\u5f97\u5230\u7b26\u5408MD5\u7684\u6587\u4ef6\u5939\uff0c\u8fd9\u6837\u7684\u6587\u4ef6\u5939\u4e0b\u53ef\u80fd\u4f1a\u6709\u4ec5\u5b58\u5728\u4e00\u4e2aenMicriMsg,db\u6587\u4ef6\uff0c\u904d\u5386\u4ee5\u540e\u4e00\u822c\u7684\u624b\u673a\u4e0d\u4f1a\u5927\u4e8e5\u4e2a\u5fae\u4fe1\u53f7\uff0c\n\u8fd9\u4e5f\u4f7f\u6211\u4eec\u7684\u5de5\u4f5c\u53d8\u5f97\u597d\u505a\u4e86\u5f88\u591a\uff0c\u4e0a\u9762\u7684\u65b9\u6cd5\u7684\u6548\u7387\u4e5f\u5c31\u4e0d\u4f1a\u5f88\u4f4e\u3002\u89e3\u5bc6\u540e\uff0c\u6211\u4eec\u5c06\u6570\u636e\u5e93\u7684\u8def\u5f84\uff0c\u5bc6\u7801\uff0c\u548cuin\u4fe1\u606f\u4e00\u4e00\u5bf9\u5e94\u7684\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\uff0c\u4e0b\u6b21\u518d\u8fdb\u884c\u89e3\u5bc6\u7684\u65f6\u5019\u5c31\u76f4\u63a5\u627e\u5230\u6b64\u6587\u4ef6\u5c31\u884c\u4e86\u3002\n<br/>\n\u81f3\u6b64\uff0c\u6211\u4eec\u5c31\u5c06\u6570\u636e\u5e93\u2014\u2014enMicroMsg.db\u6587\u4ef6\u62ff\u51fa\u6765\uff0c\uff08\u8bb0\u4f4f\u8fd9\u4e2aenMicroMsg\u662f\u5f53\u524d\u767b\u5f55\u7528\u6237\u7684\u6570\u636e\u5e93\u6587\u4ef6\uff09\uff0c\u5e76\u4e14\u8fdb\u884c\u4e86\u89e3\u5bc6\uff1b\n\n<strong>\u4e0b\u9762\u662f\u89e3\u5bc6\u7684\u6574\u4f53\u6b65\u9aa4\uff1a<strong/>\n![\u6574\u4f53\u6b65\u9aa4](https://github.com/szhua/CustomerServiceAssistant/blob/master/\u4f7f\u7528sqlChiper\u89e3\u5bc6enMicrMsg.db.png)\n\n\u8d34\u51fa\u4f7f\u7528sqlChiper\u89e3\u5bc6enMicroMsg\u7684\u6b65\u9aa4\u4ee3\u7801\uff1a\n\n```java\n\n/**\n* \u4f7f\u7528Sqlcipher\u83b7\u5f97\u89e3\u5bc6\u6570\u636e\u5e93\u5e76\u4e14\u83b7\u5f97\u6570\u636e;\n* @param dbFile\n* @param pass\n* @param isFromOld\n* @param uin\n* @param dbManager\n* @return\n*/\npublic  UserInfo getDataWithSqlcipher(File dbFile, String pass, boolean isFromOld , String uin , DbManager dbManager) {\n    //\u83b7\u5f97\u6700\u7ec8\u7684db\u6587\u4ef6\u5e76\u8fdb\u884c\u8bfb\u53d6\u5176\u4e2d\u7684\u6570\u636e\uff1b\n    UserInfo userInfo = null;\n\n        SQLiteDatabaseHook hook = new SQLiteDatabaseHook()  {\n            public void preKey(SQLiteDatabase database) {\n            }\n            public void postKey(SQLiteDatabase database) {\n                //\u6267\u884c\u8fd9\u6837\u7684sql\u8bed\u53e5\u8fdb\u884c\u5bf9\u6570\u636e\u5e93\u7684\u89e3\u5bc6\uff1b\n                database.rawExecSQL(\"PRAGMA cipher_migrate;\");\n            }\n        };\n\n        try {\n\n            /**\n            * \u4ece\u6570\u636e\u5e93\u4e2d\u83b7\u5f97\u4e2a\u4eba\u4fe1\u606f\uff1b\n            */\n            UserInfo userinfo =readUserInfoFromWx(dbFile,pass,hook);\n            /**\n            * \u82e5\u662f\u6570\u636e\u4e2d\u6ca1\u6709\u6570\u636e\u7684\u8bdd;\n            */\n            if (!isFromOld) {\n                  setNewUsersInDb(dbFile.getAbsolutePath(),pass,uin,dbManager);\n            }\n\n            return userinfo;\n        } catch (Exception e) {\n            Log.e(\"szhua\", \"there is what erro is happened in sqlCipher : \"+e.toString());\n            return null;\n        }\n\n}\n\n\n\n/**\n* \u4ece\u5fae\u4fe1\u6570\u636e\u5e93\u4e2d\u83b7\u5f97\u4e2a\u4eba\u4fe1\u606f;\n* @param dbFile\n* @param pass\n* @param hook\n* @return\n*/\nprivate  UserInfo  readUserInfoFromWx(File dbFile ,String pass ,SQLiteDatabaseHook hook){\n    //\u4ee5\u8fd9\u6837\u7684\u65b9\u5f0f\u53bb\u8bfb\u53d6\u6570\u636e\u5e93\u4e2d\u7684\u6587\u4ef6\uff0c\u786e\u4fdd\u6587\u4ef6\u7684\u5b8c\u6574\u6027\uff1a\n    //@WeChat ====\u300b\u5fae\u4fe1\u5ba2\u6237\u7aef\u6237\u5bf9\u672c\u5730\u7684\u6570\u636e\u5e93\u8fdb\u884c\u5224\u65ad\uff0c\u53d1\u73b0\u6587\u4ef6\u88ab\u7834\u574f\u7684\u8bdd\u5c31\u4f1a\u6267\u884c\u91cd\u65b0\u767b\u5f55\u64cd\u4f5c\uff0c\u5e76\u4e14\u4f1a\u5bf9\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u6e05\u9664\uff1a\n    //\u8fd9\u6837\u7684\u4f53\u9a8c\u5bf9\u7528\u6237\u6765\u8bf4\u80af\u5b9a\u662f\u4e0d\u884c\u7684\u3002 \u6545\u653e\u5f03\u5b98\u65b9\u7684\u6253\u5f00\u65b9\u5f0f \u4f7f\u7528\u4e0b\u9762\u7684\u65b9\u6cd5\u3002\n    SQLiteDatabase db = SQLiteDatabase.openDatabase(dbFile.getAbsolutePath(), pass, null, SQLiteDatabase.OPEN_READWRITE, hook);\n    Cursor c = db.query(\"userinfo\", null, null, null, null, null, null);\n    UserInfo userinfo = new UserInfo();\n    while (c.moveToNext()) {\n        String id = c.getString(c.getColumnIndex(\"id\"));\n        String value = c.getString(c.getColumnIndex(\"value\"));\n        if (id.equals(\"12325\")) {\n            userinfo.setProvince(value);\n        } else if (id.equals(\"12326\")) {\n            userinfo.setCity(value);\n        } else if (id.equals(\"4\")) {\n            userinfo.setNickName(value);\n        } else if (id.equals(\"12293\")) {\n            userinfo.setProvinceCn(value);\n        } else if (id.equals(\"12292\")) {\n            userinfo.setCityCn(value);\n        } else if (id.equals(\"2\")) {\n            userinfo.setWxId(value);\n        } else if (id.equals(\"6\")) {\n            userinfo.setPhone(value);\n        } else if (id.equals(\"42\")) {\n            userinfo.setWxNumber(value);\n        }\n    }\n    c.close();\n    db.close();\n    return  userinfo ;\n}\n\n\n```\n\n<strong> \u4ee5\u4e0a\u7b80\u5355\u7684\u4ecb\u7ecd\u4e00\u4e0b\u600e\u6837\u7684\u89e3\u5bc6\u5fae\u4fe1\u7684\u6570\u636e\u5e93\uff0c\u5728\u624b\u673aroot\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u600e\u6837\u7684\u65b9\u5f0f\u53bb\u83b7\u5f97\u3002\u9700\u8981\u63d0\u53ca\u7684\u662f\uff0candroid\u7684\u5fae\u4fe1\u6570\u636e\u5e93\u52a0\u5bc6\u4f7f\u7528\u7684\u5c31\u662fsqlChiper\u7684\u5f00\u6e90\u5e93\u6240\u4ee5\u6211\u4eec\u624d\u5f97\u4ee5\u987a\u5229\u7684\u7834\u89e3\u4eba\u5bb6\u6570\u636e\u5e93\u3002\u82e5\u662f\u5fae\u4fe1\u7248\u672c\u53d8\u66f4\uff0c\u4f7f\u7528\u522b\u7684\u52a0\u5bc6\u65b9\u5f0f\u6211\u4eec\u5c31\u96be\u4ee5\u4e0b\u624b\u4e86~\n<strong/>\n<hr/>\n<hr/>\n<strong \u00a0>\u5173\u4e8e\u672c\u4f8b\u4e2d\u7684\u8fdb\u7a0b\u4fdd\u6d3b\u624b\u6bb5\n \u5fae\u4fe1\u6570\u636e\u7684\u4e0a\u4f20\u9700\u8981\u4e00\u4e2a\u5e38\u9a7b\u7684\u540e\u53f0\u4efb\u52a1\uff0c\u8fd9\u4e2a\u4efb\u52a1\u7684\u4f18\u5148\u7a0b\u5ea6\uff0c\u5b58\u6d3b\u7387\u4fdd\u8bc1\u4e86\u6570\u636e\u4e0a\u4f20\u7684\u7a33\u5b9a\u6027\uff0c\u4e0b\u9762\u662f\u4e3a\u4e86\n\u4fdd\u6d3b\u5b9e\u884c\u7684\u4e00\u4e9b\u624b\u6bb5\u3002<strong/>\n\n\n> IPC\u673a\u5236\uff1aAIDL\u8ba9\u8fdb\u7a0b\u4e4b\u95f4\u4fdd\u6301\u957f\u8fde\u63a5\n\n ```java\n   /*\u5173\u4e8eAIDL\u673a\u5236\uff0c\u8bf7\u53c2\u8003\u4e00\u4e9b\u6587\u6863\n   \u6ce8\uff1a\u4e3b\u8fdb\u7a0b\u4e0e\u5b88\u62a4\u8fdb\u7a0b\u90fd\u8981\u8fdb\u884c\u8fd4\u56deBinder\n   */\n   @Override\n    public IBinder onBind(Intent intent) {\n        return customBinder;\n    }\n  private class CustomBinder extends  CustomAidlInterface.Stub{\n        @Override\n        public void basicTypes(int anInt, long aLong, boolean aBoolean, float aFloat,\n        double aDouble, String aString) throws RemoteException {\n        }\n    }\n ```\n>\u4f7f\u7528bindService\u76d1\u542c\u670d\u52a1\u4e4b\u95f4\u7684\u94fe\u63a5\u3002\n\n```java\n /*\u670d\u52a1\u4e4b\u95f4\u7684\u7ed1\u5b9a\uff1abindService*/\n WorkService.this.bindService(new Intent(WorkService.this,WatchDogService.class),customConnection, Context.BIND_IMPORTANT);\n\n /*\u76d1\u542c\u670d\u52a1\u4e4b\u95f4\u7684\u94fe\u63a5\u60c5\u51b5*/\n private class CustomConnection implements ServiceConnection{\n        @Override\n        public void onServiceConnected(ComponentName name, IBinder service) {\n        }\n        @Override\n        public void onServiceDisconnected(ComponentName name) {\n            WorkService.this.startService(new Intent(WorkService.this,WatchDogService.class));\n            WorkService.this.bindService(new Intent(WorkService.this,WatchDogService.class),customConnection,Context.BIND_IMPORTANT) ;\n        }\n    }\n```\n\n>\u542f\u52a8\u524d\u53f0\u670d\u52a1\uff0c\u63d0\u9ad8\u8fdb\u7a0b\u7684\u4f18\u5148\u7ea7\u3002\n\n```java\n   /*\u8fd9\u6837\u4f1a\u5728\u542f\u52a8\u670d\u52a1\u7684\u65f6\u5019\u591a\u4e00\u4e2aNOtification,\u4f46\u662f\u65e0\u4f24\u5927\u96c5\uff0c\u80fd\u591f\u63d0\u9ad8\u8fdb\u7a0b\u7684\u4f18\u5148\u7ea7*/\n   \n   Notification.Builder builder =new Notification.Builder(this);\n        PendingIntent pendingIntent =PendingIntent.getService(this,0,intent,0);\n        builder.setSmallIcon(R.mipmap.ic_launcher)\n                .setContentIntent(pendingIntent)\n                .setTicker(\"\u542f\u52a8\u670d\u52a1\u4e2d\")\n                .setAutoCancel(false)\n                .setWhen(System.currentTimeMillis())\n                .setContentTitle(\"\u5ba2\u670d\u52a9\u624b\");\n        if(Build.VERSION.SDK_INT>=Build.VERSION_CODES.JELLY_BEAN){\n            builder.setContentInfo(\"\u5ba2\u670d\u52a9\u624b\");\n        }\n        if(Build.VERSION.SDK_INT>Build.VERSION_CODES.JELLY_BEAN){\n            startForeground(startId,builder.build());\n        }else{\n            startForeground(startId,builder.getNotification());\n        }\n```\n> onStartCommand\u4e2d\u8fd4\u56de\u6570\u503c\uff0c\u4fdd\u8bc1\u670d\u52a1\u7684\u5b58\u6d3b\u7387\u3002\n\n ```java\n   return START_STICKY;\n```\n\n> android5.0\u4ee5\u4e0a\u7684\u4f7f\u7528JobScheduler,5.0\u4ee5\u4e0b\u7684\u4f7f\u7528AlarmManager\uff0c\u5b9a\u65f6\u4efb\u52a1\u5b9a\u65f6\u5524\u9192\n\n ```java\n /**\n * Android 5.0+ \u4f7f\u7528\u7684 JobScheduler.\n * \u8fd0\u884c\u5728 :watch \u5b50\u8fdb\u7a0b\u4e2d.\n */\n@TargetApi(Build.VERSION_CODES.LOLLIPOP)\npublic class JobSchedulerService extends JobService {\n...\n}\n/*\u4f7f\u7528*/\n  if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {\n            JobInfo.Builder builder = new JobInfo.Builder(sHashCode, new ComponentName(getApplication(), JobSchedulerService.class));\n            builder.setPeriodic(INTERVAL_WAKE_UP);\n            //Android 7.0+ \u589e\u52a0\u4e86\u4e00\u9879\u9488\u5bf9 JobScheduler \u7684\u65b0\u9650\u5236\uff0c\u6700\u5c0f\u95f4\u9694\u53ea\u80fd\u662f\u4e0b\u9762\u8bbe\u5b9a\u7684\u6570\u5b57\n        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.N) builder.setPeriodic(JobInfo.getMinPeriodMillis(), JobInfo.getMinFlexMillis());\n            builder.setPersisted(true);\n            JobScheduler scheduler = (JobScheduler) getSystemService(JOB_SCHEDULER_SERVICE);\n            scheduler.schedule(builder.build());\n        } \n        \n        \n          //Android 4.4- \u4f7f\u7528 AlarmManager\n            AlarmManager am = (AlarmManager) getSystemService(ALARM_SERVICE);\n            Intent i = new Intent(getApplication(), WorkService.class);\n            PendingIntent pi = PendingIntent.getService(getApplication(), sHashCode, i, PendingIntent.FLAG_UPDATE_CURRENT);\n            am.setRepeating(AlarmManager.RTC_WAKEUP, System.currentTimeMillis() + INTERVAL_WAKE_UP, INTERVAL_WAKE_UP, pi);\n```\n> \u4f7f\u7528RxJava\u4e2d\u7684\u8ba2\u9605\u673a\u5236\uff0c\u9632\u6b62\u4efb\u52a1\u7684\u91cd\u590d\u542f\u52a8\uff0c\u76d1\u6d4b\u670d\u52a1\u7684\u5b58\u6d3b\u3002\n\n ```java\n   if (sSubscription != null && !sSubscription.isUnsubscribed()&&sSubscription2!=null&&!sSubscription2.isUnsubscribed()) return START_STICKY;\n\n        sSubscription = Observable\n                .interval(5, TimeUnit.SECONDS)\n                .subscribe(new Subscriber<Long>() {\n                    @Override\n                    public void onCompleted(){\n                    }\n                    @Override\n                    public void onError(Throwable e){\n                    }\n                    @Override\n                    public void onNext(Long count) {\n                        Log.i(\"testIsRunning\",\"isRunning!\"+Thread.currentThread().getName());\n                    }\n                });\n\n\n\n        sSubscription2 =  Observable\n                .interval(60, TimeUnit.SECONDS)\n                .subscribeOn(AndroidSchedulers.mainThread())\n                .subscribe(new Subscriber<Long>() {\n                    @Override\n                    public void onCompleted(){\n                    }\n                    @Override\n                    public void onError(Throwable e){\n                    }\n                    @Override\n                    public void onNext(Long count) {\n                        Log.i(\"testIsRunning\",\"isRunning2!\"+Thread.currentThread().getName());\n                        /*\u4e4b\u6240\u4ee5\u4f7f\u7528handler\u6765\u6267\u884c\u4efb\u52a1\uff0c\u8fd9\u91cc\u662f\u4fdd\u8bc1\u6b64Subscription\u4e0d\u88ab\u9500\u6bc1\uff0c\u6301\u7eed\u7684\u8fdb\u884c\u5468\u671f\u6027\u7684\u4e0a\u4f20*/\n                        workHandler.sendEmptyMessage(0) ;\n                    }\n                });\n\n```\n>\u7b80\u5355\u5b88\u62a4\u5f00\u673a\u5e7f\u64ad\n\n   ```java\n     //\u82e5\u9700\u8981\u9632\u6b62 CPU \u4f11\u7720\uff0c\u8fd9\u91cc\u7ed9\u51fa\u4e86 WakeLock \u7684\u53c2\u8003\u5b9e\u73b0\n         PowerManager pom = (PowerManager) getSystemService(POWER_SERVICE);\n        sWakeLock = pom.newWakeLock(PowerManager.PARTIAL_WAKE_LOCK, WatchDogService.class.getSimpleName());\n        sWakeLock.setReferenceCounted(false);\n        IntentFilter intentFilter = new IntentFilter();\n        intentFilter.addAction(Intent.ACTION_SCREEN_OFF);\n        intentFilter.addAction(Intent.ACTION_SCREEN_ON);\n        try {\n            getApplication().registerReceiver(WakeLockReceiver.getInstance(), intentFilter);\n        } catch (Exception ignored) {\n        }\n```\n\n\n\n\n\n\n\n\n"}, {"repo": "deepansh27/CustomerSegmentation", "language": "Jupyter Notebook", "readme_contents": "What is Customer Segmentation?\nCustomer segmentation is the process of dividing customers into groups based on common characteristics so companies can market to each group effectively and appropriately. A company might segment customers according to a wide range of factors, including:\n* Demographics\n* Transaction history\n* Geography\n* Psychographic\n\n\nWhy Segment Customers?\nSegmentation allows marketers to better tailor their marketing efforts to various audience subsets. Those efforts can relate to both communications and product development. Specifically, segmentation helps a company:\nCreate and communicate targeted marketing messages that will resonate with specific groups of customers, but not with others (who will receive messages tailored to their needs and interests, instead).\nSelect the best communication channel for the segment, which might be email, social media posts, radio advertising, or another approach, depending on the segment.\n-Identify ways to improve products or new product or service opportunities.\n-Establish better customer relationships.\n-Test pricing options.\n-Focus on the most profitable customers.\n-Improve customer service.\n-Upsell and cross-sell other products and services.\n\nHow to Segment Customers?\nCustomer segmentation requires a company to gather specific information about customers and analyze it to identify patterns that can be used to create segments.\nSome of that can be gathered from the customers purchasing information such as job title, geography, products purchased, for example. Some of it might be gleaned from how the customer entered your system. An online marketer working from an opt-in email list might segment marketing messages according to the opt-in offer that attracted the customer, Other information, however, including consumer demographics such as age and marital status, will need to be acquired in other ways.\nTypical information-gathering methods include:\n-Face-to-face or telephone interviews\n-Surveys\n-General research using published information about market categories\n-Focus groups\n\nOne commonly used techinque for business-to-consumer marketing is Recency Frequency Monetary analysis, where:\nRecency: Recency is how recently for the date of analysis did the customer make a purchase. Customers who have purchased recently are more likely to purchase again when compared to those who did not purchase recently.\nFrequency: Frequency is how often did the customer made purchases. The higher the frequency, the higher is the chances of these responding to the offers.\nMonetary: Monetary is the total revenue generated by the customer through thier purchases. Customers who have spent higher contribute more value to the business as compared to those who have spent\nProduct Categorizing: To find the categories of products previously purchased by the customers"}, {"repo": "zhuangjiesen/CustomerAndProvider", "language": "Java", "readme_contents": "# CustomerAndProvider\n\u4e24\u4e2a\u9879\u76ee\uff1a\u57fa\u4e8espring + maven \u7684java\u5bb9\u5668\n### 1.\u7528\u4f5c\u5ba2\u6237\u7aef\u7684 JavaProCustomer \u9879\u76ee\n### 2.\u7528\u4f5c\u670d\u52a1\u7aef\u7684 JavaProServer \u9879\u76ee \n\n### Thrift\u8fde\u63a5\u6c60\uff0cspring\u914d\u7f6e\u5316\uff0c\u900f\u660e\u5316\u8c03\u7528 \u9879\u76ee\u535a\u5ba2\u5730\u5740\uff1a\n[\u9879\u76ee\u535a\u5ba2\u5730\u5740](http://blog.csdn.net/zjs40/article/details/63265407)\n\n\n### WebSocket \u4e0e netty\u670d\u52a1\u5668 \u5b9e\u6218 \u9879\u76ee\u535a\u5ba2\u5730\u5740\n[\u9879\u76ee\u535a\u5ba2\u5730\u5740](http://blog.csdn.net/zjs40/article/details/70185790)\n\n"}, {"repo": "krishnaik06/Predict_customer_Will_Leave_the-Bank", "language": "Python", "readme_contents": "# Predict_customer_Will_Leave_the-Bank"}, {"repo": "IBM/run-campaigns-target-customers", "language": "Jupyter Notebook", "readme_contents": "# Determine target audience and run marketing campaigns \n\nA business runs marketing campaigns to promote products with the objective of boosting revenues. The campaigns need to be run on appropriate audiences for maximum impact. A consumer not interested in a product will ignore the campaign offer.\n\nThere are two steps to running a marketing campaign:\n\n* Identifying the target audience - The target audience can be determined by analyzing the purchases and browsing history of customers, social media posts, reviews and other data sources. This will help identify customers who could be interested in a product.  \n\n* Run campaigns(e-mail, sms, phone etc.) on target audience.\n\n\n![](images/usecase_flow.png)\n\nIn this pattern, the following aspects are covered:\n* Identifying the target audience to run a campaign in a certain product category.\n* Run an E-mail campaign for the target audience.\n\nThe customer demographics and sales data available [here](https://dataplatform.ibm.com/exchange/public/entry/view/f8ccaf607372882403a37d9019b3abf4) is used to demonstrate the above. The sales data includes multiple categories of products. For this pattern, we demonstrate identifying the target audience and running a campaign for *Canned Foods*.\n\nWhen the reader has completed this pattern, they will understand how to:\n* Create and run a Jupyter notebook in Watson Studio.\n* Use Object Storage to access data files.\n* Use Python Pandas to derive insights and come up with a target audience based on the customer purchase data.\n* Integrate with Watson Campaign Automation to run a campaign on the target audience.\n\nThe intended audience for this code pattern are Developers and Data Scientists who want to build an end to end marketing campaign solution for a business.\n\n![](images/architecture.png)\n\n\n1. The Object storage stores the data.\n2. Data is utilized as csv files.\n3. The Jupyter notebook processes the data and generates the target audience.\n4. The Jupyter notebook is powered by Spark.\n5. The target audience information is sent to Watson campaign automation(WCA) to run campaigns.\n\n## Included components\n\n* [IBM Watson Studio](https://www.ibm.com/cloud/watson-studio): Analyze data using RStudio, Jupyter, and Python in a configured, collaborative environment that includes IBM value-adds, such as managed Spark.\n\n* [IBM Cloud Object Storage](https://console.bluemix.net/catalog/infrastructure/cloud-object-storage): An IBM Cloud service that provides an unstructured cloud data store to build and deliver cost effective apps and services with high reliability and fast speed to market.\n\n* [Watson campaign automation](https://www.ibm.com/in-en/marketplace/digital-marketing-and-lead-management): Smarter marketing automation across all digital channels, powered by Watson.\n\n## Featured technologies\n\n* [Data Science](https://medium.com/ibm-data-science-experience/): Systems and scientific methods to analyze structured and unstructured data in order to extract knowledge and insights.\n\n* [Marketing automation](https://en.wikipedia.org/wiki/Marketing_automation): Software platforms and technologies designed for marketing departments and organizations to more effectively market on multiple channels online (such as email, social media, websites, etc.) and automate repetitive tasks.\n\n# Watch the Video\n\n[![](http://img.youtube.com/vi/_Nk4vjHsxfI/0.jpg)](https://youtu.be/_Nk4vjHsxfI)\n\n# Steps\n\nFollow these steps to setup and run this developer journey. The steps are\ndescribed in detail below.\n\n1. [Sign up for Watson Studio](#1-sign-up-for-watson-studio)\n1. [Create contacts database in Watson Campaign Automation](#2-create-contacts-database-in-watson-campaign-automation)\n1. [Create campaign contact list in Watson Campaign Automation](#3-create-campaign-contact-list-in-watson-campaign-automation)\n1. [Create campaign mailing template in Watson Campaign Automation](#4-create-campaign-mailing-template-in-watson-campaign-automation)\n1. [Configure application access in Watson Campaign Automation](#5-configure-application-access-in-watson-campaign-automation)\n1. [Create the notebook](#6-create-the-notebook)\n1. [Add the data](#7-add-the-data)\n1. [Update the notebook with service credentials](#8-update-the-notebook-with-service-credentials)\n1. [Update Watson Campaign Automation URLs and credentials in the notebook](#9-update-watson-campaign-automation-urls-and-credentials-in-the-notebook)\n1. [Run the notebook](#9-run-the-notebook)\n1. [Analyze the results](#10-analyze-the-results)\n\n## 1. Sign up for Watson Studio\n\nSign up for IBM's [Watson Studio](https://datascience.ibm.com/). \n\n## 2. Create contacts database in Watson Campaign Automation\n\nLogin to the [Watson campaign automation](https://www.ibm.com/in-en/marketplace/digital-marketing-and-lead-management) instance.\n\nSelect `Data` and then `Databases`. Select `Shared` and click `Create`.\n\n![](images/create_database.png)\n\nConfigure the database by entering a name `wdp_wca_db`. Click on `Next`.\n\n![](images/configure_database.png)\n\nClick on `Create` to create the database.\n\n![](images/create_db_finish.png)\n\nSelect `Data` and then `Databases`. Click `View` to view the list of databases.\n\n![](images/goto_contact_db.png)\n\nClick on the database `wdp_wca_db`.\n\n![](images/select_contact_db.png)\n\nNote the database id.\n\n![](images/note_database_id.png)\n\n\n## 3. Create campaign contact list in Watson Campaign Automation\n\nSelect `Data` and then `Contact Lists`. Click on `Create Contact List`.\n\n![](images/view_contact_list.png)\n\nConfigure the campaign list by selecting the parent database as `wdp_wca_db`. Enter a name `campaign_canned_food_contact_list`. Click `OK`.\n\n![](images/configure_campaign_list.png)\n\nSelect `Data` and then `Contact Lists`. Click `View` to view the list of contact lists.\n\n![](images/goto_contact_list.png)\n\nClick on the list `campaign_canned_food_contact_list`.\n\n![](images/select_contact_list.png)\n\nNote the contact list id.\n\n![](images/note_contact_list_id.png)\n\n## 4. Create campaign mailing template in Watson Campaign Automation\n\nClick on `Content`. Under `Create Mailings` click on `Mailing Template`.\n\n![](images/invoke_create_mailing_template.png)\n\nSelect the `Welcome Simple` mailing template.\n\n![](images/select_mailing_template.png)\n\nClick to open the selected mailing template. Click `Open` to customize the mailing template.\n\n![](images/open_mailing_template.png)\n\nClick on `Select Contacts` and select `campaign_canned_food_contact_list`. Click `Done`.\n\n![](images/customize_mailing_template.png)\n\nCustomize the template as shown below and change the name to `CannedFoodsCampaignJune2018`. Click on `Save`.\n\n![](images/edit_name_save_template.png)\n\nClick on `Content`. Under `View Mailings` click on `Templates`.\n\n![](images/view_mailing_templates.png)\n\nThe list of templates can be seen. Hover the mouse over `CannedFoodsCampaignJune2018` and note the template id.\n\n![](images/list_templates.png)\n\n## 5. Configure application access in Watson Campaign Automation\n\nLogin to the Watson Campaign Automation instance. Go to `Settings` and select `Organization Settings`.\n\n![](images/invoke_settings.png)\n\nGoto `Application Account Access` and click `Add Application`.\n\n![](images/invoke_add_application.png)\n\nAdd an application name `customer_insights_wstudio` and click `Add`.\n\n![](images/add_application_name.png)\n\nNote the client id and client secret. Click `Close`.\n\n![](images/note_client_secret.png)\n\nClick on `Add Account Access`.\n\n![](images/invoke_account_access.png)\n\nSelect the application `customer_insights_wstudio` and click `Add`.\n\n![](images/enter_account_access_details.png)\n\nA refresh token is sent to the registered email-d. Please note the refresh token.\n\n## 6. Create the notebook\n* In [Watson Studio](https://dataplatform.ibm.com) - create a project if necessary, provisioning an object storage service if required.\n* Go to `Tools` and select `Notebook`.\n* Select the `From URL` tab.\n* Enter a name for the notebook.\n* Optionally, enter a description for the notebook.\n* Select the project.\n* Enter this Notebook URL: https://github.com/IBM/run-campaigns-target-customers/blob/master/notebooks/campaign_management.ipynb\n* Select the runtime as shown below.\n* Click the `Create` button.\n\n![](images/create_notebook_from_url.png)\n\n## 7. Add the data\n\n#### Add the data to the notebook\n\n* Please download the file from https://dataplatform.ibm.com/exchange/public/entry/view/f8ccaf607372882403a37d9019b3abf4.\n* Rename the file as `customer_orders.csv`\n* All the email addresses in the file are invalid. For testing the pattern, modify and add valid email address for couple of contacts that get added to Watson Campaign Automation after the notebook execution.\n* From your project page in Watson Studio, click `Find and Add Data` (look for the `10/01` icon)\nand its `Files` tab.\n* Click `browse` and navigate to where you downloaded `customer_orders.csv` on your computer.\n* Add the files to Object storage.\n\n![](images/add_file.png)\n\n## 8. Update the notebook with service credentials \n\n#### Add the Object Storage credentials to the notebook\n\n* Select the cell below `2.1 Add your service credentials for Object Storage` section in the notebook to update the credentials for Object Store.\n* Use `Find and Add Data` (look for the `10/01` icon) and its `Files` tab. You should see the file names uploaded earlier. Make sure your active cell is the empty one created earlier.\n* Select `Insert to code` below `customer_orders.csv`.\n* Click `Insert Crendentials` from the drop down menu.\n* If the credentials are written as `credential_2` change them to `credentials_1`.\n\n![](images/objectstorage_credentials.png)\n\n## 9. Update Watson Campaign Automation URLs and credentials in the notebook\n\n#### Enter the access token generation URL,client id, client secret and refresh token in section 5.\n\nReplace the `<BASE_URL>` with the base url for the Watson Campaign Automation instance.\nAdd the client id, client secret and refresh token noted in section 5.\n\n![](images/clientid_secret_token.png)\n\n#### Enter the url, database id, contact list id and template id noted in sections 2,3 and 4\n\nReplace the `<BASE_URL>` with the base url for the Watson Campaign Automation instance. The `<BASE_URL>` specified here can be different from the `<BASE_URL>` entered previously for getting access tokens.\n\nEnter the database id , contact list id and template id we noted in sections 2,3 and 4.\n\n![](images/xmlapiurl_dbid.png)\n\n## 9. Run the notebook\n\nWhen a notebook is executed, what is actually happening is that each code cell in\nthe notebook is executed, in order, from top to bottom.\n\nEach code cell is selectable and is preceded by a tag in the left margin. The tag\nformat is `In [x]:`. Depending on the state of the notebook, the `x` can be:\n\n* A `blank`, this indicates that the cell has never been executed.\n* A `number`, this number represents the relative order this code step was executed.\n* A `*`, this indicates that the cell is currently executing.\n\nThere are several ways to execute the code cells in your notebook:\n\n* One cell at a time.\n  * Select the cell, and then press the `Play` button in the toolbar.\n* Batch mode, in sequential order.\n  * From the `Cell` menu bar, there are several options available. For example, you\n    can `Run All` cells in your notebook, or you can `Run All Below`, that will\n    start executing from the first cell under the currently selected cell, and then\n    continue executing all cells that follow.\n* At a scheduled time.\n  * Press the `Schedule` button located in the top right section of your notebook\n    panel. Here you can schedule your notebook to be executed once at some future\n    time, or repeatedly at your specified interval.\n\nFor this Notebook, you can simply `Run All` cells.\n\n## 10. Analyze the results\n\nOnce the notebook execution is complete, an email is sent to all the contacts in the contact list `campaign_canned_food_contact_list`.\n\n![](images/campaign_mail.png)\n\n## Troubleshooting\n\n[See DEBUGGING.md.](DEBUGGING.md)\n\n## License\n[Apache 2.0](LICENSE)\n\n\n\n"}, {"repo": "Hari365/customer-segmentation-python", "language": "Jupyter Notebook", "readme_contents": "# customer-segmentation-python\nThis project applies customer segmentation to the customer data from a company and derives conclusions and data driven ideas based on it.\n### Dataset\nThis data set is the customer data of a online super market company Ulabox. The data set is available in this link https://github.com/ulabox/datasets\n### Customer segmentation\nIn customer segmentation we categorize similar customers together in the same cluster and analyse them. It can reveal information like: \n1) who are the most valuable customers of the company \n2) what kinds of customers does the company have\n3) This can be used for targeted marketing and other marketing strategies.\n4) Sometimes it can even reveal a potential white space in the market place which no company has yet occupied.\nWell we can get creative here.\n### Clustering\nClustering is a process in which we put similar data points into the same cluster. There are a lot of algorithms to do this, for example agglomerative heirarchical clustering, kmeans clustering, Gaussian Mixture Model etc.\n## Map to the project\n1) The order_segmentation_0.0.ipynb file contains detailed notes and explanation of doing segmentation of orders in the data. I have also added my ideas in it. It's a clean walk through. I suggest to start there.\n2) The customer_segmentation.ipynb file tries to do segmentation of customers in the data. It is very much similar to the order segmentation notebook. Though it doesn't have a lot of explanation you should be able to understand it after going through the former notebook. At the end of this notebook it gets real interesting.\n3) I have added another file which is a bunch of functions that could help in visualizing and finding meaningful clusters within the data. These functions provide various ways to analyse for clusters in the data.<br>\n4) Model_Building.ipynb is where we build a model to predict the class of each customer, which can be used to find the classes of customers in future. I have added some ideas there.\n5) The two csv files are the results after clustering.<br>\nThank you for your time :)\n"}, {"repo": "Rishabh42/Ethereum-customer-loyalty-program", "language": "JavaScript", "readme_contents": "# Loyalty Program\n\n## Business Problem\nConsumer loyalty programs represent strategic investments and are applicable to all\ntypes of organizations. Currently, their disparate nature does not allow consumers (or\nhave limited capability) to consume them effectively across different\norganizations/establishments. This tends to become cumbersome, complex and is\nvulnerable to manipulations.\nSolution\nA Blockchain based platform that cuts through the loyalty points earned by an individual\nproliferated across domains like travel, retail, financial services, and other economic\nsectors. The corresponding points can be consumed by individual in any of the sphere\nof his choice irrespective of the country/region/sector. The loyalty points\nearned/consumed by an individual cannot be altered by individual/sector. The loyalty\npoints Blockchain does not persist any individual information. The solution is secure to\nensure the data integrity, stable &amp; scalable to rapidly add and maintain loyalty\npartnerships without adding complexity. The platform caters to near real time\nprocessing. The aim is to eliminate any intermediary from the system.\nUse Case <br />\n1. Various companies offering loyalty cards will act as nodes in the private network. <br />\n2. Admin will enable permissions to various organizations on the network. <br />\n3. Admin will list down all cards with their loyalty points measures against a\nstandard unit. This will act as the master table for exchange of points. <br />\n4. A user having loyalty points earned on a particular card can log in and exchange <br />\nthese points against another card. An option to select current card, chose points\nto be exchanged and corresponding new card selection from drop down should\nbe provided to user. Preview option will depict new balance in both the cards and\nif user clicks on \u2018confirm\u2019, changes will take place immediately. <br />\n5. New card and existing value should reflect in respective local DBs of card\ncompanies so that earned/exchanged points can be honored. <br />\n\n## P2P Network\nNo of Nodes: 6\nNode # 1: Reliance Digital <br />\nNode # 2: Star Alliance <br />\nNode # 3: Shoppers Stop <br />\nNode # 4: Mr. Tim <br />\nNode # 5: Mr. John <br />\nNode # 6: Admin <br />\nUsers: Admin, Companies, Users <br />\nAdmin:\nResponsible for maintaining master tables to exchange loyalty points [Need View UI]\nAdd-delete companies [Need UI]\nAdd-delete users [Need UI]\nCompanies:\nMaintenance of loyalty points [Need View UI]\nUsers:\nExchange loyalty points [Maximum 50% of earned loyalty points can be exchanged]\n\n## Smart Contract\nTransfer of relevant $ amount between transacting companies. If balance is less than\nthe required transfer amount, exception will be thrown and user will be requested to\ncontact the parent company.\nIf transfer is successful then exchange of loyalty points based on the master table\ndefined in the blockchain shall take place.\nIn case there is any issue in transferring loyalty points, money exchanged shall be\nreturned back to parent company.\n"}, {"repo": "saikrithik/Janatahack-Customer-Segmentation", "language": "Jupyter Notebook", "readme_contents": "# Janatahack-Customer-Segmentation\nRank 2 Public LB Solution\n# Problem Statement\n<img src=\"Utils/PS.png\"\n     alt=\"Markdown Monster icon\"\n     style=\"float: left; margin-right: 10px;\" />\n## Data at a Glance\n<img src=\"Utils/Data.png\"\n     alt=\"Markdown Monster icon\"\n     style=\"float: left; margin-right: 10px;\" /> \n    \n## Dependencies\n* Python >3.6\n* pandas\n* numpy\n* scikit-learn\n* matplotlib\n* seaborn\n* lightgbm\n* catboost\n* xgboost\n* rfpimp\n* jupyter notebook\n\n## Install dependencies\n```\nPandas:           $ pip install pandas\nnumpy:            $ pip install numpy\nscikit-learn:     $ pip install -U scikit-learn\nmatplotlib:       $ pip install matplotlib \nseaborn:          $ pip install seaborn\nlightgbm:         $ pip install lightgbm\nxgboost:          $ pip install xgboost\nrfpimp:           $ pip install rfpimp\ncatboost:         $ pip install catboost\n```\n\n## Usage\n* Run the code given in ipython notebook `Final.ipynb`\n\n## Contributors\nTeam Name: [Figured the Trick!](https://datahack.analyticsvidhya.com/teams/figured-the-trick) \\\n[Sai Krithik](https://www.linkedin.com/in/venkata-sai-krithik-6a344a199/) & [Karan Juneja](https://www.linkedin.com/in/karan-juneja32/)\n\n[Link to Leaderboard](https://datahack.analyticsvidhya.com/contest/janatahack-customer-segmentation/#LeaderBoard)\n"}, {"repo": "tuyennn/magento2-change-customer-password", "language": "PHP", "readme_contents": "# Change Customer Password In Admin - Magento 2\n\n    composer require ghoster/changecustomerpassword\n\n\nChange Customer Password In Admin Magento 2 module is implements the form in customer edit which allow admin change customer password directly like old-fashion way Magento 1.\n\n[![Latest Stable Version](http://poser.pugx.org/ghoster/changecustomerpassword/v)](https://packagist.org/packages/ghoster/changecustomerpassword)\n[![Total Downloads](http://poser.pugx.org/ghoster/changecustomerpassword/downloads)](https://packagist.org/packages/ghoster/changecustomerpassword)\n[![Latest Unstable Version](http://poser.pugx.org/ghoster/changecustomerpassword/v/unstable)](https://packagist.org/packages/ghoster/changecustomerpassword)\n[![License](http://poser.pugx.org/ghoster/changecustomerpassword/license)](https://packagist.org/packages/ghoster/changecustomerpassword)\n[![PHP Version Require](http://poser.pugx.org/ghoster/changecustomerpassword/require/php)](https://packagist.org/packages/ghoster/changecustomerpassword)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/ae1071a530754edc944356b4e1bcb92f)](https://www.codacy.com/gh/tuyennn/magento2-change-customer-password/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=tuyennn/magento2-change-customer-password&amp;utm_campaign=Badge_Grade)\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.me/thinghost)\n[![Build Status](https://app.travis-ci.com/tuyennn/magento2-change-customer-password.svg?branch=master)](https://app.travis-ci.com/github/tuyennn/magento2-change-customer-password)\n\n\n---\n- [Extension on GitHub](https://github.com/tuyennn/magento2-change-customer-password)\n- [Direct download link](https://github.com/tuyennn/magento2-change-customer-password/tarball/master)\n\n## Main Features\n\n* Add a quick update Password button to customer view in Admin\n\n## Installation with Composer\n\n* Connect to your server with SSH\n* Navigation to your project and run these commands\n \n```bash\ncomposer require ghoster/changecustomerpassword\n\n\nphp bin/magento setup:upgrade\nrm -rf pub/static/* \nrm -rf var/*\n\nphp bin/magento setup:static-content:deploy\n```\n\n## Installation without Composer\n\n* Download the files from github: [Direct download link](https://github.com/tuyennn/magento2-change-customer-password/tarball/master)\n* Extract archive and copy all directories to app/code/GhoSter/ChangeCustomerPassword\n* Go to project home directory and execute these commands\n\n```bash\nphp bin/magento setup:upgrade\nrm -rf pub/static/* \nrm -rf var/*\n\nphp bin/magento setup:static-content:deploy\n```\n## Licence\n[Open Software License (OSL 3.0)](http://opensource.org/licenses/osl-3.0.php)\n\n\n## Donation\nIf this project help you reduce time to develop, you can give me a cup of coffee :) \n\n[![paypal](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.me/thinghost)\n"}, {"repo": "iamar7/Mining-and-Summarising-Customer-Review", "language": "Python", "readme_contents": "# Mining-and-Summarising-Customer-Review\n\nThrough\u200b \u200b this\u200b \u200b project\u200b \u200b our\u200b \u200b aim\u200b \u200b is\u200b \u200b to\u200b \u200b mine\u200b \u200b and\u200b \u200b summarize\u200b \u200b all\u200b \u200b the\u200b \u200b customer\nreview\u200b \u200b of\u200b \u200b a \u200b \u200b product.\u200b \u200b We\u200b \u200b will\u200b \u200b mine\u200b \u200b the\u200b \u200b features\u200b \u200b of\u200b \u200b the\u200b \u200b product\u200b \u200b on\u200b \u200b which\ncustomer\u200b \u200b have\u200b \u200b expressed\u200b \u200b their\u200b \u200b opinions\u200b \u200b and\u200b \u200b whether\u200b \u200b they\u200b \u200b are\u200b \u200b positive\u200b \u200b and\nnegative.\u200b \u200b Our\u200b \u200b task\u200b \u200b is\u200b \u200b done\u200b \u200b in\u200b \u200b three\u200b \u200b steps-:\n\n\u25cf Mining\u200b \u200b product\u200b \u200b features\u200b \u200b that\u200b \u200b have\u200b \u200b been\u200b \u200b commented\u200b \u200b on\u200b \u200b by\u200b \u200b customers\n\n\u25cf Identifying\u200b \u200b features\u200b \u200b of\u200b \u200b the\u200b \u200b product\u200b \u200b that\u200b \u200b customers\u200b \u200b have\u200b \u200b expressed\ntheir\u200b \u200b opinions\u200b \u200b on\u200b \u200b (called\u200b \u200b product\u200b \u200b features)\n\n\u25cf For\u200b \u200b each\u200b \u200b feature,\u200b \u200b identifying\u200b \u200b review\u200b \u200b sentences\u200b \u200b that\u200b \u200b give\u200b \u200b positive\u200b \u200b or\nnegative\u200b \u200b opinions\n\n\u25cf Producing\u200b \u200b a \u200b \u200b summary\u200b \u200b using\u200b \u200b the\u200b \u200b discovered\u200b \u200b information\n"}, {"repo": "php-cuong/magento2-customer-address-autocomplete", "language": "JavaScript", "readme_contents": "# Customer Address Autocomplete Extension for Magento 2\nIn this module, I used the Maps Javascript API of the Google for autocompleting the customer address. When the customer enter the address in the street field, a list of addresses will be suggested by Google Maps Javascript API.\n\n## How to install or upgrade this extension?\n + Under the root of your website, please run the command lines bellowing:\n    - Before installing this extension, make sure that you have installed the PHPCuong_Core module, If you don't install this module yet, please install it by running the command line: **composer require php-cuong/magento2-module-core**\n    - Install the PHPCuong_CustomerAddressAutocomplete module:\n    - **composer require php-cuong/magento2-customer-address-autocomplete**\n    - **php bin/magento setup:upgrade**\n    - **php bin/magento setup:static-content:deploy**\n    - **php bin/magento setup:di:compile**\n    - **php bin/magento indexer:reindex**\n    - **php bin/magento cache:flush**\n\n## How to see the results?\n\n### - On the Backend:\n- Go to the Admin Panel of the Magento Store and navigate to the GiaPhuGroup \u2192 Address Autocomplete\n\n### - On the Storefront:\n- Go to the Edit Address page\n- Go to the Add New Address page\n- Go to the Create New Customer Account page\n- Go the the checkout page.\n\n## The screenshot of this extension\n\n### - On the Storefront:\n\n#### *The interface used for the customer creates new customer account page*\n![ScreenShot](https://github.com/php-cuong/magento2-customer-address-autocomplete/blob/master/Screenshot/address-autocomplete.gif)\n"}, {"repo": "pranaymodukuru/Bertelsmann-Arvato-customer-segmentation", "language": "Jupyter Notebook", "readme_contents": "# Customer Segmentation and Acquisition - Bertelsmann Arvato\n## Machine Learning Engineer Udacity Nanodegree - Capstone Project\n\nThis repository contains code and report for \"Capstone Project - Arvato Customer Segmentation\" done as part of Udacity Machine Learning Engineer Nanodegree program.\n\n\n## Table of Contents\n\n- [Project Overview](#projectoverview)\n- [Data Description](#datadescription)\n- [Technical Overview](#technicaloverview)\n- [Requirements](#requirements)\n- [Results](#results)\n\n***\n\n<a id='projectoverview'></a>\n## Project Overview\n\nIn this project, the demographic data of German population and the customer data have been analysed in order to perform Customer Segmentation and Customer Acquisition. Arvato Financial Solutions is a services company that provides financial services, Information Technology (IT) services and Supply Chain Management (SCM) solutions for business customers on a global scale.\n\nThis project is to help a Mail-Order company to acquire new customers to sell its organic products. The goal of this project is to understand the customer demographics as compared to general population in order to decide whether to approach a person for future products.\n\nThis project is divided into two steps:\n\n1. `Customer Segmentation using Unsupervised Learning`, in this part a thorough data analysis and feature engineering steps are performed to prepare the data for further steps. A Principal Component Analysis (PCA) is performed for dimensionality reduction. Then K-Means Clustering is performed on the PCA components to cluster the general population and the customer population into different segments. These clusters are studied to determine what features make a customer with the help of cluster weights and component weights.\n\n2. `Customer Acquisition using Supervised Learning`, in this part of the project the customers data with defined targets indicating the past responses of the customers has been used to train Supervised Learning algorithms. Then the trained model is used to make predictions on unseen test data to determine whether a person could be a possible customer.\n\n<a id='datadescription'></a>\n## Data Description\n\nThe data has been provided by Udacity and Arvato Financial Solutions. The dataset contains 4 data files and 2 description files. The description files have information about the features and their explanation.\nThe 4 data files include:\n* Customer Segmentation\n  * General Population demographics\n  * Customer demographics\n* Customer Acquisition\n  * Training data\n  * Test data\n\n<a id='technicaloverview'></a>\n## Technical Overview\n\nThe project has been divided into various steps which include:\n* Data Exploration and Cleaning\n* Feature Engineering\n* Dimensionality Reduction\n* Clustering\n* Supervised Learning\n* Model Evaluation\n* Predictions on Test data\n* Submission to Kaggle and Scoring\n\nAn explanation about each step and choice of algorithms, metrics has been given in the `Report.pdf`.\n\n\n<a id='requirements'></a>\n## Requirements\n\nAll of the requirements are given in requirements.txt. To install Run: `pip install -r requirements.txt`\n\n\n<a id='results'></a>\n## Results\n\nThe results have been clearly documented in the Jupyter Notebook. Please refer [Arvato Project Workbook.ipynb](https://github.com/pranaymodukuru/Bertelsmann-Arvato-customer-segmentation/blob/master/Arvato%20Project%20Workbook.ipynb).\n"}, {"repo": "dospuntocero/CustomerQuestions", "language": "PHP", "readme_contents": "#Customer Questions\n\n##Want to keep track of all the questions clients asks in your website?\n\nThe best way is to answer directly on it!\n\nThis is a module for storing those emails as dataobjects with the ability to answer those directly in the module. The client will get a branded email from your website and you will keep record of all emails ever received. What a great deal! isn't it?\n\n## Requirements ##\n\nThis module requires [Silverstripe 3](http://silverstripe.org/download) and [UncleCheese's dashboard module](https://github.com/unclecheese/silverstripe-dashboard)\n\n## Installation ##\nJust as any other silverstripe module, drag it into the root folder, run dev/build, and you are done. In the left menu of your site you will find a \"Customer Questions\" section for keeping track of all of the questions clients have sent to the website.\n\nthen you need to create a ContactPage and set the email you want to get the messages and the thank you message\n\n## Usage ##\n\nWith this module you'll get a simple Contact form tied to a dashboard that shows you the latest unanswered questions on your site, you can check the questions and even answer them directly from the cms without using your email program, you just write your answer in the module and it will send a branded email to your customer.\n\nYou also have a nice admin to review all the questions other clients have asked historically.\n\nI hope you enjoy it. [Like us in facebook for more exiting modules!](https://www.facebook.com/dospuntocero.cl)\n\n## TODO ##\nNicer email templates\n\n## Maintainer ##\nfrancisco arenas (fa@dospuntocero.cl)\n\n## Amazing coders ##\n\nUncleCheese - without him this module wont exists\n\u00ba\n## License ##\nDo what you want with it, I'm happy that you can use this small module, and if you use it, just let me know and if you can improve it, just sent me a pull request :)\n\n# The images #\n\n## Empty dashboard ##\n![module in action](http://proposals.god.cl/dospuntoceroCMS_-_Panel_de_control-20121006-013507.jpg \"nothing to answer\")\n\n## Dashboard with questions ##\n![module in action](http://proposals.god.cl/dospuntoceroCMS_-_Panel_de_control-20121006-014032.jpg \"some questions\")\n\n## Answering Questions ##\n![module in action](http://proposals.god.cl/dospuntoceroCMS_-_Customer_Questions-20121006-014349.jpg \"answering questions\")\n"}, {"repo": "skyverge/woocommerce-new-customer-report", "language": "PHP", "readme_contents": "## WooCommerce New Customer Report\n\nThis plugin adds a new report under **WooCommerce &gt; Reports &gt; Customers** to track new vs returning customers for a given date range. This lets you more accurately count metrics for new customers such as **customer acquisition cost**.\n\nThe plugin is translation-ready, and the text domain is: `woocommerce-new-customer-report`\n\n### How are New vs. Returning customers calculated?\n\nIf a customer has made a purchase prior to the date range start, s/he is counted as a returning customer, or a new customer if not.\n\nThis is also calculated on a \"per group\" basis (the bars in the bar graph). For example, if looking at a 7 day view, let's say this has happened:\n\n - Customer A purchased Monday for the first time\n - Customer B purchased Tuesday for the 5th time\n - Customer A purchased Friday again for the 2nd time\n \nThe report will show you **one** new customer (on Monday), and **two** returning customers (one Tuesday, one Friday), as customer A will be counted as both a new and returning customer.\n\nNew customer count for a time period will always show the number of people who have made a **first purchase** in this time period accurately, while returning customer count may include new customers who also come back to make a repeat purchase.\n\n### Contributing\n\nWe're happy to accept contributions! Feel free to add an issue or submit a PR :) Please follow WordPress code standards in your commits.\n\n### Helpful Links\n\n - The [screenshots](/skyverge/woocommerce-new-customer-report/tree/master/screenshots) will show you the plugin in action.\n - Found it useful? We love hearing feedback, and we always [appreciate donations](https://www.paypal.com/cgi-bin/webscr?cmd=_xclick&business=paypal@skyverge.com&item_name=Donation+for+WooCommerce+New+Customer+report) to fund more free development.\n\n### Changelog\n\n**2017.03.27 - version 1.1.0**\n * Feature: Adds support for the [GitHub updater plugin](https://github.com/afragen/github-updater)\n * Misc: Added support for WooCommerce 3.0\n * Misc: Removed support for WooCommerce 2.3\n \n**2016.03.30 - version 1.0.0**\n * Initial Release\n \n### License\n\nThis plugin is licensed under the GPL v3: [GNU General Public License v3.0](http://www.gnu.org/licenses/gpl-3.0.html)\n"}, {"repo": "customerio/customerio-php", "language": "PHP", "readme_contents": "Examples for using Customer.io with PHP\n"}, {"repo": "t-varun/Customer-Analytics", "language": null, "readme_contents": "# Customer-Analytics\n\nCoursera - https://www.coursera.org/learn/wharton-customer-analytics\n\nThis is a Coursera course on Customer analytics.\nYou can get best score as much as possible.\nThis is first phase of the course.\n\n\nIn the above document the answers for the questions in the course are available.\n\n\n#Answers at:\nhttps://github.com/princetvarun/Customer-Analytics/blob/master/Answers\n"}, {"repo": "dbashyal/Magento-Customer-Attributes", "language": "PHP", "readme_contents": "Magento-Customer-Attributes\n===========================\n\n[![Flattr this git repo](http://api.flattr.com/button/flattr-badge-large.png)](https://flattr.com/submit/auto?user_id=dbashyal&url=https://github.com/dbashyal&title=Github Repos&language=&tags=github&category=software)\n\nAdd new magento customer attributes\n\nadded new attribute: customer_referral\n```php\n<?php\n$customer_referral = array();\n$attribute = Mage::getModel('eav/config')->getAttribute('customer', 'customer_referral');\nforeach ( $attribute->getSource()->getAllOptions(true, true) as $option){\n    $customer_referral[$option['value']] = $option['label'];\n}\n?>\n```\napp/design/frontend/base/default/template/customer/form/edit.phtml\n```php\n<li>\n\t<label for=\"customer_referral\"><?php echo $this->__('Where did you hear about us?') ?></label><br/>\n\t<div class=\"input-box\">\n\t\t<select name=\"customer_referral\" id=\"customer_referral\" title=\"<?php echo $this->__('Where did you hear about us?') ?>\">\n\t\t\t<?php $selected = $this->htmlEscape($this->getCustomer()->getData('customer_referral')) ?>\n\t\t\t<?php foreach($customer_referral as $id => $label): ?>\n\t\t\t  <option value=\"<?php echo $id ?>\"<?php if($id == $selected): ?> selected=\"selected\"<?php endif; ?>><?php echo $this->htmlEscape($label) ?></option>\n\t\t\t<?php endforeach; ?>\n\t\t</select>\n\t</div>\n</li>\n\n```\n"}, {"repo": "Dashride/mongoose-stripe-customers", "language": "JavaScript", "readme_contents": "mongoose-stripe-customers\n====================\n\n[![Greenkeeper badge](https://badges.greenkeeper.io/Dashride/mongoose-stripe-customers.svg)](https://greenkeeper.io/)\n[![Build Status](https://travis-ci.org/Dashride/mongoose-stripe-customers.svg?branch=master)](https://travis-ci.org/Dashride/mongoose-stripe-customers)\n[![Coverage Status](https://coveralls.io/repos/Dashride/mongoose-stripe-customers/badge.svg?branch=master&service=github)](https://coveralls.io/github/Dashride/mongoose-stripe-customers?branch=master)\n[![Dependency Status](https://david-dm.org/Dashride/mongoose-stripe-customers.svg)](https://david-dm.org/Dashride/mongoose-stripe-customers)\n[![npm version](https://badge.fury.io/js/mongoose-stripe-customers.svg)](http://badge.fury.io/js/mongoose-stripe-customers)\n\nA [mongoose.js](https://github.com/LearnBoost/mongoose/) A mongoose plugin that creates a stripe customer when a new document is created and stores the [Stripe](https://stripe.com) customer ID to that document.\n\n## Use Case\nIf you are running an ecommerce type application using Stripe and create your own customer/user database objects, as well as Stripe customer objects, use this plugin to simultaneously aggregate your customer information to Stripe when a new customer is created within your database.\n\nThis plugin is configurable to allow enough flexibility to work with various schema setups. Just provide the names of your fields such as `email` or `phone` and the plugin will map those over to Stripe meta data objects.\n\n## Installation\n\n`npm install --save mongoose-stripe-customers`\n\n## API Reference\n**Example**  \n```js\nvar mongooseStripeCustomers = require('mongoose-stripe-customers');\n\nvar schema = Schema({...});\n\nschema.plugin(mongooseStripeCustomers, {\n    stripeApiKey: 'XXXXXXXXXXXXXXXX',\n    hook: 'save',\n    firstNameField: 'first_name',\n    lastNameField: 'last_name',\n    emailField: 'email',\n    metaData: [ '_id', 'phone', 'customerType' ]\n});\n ```\n<a name=\"module_mongoose-stripe-customers..options\"></a>\n### mongoose-stripe-customers~options\n**Kind**: inner property of <code>[mongoose-stripe-customers](#module_mongoose-stripe-customers)</code>  \n\n| Param | Type | Default | Description |\n| --- | --- | --- | --- |\n| options | <code>object</code> |  |  |\n| options.stripeApiKey | <code>string</code> |  | The Stripe secret key used to access the Stripe API. |\n| [options.hook] | <code>string</code> | <code>&quot;save&quot;</code> | The document hook you want this to run before. |\n| [options.fieldNames] | <code>object</code> |  | Response field overrides. |\n| [options.stripeCustomerIdField] | <code>string</code> | <code>&quot;stripe_customer_id&quot;</code> | The field in which you want the Stripe customer ID value to be stored. |\n| [options.firstNameField] | <code>string</code> |  | The field in which the customer's first name is stored. |\n| [options.lastNameField] | <code>string</code> |  | The field in which the customer's last name is stored. |\n| [options.emailField] | <code>string</code> |  | The field in which the customer's email address is stored. |\n| [options.metaData[]] | <code>Array.&lt;string&gt;</code> |  | If you want any extra data stored with the customer on Stripe, provide an array of field names. |\n\n"}, {"repo": "ouissa/Customer_Service_Messenger_Bot", "language": "JavaScript", "readme_contents": "<h1>Create a Customer Service Messenger Bot with wit.ai and the Facebook Marketing API</h1>\n<h3>Authors: Ouissal Moumou & Mohamed Moumou</h3>\n<hr>\n\n**This project was submitted to the 2020 Facebook Developer Circles Community Challenge (begineers track), and won the MENA region prize.**\n\n### Abstract\n\nThis tutorial will walk you through the steps of building a messenger chatbot for a t-shits company. The chatbot will answer different customer questions about availability, colors, sizes, and prices of t-shirts that are available in the company's Facebook catalog. The flowchart below explains the whole process.\n\n<p><img src=\"images/flowchart.png\" width=\"100%\"></p>\n<h6 align=\"center\">Figure 1. Hight architecture of the application</h6>\n\nYou can also watch this [walkthrough video](https://www.youtube.com/watch?v=i7cb0FveTfM) for a better understanding of the project.\n\n### Table of contents\n  1. [Prerequisites](#Prerequisites)\n  2. [What is wit.ai?](#What-is-witai)\n  3. [What is the Facebook Marketing API](#What-is-the-Facebook-Marketing-API)\n  4. [Getting Started: Creating a wit.ai App](#Getting-Started-Creating-a-witai-App)\n  5. [How does Wit.ai work?](#How-does-Witai-work)\n  6. [Training the bot](#Training-the-bot)\n  7. [Response of The Bot](#Response-of-The-Bot)\n  8. [Linking Messenger to the wit.ai app](#Linking-Messenger-to-the-witai-app)\n  9. [Handling intents](#Handling-intents)\n  10. [Next Steps](#Next-Steps)\n  11. [Resources](#Resources)\n  12. [License](#License)\n\n### Prerequisites\n\n  - You have a Facebook catalog for your products linked to a Facebook business account.\n  - You have basic knowledge of NodeJS.\n  - You have an E-commerce platform.\n\n\n### What is wit.ai?\n\n<blockquote>\"Wit.ai is an open and extensible NLP engine for developers, acquired by Facebook, which allows you to build conversational applications and devices that you can talk or text to. It provides an easy interface and quick learning APIs to understand human communication from every interaction and helps to parse the complex message (which can be either voice or text) into structured data. It also helps you with predicting the forthcoming set of events based on the learning from the gathered data.\"\n<br />\n\n#### References\nShetty, S. (2018, May 21). \n<i>Facebook\u2019s Wit.ai: Why we need yet another chatbot development framework?</i>. \npacktpub.https://hub.packtpub.com/facebooks-wit-ai-why-we-need-yet-another-chatbot-development-framework/.\n</blockquote>\n\n### What is the Facebook Marketing API\n\n The **Facebook Marketing API** is an HTTP-based API that you can use to programmatically query data, create and manage ads, and perform a wide variety of other tasks. \n\n In our case we will use it to query products from our catalog and specifically apply filters to query the right products depending on the customer request. For example, if the customer wants to know all the available black t-shirts with a price less than 100, we will send a GET request with a filter to only return the t-shirts that have the properties color as \u201cblack\u201d, and price as a number less than 100. \n\nTo use the Marketing API, you should have basic knowledge of the Facebook Grah API and how it works. You can find that [here](https://developers.facebook.com/docs/graph-api/).\n\n### Getting Started: Creating a wit.ai App\n\nBefore we can train the bot, we need to create a new Wit.ai app that will represent the natural language processing part of the project. \n  1. Go to https://wit.ai/ and login with Facebook or GitHub.\n\n<p><img src=\"images/witai_login.png\" width=\"100%\"></p>\n<h6 align=\"center\">Figure 2. Wit.ai login page</h6>\n\n  2. Click the new \"app\" button at the top right corner of the screen. \n  3. Choose a name for your app and also the language you will use to train the bot. \n  4. Click \"create\".\n\n<p align=\"center\"><img src=\"images/create_app.gif\" width=\"80%\"></p>\n<h6 align=\"center\">Figure 3. Create an app using Wit.ai</h6>\n\nThe first thing you will see after creating the app is a text area that says, \u201ctype your utterance\u201d. In this area we will type the question that will train the bot. On the left side, you can see a menu that contains the pieces of our app, e.g. under \"entities\", you will find the list of entities that you created. \n\n### How does Wit.ai work?\n\nWit.ai training process depends on three key elements: \n\n- **Utterances**: the questions or the statements that a user usually asks. Utterances represent the user's end. It can be an action that a user wants to do, or a piece of information that the user wants to know. \n\n  ![alt text](images/utterances.png)\n\n  <h6 align=\"center\">Figure 4. Utterances in Wit.ai</h6>\n\n- **Intents**: an intent in wit.ai represents the purpose of asking a question, e.g. the question \u201cwhat is the price of a blue Facebook t-shirt\u201d is of intent \"asking for price\". \n\n  ![alt text](images/intents.png)\n\n  <h6 align=\"center\">Figure 5. Intents in Wit.ai</h6>\n\n- **Entities**: constitute a very important part of the training process. Simply defined, entities are keywords in utterances that help identify the intent of the question. In the previous example, \u201cwhat is the price\u201d can be of entity \"price\". It also helps detect the intent of the question that is \"asking for price\"\n\n  ![alt text](images/entities.png)\n  \n  <h6 align=\"center\">Figure 6. Entities in Wit.ai</h6>\n\n### Training the bot\n\nTo train the bot we need to get it familiarized with the questions that users might ask. \n\nUnder \"understanding\", we can type the questions that users might ask in the text area that says type your utterance. To specify the intent for the question, you can add it in the intent text area under utterance. Note that after training the bot with some questions, the intent area will be filled automatically. Therefore, you need to check if the prediction made by Wit.ai is correct. Under the \"intent\" attribute, you can find the list of entities. To add an entity to the list you need to highlight the keyword that expresses the entity and either select the appropriate entity from the list of entities previously created or add one if it does not exist. As intents, along the way entities will be detected automatically by Wit.ai. After finishing these steps, you can click on \"train\" and \"validate\". \n\nIn this tutorial, we will introduce the case of a t-shirts company that wants to create a chatbot to provide the best support for its customers. \n\nThe bot in this tutorial will deal with three intents that the customer might include in his/her question. \n\n- Asking for availability \n\n- Asking for price \n\n- Asking for methods of payments \n\n####  Asking for availability: \n\nLet's take the following utterance as an example: \u201care pink long sleeve female t-shirts available?\u201d \n\nFor this utterance we can distinguish between four entities: \n\n- **t-shirt color**: in the example above, we have the word pink representing the entity t-shirt color. Wit.ai also gives us the ability to add other possible keywords to any entity. In this case, we would add other colors that customers might ask about. \n\n- **Sleeves types**: it can be long sleeves, short sleeves, sleeveless, etc. \n\n- **Gender**: in the example, we have female as the keyword representing the entity gender. We can have other keywords such as male, men, women, unisex, etc. \n\n- **Availability**: what we can say about the question above is that the word available determines the intent of the question asking for availability. Therefore, available is among the keywords possible for the availability entity. \n\nTo ensure the best performance of the bot, the more the utterances the better. For the \u201casking for availability\u201d intent, we can have other utterances such as: \n\n- What are the available options for XL t-shirts? (**Entities**: availability: \u201cavailable\u201d, t-shirt_size: \u201cXL\u201d). \n\n- Are there any available t-shirts for less than 70 MAD? (**Entities**: availability: \u201cavailable\u201d, wit/amount_of_money: \u201cless than 70 MAD\u201d). \n\n- Do you sell t-shirts for kids? (**Entities**: availability: \u201cdo you sell\u201d, group_age: \u201ckids\u201d). \n\nAs you can notice in the second example, wit comes with some predefined entities such as wit/amount_of_money. These entities come very handful in cases such as prices, distances, or measurable quantities as they provide us with precise detection of the information needed. \n\n<p align=\"center\"><img src=\"images/asking_for_avialabilty_utterance.gif\" width=\"100%\"></p>\n<h6 align=\"center\">Figure 7. example of asking for avilability intent</h6>\n\n#### Asking for price: \n\nFor this intent we can have questions similar to the following: \n\n-  What is the price of a blue Facebook t-shirt? (**Entities**: asking_price: \u201cwhat is the price\u201d, t-shirt_title: \u201cblue Facebook t-shirt\u201d). \n\n- How much a blue Facebook t-shirt cost? (**Entities**: asking_price: \u201chow much\u201d, t-shirt_title: \u201cblue Facebook t-shirt\u201d\n\n<p align=\"center\"><img src=\"images/asking_for_price_utterance.gif\" width=\"100%\"></p>\n<h6 align=\"center\">Figure 8. example of asking for price intent</h6>\n\n#### Asking for methods of payment: \n\nFor this intent we can have question similar to the following: \n\n- Can I pay with a visa card? (**Entities**: payment: \u201cCan I pay\u201d, method_of_payment: \u201cvisa card\u201d); \n\n- What are the methods of payment? (**Entities**: payment: \u201cwhat are the methods of payment\u201d); \n\n- Can I pay for a blue Facebook t-shirt using a master card? (**Entities**: payment: \u201cCan I pay\u201d, method_of_payment: \u201cmaster card\u201d, t-shirt_title: \u201cblue Facebook t-shirt\u201d); \n\n<p align=\"center\"><img src=\"images/asking_for_methods_of_payment_utterance.gif\" width=\"100%\"></p>\n<h6 align=\"center\">Figure 9. example of asking for methods of payment intent</h6>\n\n### Response of The Bot\n\nNow You might be asking: if the bot can understand the sentences communicated to it, and match them to an intent, how can it determine what to do after? how can it determine what to answer?  \n\nTo do that we will have to create an API that will link the wit.ai app (containing our training of the intents, entities...), with Messenger. The API is also the place where we will have our functions for all the intents.\n\nThis is how everything happens:  \n\n1. A user asks a question in Messenger \n2. The node application listens and receives the message \n3. Wit.ai detects the intent of the message \n4. The application matches then intent with the appropriate function and returns the appropriate message \n5. The returned answer is sent back to the chat \n\n### Linking Messenger to the wit.ai app\n\nTo do that, make sure to check out this [video](https://vimeo.com/427865122) made by Facebook. It shows how to link Messenger with wit.ai using a simple node server hosted on [Glitch](https://glitch.com/).\n\n### Handling intents\n\nThe following in this tutorial is based on the same code by Facebook used in the video. Here is the link to the [code](https://glitch.com/edit/#!/cotton-mysterious-package), you can remix it and start coding your own methods for your intents.\n\nNow that you have watched the video and filled your environment variables properly, we are interested in the wit_handler.js  file. It is where handling the intents takes place.\n\nAs you may have noticed, the function **responseFromWit(data)**  takes the data received from messenger and wit, and use it to extract the intent of the message sent. The \"data\" parameter is an object containing the text of the message, entities, intents, and traits. This  method is interested in the intents, so it takes the intent of the data it receives: \n\n```javascript\n const intent = (data.intents.length > 0 && data.intents[0]) || \"__foo__\";\n```\nThen we use \"switch\" to call the appropriate function for each detected intent. In case no intent is detected, we have a function called **handleGibberish()** that takes care of that. In this function, we can for example render the most sold items.\n\nSumming up, here is what **responseFromWit(data)** looks like:\n\n```javascript\nfunction responseFromWit(data) {\n  console.log(\"data from wit:\");\n  console.log(JSON.stringify(data));\n\n  const intent = (data.intents.length > 0 && data.intents[0]) || \"__foo__\";\n\n  switch (intent.name) {\n    case \"asking_for_price\":\n      return askingForPrice(data);\n    case \"asking_for_availability\":\n        return askingForAvailability(data)\n    case \"asking_for_payment\":\n      return askingForPayment(data)\n  }\n\n  return handleGibberish();\n}\n```\n\nNow let's look closely at an \"intent\" function. We will take the intent of asking for the price as an example.\n\n#### Asking for Price Intent\n\nFor this intent, we need to take the name of an item, which a t-shirt in our simple catalog and return its price. For example, if the customer asks \u201cwhat is the price of a Facebook t-shirt\u201d, we are interested in the t-shirt \"title\" entity that holds the value \u201cFacebook t-shirt\u201d to use it for the search. To get that, we use the \u201centities\u201d key of the data object, and get the first value since this example is simple and we know there is only going to be one value:\n\n```javascript\nasync function askingForPrice(data) {\n  const shirt_title = data.entities[\"t-shirt_title:t-shirt_title\"];\n```\nReturning the right products is all left now. Here is where the Facebook Marketing API (particularly the products edge) comes to play. Visit the [Facebook documentation](https://developers.facebook.com/docs/marketing-api/reference/product-catalog/products/) for more details. \n\nTo use the API, we need 2 things: \n\n- A **catalog ID** that you can obtain from the link of the catalog \n\n- A **Facebook Access Token** that you can get using different ways. An access token is a way to authenticate and have access to the resources available through the API. There are different types of access tokens: short-living access tokens that last about 1 hour, long-living access tokens that last about 3 months, and never-expiring access tokens. More on the tokens and how to get them in this [medium](https://medium.com/@blienart/get-a-permanent-facebook-page-access-token-a96470dc03ca#:~:text=To get this token%2C go,Graph API Explorer of Facebook.&text=Then%2C in the User or,User Token\" (2).&text=Finally click on \"Get Access,us for the next step.) article.\n\nNow that we have all that we need, we must send the requests to this endpoint of the Facebook Marketing API: https://graph.facebook.com/CATALOG_ID/products. We must apply filters using request parameters to get the appropriate products. To filter products, we must use some filtering rules given by Facebook available [here](https://developers.facebook.com/docs/marketing-api/reference/product-set#Creating).\n\nIt is to note that Facebook has a tool called [Graph Explorer](https://developers.facebook.com/tools/explorer/) that allows one to send requests in a very fast way and test their responses. \n\nIn our request we will have has 3 parameters: \n\n- access token \n\n- fields: these are the item (or t-shirt) properties that we will use either for the filters or for the information returned. In our case, we want the price of the product to be returned, so we will include the \"price\u201d in the fields. Also, we want to filter using the name of the product communicated by the customer, so we will include \"name\" in the fields too. \n\n- filter: this will contain the rule of filtering, in our case we want the product that contains the name that the customer entered. So we will use the \u201ci_contains\u201d rule. Note that the \u201ci\u201d, means that it is case insensitive. If you want it to be case sensitive, you can use \"contains\". The filter is a JSON that has the following format: \n\n```json\nfilter: { \n    Name_of_property: { \n        \"filtering_rule\": value \n    } \n}\n```\nFor the \"asking_for_availability\" intent, we can have the following request: \n\n```javascript\nif(price != null && shirt_title != null){\n const response = await axios.get(`https://graph.facebook.com/${catalog_id}/products`, {\n    params: {\n        access_token,\n        fields: [\"name\", \"price\"],\n        filter: {\n            name: {\n              \"i_contains\": shirt_title[0].body\n            }\n         }\n    }\n  })\n }\n```\nThe response will include an array called data that includes all the products that satisfy the condition: \n```json\n  \"data\": [\n    {\n      \"name\": \"Facebook T-shirt\",\n      \"price\": \"\u062f.\u0645.100.00\",\n      \"id\": \"3982464375100270\"\n    }\n  ]\n}\n```\n\nNow our function for asking_for_price intent is ready, here is how it looks in the chat:\n\n<p align=\"center\"><img src=\"images/demo.gif\" width=\"50%\"></p>\n<h6 align=\"center\">Figure 10. Demo of the application</h6>\n\n#### Asking for Availability Intent\n\nSometimes, we will need to combine many rules. For example, in the case of asking for the availability of a product, a customer might ask about a t-shirt that has a specific price for a specific gender. In this case, we want to have many rules grouped together. Luckily, we have the \u201cor\u201d and \u201dand\u201d operators. So for this specific case, we will have to select all the products that have the price AND the gender asked by the user. \n\nAnother issue is that the user is asking for a product, the best thing we can do is return the link of the product so that we redirect traffic to the web app of the company. Yet, make sure that on the page of your products, you have Facebook tags added to the header, so that Facebook gets the picture and the right title included with the URL of the item. More information on this [here](https://developers.facebook.com/docs/sharing/webmasters#markup).\n\nHere is the request that we will send in the case of asking for availability:\n\n```javascript\n      const response = await axios.get(\n      `https://graph.facebook.com/${catalog_id}/products`,\n        {\n          params: {\n            access_token,\n            fields: [\"url\", \"gender\", \"color\"],\n            filter: {\n              and: [\n                {\n                  color: {\n                    i_contains: shirt_color[0].body\n                  }\n                },\n                {\n                  gender: {\n                    eq: gender[0].body\n                  }\n                }\n              ]\n            }\n          }\n        }\n      );\n```\n\n### Next Steps\n\nWhat we have built was a simple application that only answers some intents with limited choices. We can add other features like:\n\n- More Rules in the requests for complicated entries.\n- Returning ranges of prices instead of a static price.\n- Using regular expressions for more flexibility with product names.\n- Suggesting similar products in case of no matching criteria with the user's questions.\n\n\n### Resources\n\n[wit.ai Documentation](https://wit.ai/docs)\n\n[Marketing API (products edge) Documentation](https://developers.facebook.com/docs/marketing-api/reference/product-catalog/products/)\n\n[Facebook Tutorial on how to build a messenger bot and link it with wit.ai](https://vimeo.com/427865122)\n\n### License\n\nThis repository is licensed under MIT license.\n"}, {"repo": "Vetrivel-PS/Janatahack-Customer-Segmentation", "language": "Jupyter Notebook", "readme_contents": "#### Best PUBLIC Score 0.94952380952381 gave Best PRIVATE Score 0.95117311\n#### Public LeaderBoard Rank - 44 | Private LeaderBoard Rank - 6 | Moved Up 38 Places - Happy to build a Generalised Model :-)\n\n#### Friends Interested in Hackathons can contact me at - https://www.linkedin.com/in/%E2%99%94-vetrivel-ps-%E2%99%94-456b3b73/ | Follow me for Kaggle Notebooks - https://www.kaggle.com/vetrirah/notebooks\n\n#### Code is an update of Pulkit's Code - https://www.linkedin.com/in/pulkitmehta1985/ - He is Kaggle Expert you can follow him here - https://www.kaggle.com/pulkitmehtawork1985\n\n# Janatahack-Customer-Segmentation\nCustomer segmentation is the practice of dividing a customer base into groups of individuals that are similar in specific ways relevant to marketing, such as age, gender, interests and spending habits.  Companies employing customer segmentation operate under the fact that every customer is different and that their marketing efforts would be better served if they target specific, smaller groups with messages that those consumers would find relevant and lead them to buy something. Companies also hope to gain a deeper understanding of their customers' preferences and needs with the idea of discovering what each segment finds most valuable to more accurately tailor marketing materials toward that segment.  This weekend, we are back with another Janatahack, this time dealing in a problem statement on customer segmentation. Stay tuned and make the maximum out of this learning opportunity.\n\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market. \n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n\nYou are required to help the manager to predict the right group of the new customers.\n"}, {"repo": "salinaaaaaa/customer-lifetime-value-contractual-or-non-contractual-relationship", "language": "Jupyter Notebook", "readme_contents": "# Customer lifetime value analysis for a contract and non-contract based business\n\nUnfortunately, many businesses calculate CLV using historical customer behavior without accounting for variation in that behavior. For example, a customer who buys several expensive items at once might be assigned a higher value than a customer who consistently buys moderately priced items \u2014 even if he or she never buys from the business again after the initial purchase. \n\nThere are two classes of business contexts that influence how a data scientist should go about modeling customer lifetime value: \n\n1. Whether purchases are discrete or continuous.\n\n2. Whether the setting is contractual or non-contractual. \n\nDiscrete purchases occur at fixed periods or frequencies, whereas continuous purchases occur at any time. <strong>Whether purchases are contractual or not determines whether customer churn is visible or must be inferred;</strong> This example is for non-contractual, but you can hook up <strong>AutoML for Customer Churn</strong> project, [here](https://github.com/joehoeller/AutoML-with-Genetic-Algorthims-to-Accurately-Predict-Customer-Churn), for a contractual business.\n\nCLV models can provide lots of actionable information, like the probability that a customer will churn or a population-level prediction of how many orders customers will be placing at a given time. These insights are critical for data-driven retention measures and sales forecasts, respectively.\n\nAdditional reading:\n\n* [Modelling customer lifetime value in contractual settings](https://pdfs.semanticscholar.org/0d02/c0faa1ada84a1a67a5dc134826b453394966.pdf)\n\n* [\"What's a Customer Worth?\" (non-contractual setting), by Susan Li](https://towardsdatascience.com/whats-a-customer-worth-8daf183f8a4f)\n\n* [Gamma Gamma Model](http://kpei.me/blog/?p=921)\n\n\n## How to set up the project\n#### TODO: Turn this into a Docker container\n\n```\nPandas:           $ sudo pip install pandas\nnumpy:            $ sudo pip install numpy\nscipy:            $ sudo pip install scipy\nmatplotlib: \n                  $ sudo apt-get install libfreetype6-dev libpng-dev\n                  $ sudo pip install matplotlib \nseaborn:          $ sudo pip install seaborn\njupyter notebook: $ sudo apt-get -y install ipython ipython-notebook\n                  $ sudo -H pip install jupyter\nlifetimes:        $ sudo pip install lifetimes\n\n```\n\n## Dataset\n\n* Data set can be download from this [link](http://archive.ics.uci.edu/ml/datasets/online+retail)\n* There is no need to download dataset because it is already downloaded. \n* Path of dataset is `./input_data/`\n\n## Usage\nRun the code given in ipython notebook `customer_lifetime_value.ipynb`\n\n## Credit\nBoilerplate code credits for this code go to [Susan Li](https://github.com/susanli2016), I just made it more accessible to a contractual lifetime value analysis which can be coupled with my [AutoML for Customer Churn](https://github.com/joehoeller/AutoML-with-Genetic-Algorthims-to-Accurately-Predict-Customer-Churn) project.\n\n\n\n"}, {"repo": "ouyangfeng/CustomerViewGroup", "language": "Java", "readme_contents": "# CustomerViewGroup\n\u6dd8\u5b9d\u5546\u54c1\u8be6\u60c5\u9875\u6539\u8fdb\n\n\u4e0a\u4e0b\u4e24\u90e8\u5206\u5c55\u793a\u533a\u57df\uff0c\u4e0a\u90e8\u5206\u53ef\u4e0a\u4e0b\u6ed1\u52a8\uff0c\u4e0b\u90e8\u5206\u53ef\u5de6\u53f3\u6ed1\u52a8\u5207\u6362\u53ca\u4e0a\u4e0b\u6ed1\u52a8\n"}, {"repo": "afaq-ahmad/Gender-and-Age-Detection-based-Customer-Tracking-Classification-on-Raspberry-pi", "language": "Python", "readme_contents": "# Gender-and-Age-Detection-based-Customer-Tracking-Classification\nThis algorithm tracks a person entering from a particular direction while his/her gender &amp; age group are predicted &amp; stored with the respective time of detection.\n\nDownload weights file: [Download Link](https://www.dropbox.com/sh/gzo6mz3ri7gwgmq/AADgFxJpPCTPUOlgDLdIe66ra?dl=0)\n\nIdentifying the data of potential customers passing by a specific spot in a market can help an investor in choosing the right business to be started on that spot. Mainly two main parameters that are very important in this regard are gender ratio & age parity of the people that pass by a specific spot. By keeping that in mind I made an algorithm in which I trained models for:\n\u2022\tFace detection\n\u2022\tAge detection\n\u2022\tGender detection\nI then deployed these models on the frames of a camera feed that can be subjected to a spot suitable for a above mentioned application. I then saved that data gathered in an Excel-Database so that can be used later. As the data-sets used to train age group consisted of cropped faces with facing towards the camera so this constraint will catered for in future when it will have enough labeled data from the specified orientation.\n\n\n\nThis project is divided into two parts which are as follows:\n\n    \u2022\tTraining Age/Gender detection model.\n    \u2022\tImplementing the model for extracting predictions.\n\n\n## Model Training:\n\n### Face Detection:\nBefore moving to the main objective, the first task was to detect a face in our video to pinpoint the points as to where we will apply our predicted models. Otherwise it will increase the computations many folds. \n    \n    So, as there are many methods available for detecting face. The following methods were tested so that the best one for our application can be used:\n    \n    1.\tHaar Cascade Face Detector in OpenCV\n    2.\tDeep Learning based Face Detector in OpenCV\n    3.\tHoG Face Detector in Dlib\n    4.\tDeep Learning based Face Detector in Dlib\n\nThe second one performed better in our testing phase as the others were skipping a lot of faces. The second model was included in OpenCV from version 3.3. It is based on Single-Shot-Multibox detector and uses ResNet-10 Architecture as backbone. The model was trained using images available from the web, but the source is not disclosed. OpenCV provides 2 models for this face detector.\n\n    \u2022\tFloating point 16 version of the original Caffe implementation (5.4 MB)\n    \u2022\t8-bit quantized version using TensorFlow (2.7 MB)\n\n\nWe used the floating point 16 version file as it was more powerful in terms of accuracy as the people in our application will be in motion. The method has the following merits :\n\n    \u2022\tMost accurate out of the four methods\n    \u2022\tRuns at real-time on CPU\n    \u2022\tWorks for different face orientations \u2013 up, down, left, right, side-face etc.\n    \u2022\tWorks even under substantial occlusion\n    \u2022\tDetects faces across various scales (detects big as well as tiny faces)\n\nThe DNN based detector overcomes all the drawbacks of Haar cascade-based detector, without compromising on any benefit provided by Haar. We could not see any major drawback for this method except that it is slower than the Dlib HoG based Face Detector.\nI tried to evaluate the 4 models using the FDDB dataset using the script used for evaluating the OpenCV-DNN model. However, I found surprising results. Dlib had worse numbers than Haar, although visually dlib outputs look much better. Given below are the Precision scores for the 4 methods.\n\n![Figure 1: Score vs Metrics](Redme_Images/score_vs_metrics.jpg)\n\n\nWhere,\n\n    \u2022\tAP_50 = Precision when overlap between Ground Truth and predicted bounding box is at least 50% (IoU = 50%)\n    \u2022\tAP_75 = Precision when overlap between Ground Truth and predicted bounding box is at least 75% (IoU = 75%)\n    \u2022\tAP_Small = Average Precision for small size faces (Average of IoU = 50% to 95%)\n    \u2022\tAP_medium = Average Precision for medium size faces (Average of IoU = 50% to 95%)\n    \u2022\tAP_Large = Average Precision for large size faces (Average of IoU = 50% to 95%)\n    \u2022\tmAP = Average precision across different IoU (Average of IoU = 50% to 95%)\n    \n    \nDlib is faster than the model we used but the major issue with dlib is its\u2019s inability to detect small faces which further drags down the numbers.\nI used a 300\u00d7300 image for the comparison of the methods. The MMOD detector can be run on a GPU, but the support for NVIDIA GPUs in OpenCV is still not there. So, we evaluate the methods on CPU only and report result for MMOD on GPU as well as CPU.\n\nHardware used:\n\n      \u2022\tProcessor: Intel Core i7 6850K \u2013 6 Core\n      \u2022\tRAM: 32 GB\n      \u2022\tGPU: NVIDIA GTX 1080 Ti with 11 GB RAM\n      \u2022\tOS: Linux 16.04 LTS\n      \u2022\tProgramming Language: Python\n\nWe run each method 10000 times on the given image and take 10 such iterations and average the time taken. Given below are the results:\n\n![Figure 2: Speed comparison](Redme_Images/speed_comparison.jpg)\n\n\nIn our application the basic issue was off the non-frontal faces as they can be described as looking towards right, left, up, down. Again, to be fair with dlib, we make sure the face size is more than 80\u00d780. Given below are some examples.\n\n![Figure 3 Comparison of different models](Redme_Images/Comparison_of_different_models.jpg)\n\n\n### Age & Gender model training:\nThe network architecture that we used throughout our experiments for both age and gender classification is illustrated in Figure 4.\n\n![Figure 4 Network Structure](Redme_Images/network_structure.jpg)\n\nThe network comprises of only three convolutional layers and two fully connected layers with a small number of neurons. Our choice of a smaller network design is motivated both from our desire to reduce the risk of overfitting as well as nature of the problems we are attempting to solve: age classification on the Adience set requires distinguishing between eight classes; gender only two.\n\n#### Model Description:\nAll three-color channels are processed directly by the network. Images are first rescaled to 256\u00d7256 and a crop of 227\u00d7227 is fed to the network. The three subsequent convolutional layers are then defined as follows:\n\n    1.\t96 filters of size 3\u00d77\u00d77 pixels are applied to the input in the first convolutional layer, followed by a rectified linear operator (ReLU), a max pooling layer taking the maximal value of 3\u00d73 regions with two-pixel strides and a local response normalization layer. \n    2.\tThe 96\u00d728\u00d728 output of the previous layer is then processed by the second convolutional layer, containing 256 filters of size 96\u00d75\u00d75 pixels. Again, this is followed by ReLU, a max pooling layer and a local response normalization layer with the same hyper parameters as before.\n    3.\tFinally, the third and last convolutional layer operates on the 256\u00d714\u00d714 blob by applying a set of 384 filters of size 256\u00d73\u00d73 pixels, followed by ReLU and a max pooling layer.\n    The following fully connected layers are then defined by: \n    4.\tA first fully connected layer that receives the output of the third convolutional layer and contains 512 neurons, followed by a ReLU and a dropout layer.\n    5.\tA second fully connected layer that receives the 512- dimensional output of the first fully connected layer and again contains 512 neurons, followed by a ReLU and a dropout layer.\n    6.\tA third, fully connected layer which maps to the final classes for age or gender. \n\nFinally, the output of the last fully connected layer is fed to a soft-max layer that assigns a probability for each class. The prediction itself is made by taking the class with the maximal probability for the given test image.\n\n#### Testing & Training:\n##### Initialization: \n\nThe weights in all layers are initialized with random values from a zero mean Gaussian with standard deviation of 0.01. To stress this, we do not use pre-trained models for initializing the network; the network is trained, from scratch, without using any data outside of the images and the labels available by the benchmark. This, again, should be compared with CNN implementations used for face recognition, where hundreds of thousands of images are used for training. Target values for training are represented as sparse, binary vectors corresponding to the ground truth classes. For each training image, the target, label vector is in the length of the number of classes (two for gender, eight for the eight age classes of the age classification task), containing 1 in the index of the ground truth and 0 elsewhere.\n\n##### Network training:\n\nAside from our use of a lean network architecture, we apply two additional methods to further limit the risk of overfitting. First, we apply dropout learning (i.e. randomly setting the output value of network neurons to zero). The network includes two dropout layers with a dropout ratio of 0.5 (50% chance of setting a neuron\u2019s output value to zero). Second, we use data augmentation by taking a random crop of 227 \u00d7 227 pixels from the 256 \u00d7 256 input image and randomly mirror it in each forward-backward training pass. Training itself is performed using stochastic gradient decent with image batch size of fifty images. The initial learning rate is e \u22123, reduced to e \u22124 after 10K iterations.\n\n##### Prediction:\n\nWe experimented with two methods of using the network in order to produce age and gender predictions for novel faces: \n\n    \u2022\tCenter Crop: Feeding the network with the face image, cropped to 227\u00d7227 around the face center. \n    \u2022\tOver-sampling: We extract five 227\u00d7227 pixel crop regions, four from the corners of the 256 \u00d7 256 face image, and an additional crop region from the center of the face. The network is presented with all five images, along with their horizontal reflections. Its final prediction is taken to be the average prediction value across all these variations. \n\nDue to the small misalignments in the Adience images, caused by the many challenges of these images (occlusions, motion blur, etc.) can have a noticeable impact on the quality of our results. This second, over-sampling method, is designed to compensate for these small misalignments, bypassing the need for improving alignment quality, but rather directly feeding the network with multiple translated versions of the same face.\n\n### Implementing the Models:\n\nThis algorithm is a multi-model DNN program that is optimized for low computation. This algorithm works in different little parts. The method involves:\n\n    \u2022\tFirstly, faces are detected in the frame using OpenCV\u2019s Deep Learning based Face Detector: res10_300x300_ssd_iter_140000.caffemodel.\n    \u2022\tSecondly, age and gender of every person is predicted also using caffe models that were trained previously.\n    \u2022\tEach person is then assigned an ID and tracked over time, even when they are out of the frame for not so long (5 seconds), whenever they come back in the frame, their ID will remain the same.\n    \u2022\tA picture of the person is then saved with respect to their ID number & the prediction is saved in the Excel Database.\n\n![Figure 5 Steps](Redme_Images/steps.jpg)\n\n## Requirements:\nThere is a requirement.txt file in the project files that can be used to install required libraries. The important libraries used in this algorithm are as follows:\n\n    OpenCV==3.4.1\n    openpyxl==2.6.2\n    xlrd==1.2.0\n    XlsxWriter==1.1.8\n    xlutils==2.0.0\n    xlwings==0.15.8\n    xlwt==1.3.0\n\nOpenCV must be installed manually while rest of the files can easily be downloaded by using this command: \n        \n        pip install --user --requirement requirements.txt\n\n## Implementation:\n\nTo implement the program user has to enter python Detect.py & it will run the program, to turn off the video stream you have to comment out line # 161.\n\n![Figure 6 Test](Redme_Images/test.jpg)\n\n\n## Explanation:\n\nThe code initially loads all the required models. It then initializes the video-stream & that stream is subjected to our first model that is face detection. After the face is detected the first step involves assigning an id number to this detected contour. After that the program waits for 0.5 seconds to fetch a face from a frame so that it is not at the border or not occluded. This face image is then converted to blob object of 277\u00d7277 which is then sent to the gender prediction & age prediction model serially. The results are then written in the excel file with the corresponding ID, date & time. The person is tracked in the frame until he leaves the frame & the other person enters. The output file is in this manner;\n\n![Figure 7 Excel Output](Redme_Images/save_output.jpg)\n\nWhereas the respective images are saved with variable sizes depending on the distance of person from camera.\n\n## Troubleshooting:\n\nFollowing issues were catered for while writing this algorithm:\n\n    \u2022\tThe main issue that I was stuck was in the beginning was with OpenCV, as it keeps on skipping frames due to which the assertion failed error was a biggest flaw & to overcome that I had to reduce frame rate to an optimal number.\n    \u2022\tI also came across the memory issue where due to using the age & gender models continuously on each frame was using much of the core & GPU resources of the system. So, to overcome that I only performed age & gender computation once on the face image.\n    \u2022\tAnother issue was tracking & retaining the same person in the frame. As it was detecting a face on each frame, as I was matching the faces with the previous one which was computationally expensive. I used centroid tracking by pyimagesearch that reduced that computation for me as it was retaining the tracked faces as centroid for 5 seconds at least.\n    \u2022\tThe models for age & gender were trained accordingly as explained in this paper: Age and Gender Classification Using Convolutional Neural Networks.\n"}, {"repo": "bandiang2/Customer-Churn", "language": "Jupyter Notebook", "readme_contents": "# Customer-Churn\nIn this project our goal is to predict the probability of a customer is likely to churn using machine learning techniques.\n"}, {"repo": "dwebdevcore/client_portal_customer_FE", "language": "HTML", "readme_contents": ""}, {"repo": "mesudepolat/Customer-Lifetime-Value-Prediction", "language": "Python", "readme_contents": "# Customer Lifetime Value Prediction\n\n## Project Steps\n\n* Connect to the database and extract data\n* Customer segmentation with RFM\n* Calculation Customer Lifetime Value in basic concept\n* Predicting Customer Lifetime Value with BG-NBD & GammaGamma models by adding the concept of time\n* Export tables and forecast outputs of all models to the database\n\n\n## Dataset Information\n\nThis Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011. The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n\nhttps://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n"}, {"repo": "anchorP34/Starbucks-Customer-Clusters", "language": "Jupyter Notebook", "readme_contents": "# Starbucks-Customer-Clusters\nExploratory data analysis of Starbucks customers and grouping them into clusters.\n\nYou can see my blogpost for this project here: https://medium.com/@paytonsoicher/what-type-of-starbucks-customer-are-you-72f70e45f65\n\n# Project Overview\nThe Starbucks project was my capstone project for the Udacity Data Scientist Nanodegree. This data was provided by Starbucks to simulate their customers and transactions to see if there are better approaches to sending customers specific promotional deals. \n\nMy motivation for this project is that I really enjoy unorthodox projects and I had never done any type of clustering project with scenario data before, so I really wanted to get out of my comfort zone for this one. I also enjoy finding information about businesses that would typically not be viewed as important and taking more of a statistical approach to solving a problem that wouldn't use traditional statistics. I love problem solving, and not having any definitive end result gave me the freedom to solve a problem any way that I could find out. \n\nThe different files that were used to create this project are located in the Profile Portfolio and Transcript Data.zip file. Here is a quick overview of each file:\n1. Profile - Dimensional data about each person, including their age, salary, and gender. There is one unique customer for each record.\n2. Portfolio - Information about the promotional offers that are possible to receive, and basic information about each one including the promotional type, duration of promotion, reward, and how the promotion was distributed to customers\n3. Transcript - Records show the different steps of promotional offers that a customer received. The different values of receiving a promotion are receiving, viewing, and completing. You also see the different transactions that a person made in the time since he became a customer. With all records, you see the day that they intereacted with Starbucks and the amount that it is worth.\n\nOne important segment of any business is to look for areas that can be improved to save money or create more revenue. I was determined to look to see how well certain promotions were being used by all customers and to try and see what was a more effective method to offering promotions. Once I could see how people normally reacted, the problem then became to find a better way to make sure that the right people were receiving the right promotions.\n\nI used exploratory analysis to see what type of customers existed in the Starbucks universe, their transaction / promotional engagement habits, and created a customer matrix dataframe that gae the whole view of what type of customer they were. The best machine learning approach was then to use a clustering method that could break up customers into different segments that would respond differently to different promotional deals.\n\nBy doing this, Starbucks can allocate their promotional deals in a more effective manner. People who come back on a regular basis dont need buy one get one free deals, but should receive discount deals. People who might not be able to afford coming into Starbucks stores at a constant basis or don't go regularly for their own reasons should be given BOGO deals to reengage their interests in Starbucks and make them a more reliable customer.\n\nMy results showed that Starbucks customers fall into 4 different categories based on mostly their income and the number of times they interact with Starbucks. There are 2 clusters that come consistantly to Starbucks that don't need to receive buy one get one free offers and should only receive discounted promotions. That way those customers still feel rewarded for being a customer, while Starbucks saves some money that they were giving away to them when they didn't have to. The other 2 clusters are groups of people who should be offered more buy one get one free promotions.  These customers might not be able to afford coming into Starbucks stores at a constant basis or don't go regularly for their own reasons should be given BOGO deals to reengage their interests in Starbucks and make them a more reliable customer.\n\n\n\n# Profile Portfolio and Transcript Data.zip\nThree JSON files that show profiles of customers, promotional deals that are offered, and the transaction history of customers.\n\nportfolio.json\n\n- id (string) - offer id\n- offer_type (string) - type of offer ie BOGO, discount, informational\n- difficulty (int) - minimum required spend to complete an offer\n- reward (int) - reward given for completing an offer\n- duration (int) - UNKNOWN\n- channels (list of strings)\n\nprofile.json\n\n- age (int) - age of the customer\n- became_member_on (int) - date when customer created an app account\n- gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n- id (str) - customer id\n- income (float) - customer's income\n\ntranscript.json\n\n- event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n- person (str) - customer id\n- time (int) - time in hours. The data begins at time t=0\n- value - (dict of strings) - either an offer id or transaction amount depending on the record\n\n\n# Starbucks Capstone Project.ipynb\nJupyter Notebook that shows the start to finish of the this project. This starts with data cleanup and exploration, top level statsitics of customers and their transactions, and cluster analysis for better approaches of sending customers promotions.\n\n\n\n\n"}, {"repo": "aishwarya-pawar/Bank-Customer-Churn-Analysis-", "language": "Jupyter Notebook", "readme_contents": "# Bank-Customer-Churn-Analysis-\n\n## Project Title:\n\nBank Customer - Churn Analysis\n\n## Motivation:\n\nWith increasing competition in the banking sector, most of the banks are facing issues with retaining their customers. An European Bank (Let's call it- 'We Love Your Money!') has recently observed that it's customers are churning at a higher rate year-over-year. They want to investigate the reasons driving the customer churn.\n\n\n## Data Source:\n\nhttps://www.kaggle.com/shrutimechlearn/churn-modelling\n\n\n## Analysis Steps:\n\n- Exploratory Data Analysis\n- Churn Analysis Using K-Nearest Neighbors model (KNN)\n- Churn Analysis Using Logistic Regression\n- Churn Analysis Using Decision Tree\n- Churn Analysis Using Random Forest\n\n\n## Findings:\n\n#### Findings from the exploratory data analysis:\n- Women are more likely to churn than men\n- German and older (by age) customers are more likely to churn\n- Larger proportion of churned customers have poor credit scores\n\n#### Predicting customers who are more likely to churn:\n\n- Out of the four prediction models, random forest predicted the churned customers with the highest accuracy of 86% (#trees= 1000)\n- Age, Estimated Salary, Credit Scores, Number of products turned out to be significant variables\n\n\n\n## Recommendations:\n\n- Increase awareness amongst customers on importance of maintaining a healthy credit score\n- Customized financial advisory and investment offerings to different segments of customers\n- Launch Digital products and diverse schemes to woo customers from different age brackets\n"}, {"repo": "serhatyazicioglu/Customer-Relationship-Management", "language": "Python", "readme_contents": "# Customer-Relationship-Management\n\nIn this project, I segmented each client separately for RFM, Computed CLTV, and Predicted CLTV and combined these values in a table.\n\nFor those who want to access the data set: https://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n"}, {"repo": "SooyeonWon/customer_analytics_fmcg", "language": "Jupyter Notebook", "readme_contents": "# Customer Analytics in FMCG Industry \n\n#### by Sooyeon Won \n\n### Keywords \n\n- Marketing Mix \n- STP framework \n- Customer Analytics\n- Segmentation and Clustering\n- Dimensionality Reduction with PCA\n- Data Visualisations \n- Purchase Analytics\n- Price Elasticity\n- Modeling Purchase Incidence\n- Modeling Brand Choice\n- Modeling Purchase Quantity\n\n\n### Introduction\nThis project is about customer analytics in Fast-Moving-Consumer-Goods (FMCG) Industry. The project is consisted of Customer Analytics and Purchased Analytics. It is motivated by Customer Analytics Program in Udemy. \n- Customer Analytics: The first part of analysis focuses on how to perform customer segmentation. It involves the application of hierarchical and flat clustering techniques for dividing customers into groups. It also features applying the Principal Components Analysis (PCA) to reduce the dimensionality of the problem, as well as combining PCA and K-means for customer segmentation.\n\n- Purchase Analytics: The second part of analysis explores both the descriptive and predictive analysis of the purchase behaviour of customers, including models for purchase incidence, brand choice, and purchase quantity. \n \n\n### Summary of Findings \n\n- **Segmentation:** It turns out that it is the most appropriate group all datapoints into four segments. Each segment has different characters : 'Well-Off', 'Fewer-Opportunities', 'Standard', 'Career_Focused'. \"Fewer-Opportunities\" segment is  the largest. Almost 40 percent of customers belong to this segment.\n- **Brand Preference:** Each segment has also unequal brand preference. Fewer-Opportunities segment shows an extremely strong preference for brand 2. Ca. 63 percent of the career focus segment buys brand 5 which is the most expensive brand. Well-Off segment enjoys one of the most luxurious brands but not the most expensive one. Brand 4. Standard segment is the most heterogeneous segment.  \n- **Revenue:** Brand 5 brought the most revenue. On the other hand Brand 3 has the lowest revenue, although it is not the cheapest. \"Career-Focused\" brings the most revenue.\n- **Price Elasticity:** The higher the price of our product becomes the less likely it will be for people to want to buy it. \"Fewer Opportunities\" segment is the most price sensitive compared to the average as well as other segments. \n- **Promotion:** People are more willing to buy products at promotional prices and they are a little bit more elastic when there is a promotion. \n\n\n### Referenecs \n\n[Price Elasticity](https://365datascience.com/price-elasticity/)\n[Customer Analytics](https://www.udemy.com/course/customer-analytics-in-python/)\n"}, {"repo": "shwetapai/Predicting-Customer-Churn-for-a-Telecom-Company", "language": "Jupyter Notebook", "readme_contents": "# Project 3: Using Supervised Machine Learning to Predict Customer Churn\n\n**Objective**\n\nCustomer churn occurs when customers stop doing business with a company.As the cost of retaining an existing customer is far less than acquiring a new one, maintaining a healthy customer base is important for the success of any business.As customers have multiple options in the telecom industry, the churn rate is particularly high in this industry.Individualized customer retention is difficult because businesses usually have a lot of customers and cannot afford to spend much time on one. The costs would be too high and would outweigh the extra revenue.But if a company could predict in advance which customers are at risk of leaving, they could focus customer retention efforts by directing them solely toward such  \"high risk\" customers.\n\nThe main objective of my project was to build a predictive model to generate a  prioritized list of customers most vulnerable to churn.\n\n**Method**\n\nI used the IBM telecom dataset available from [kaggle.com](https://www.kaggle.com/blastchar/telco-customer-churn).The raw data contained 7043 rows (customers) and 21 columns (features). The \u201cChurn\u201d column was my target variable.The classification goal was to predict whether the customer will churn(1/0).\n\nPredict variable (desired target): y\u200a\u2014\u200ahas the customer churned? (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo)\n\nI cleaned the dataset to remove the missing variables and hot encoded several categorical variables. I explored the  correlation between several features and the target variable.I then used a correlation matrix to explore if there was any correlation between different features.I included all features to predict the target variable (customer churn).\n\nMy target variable was unbalanced.So I balanced the target variable with SMOTE (Synthetic Minority Oversampling Technique). With my training data created, I up-sampled minority sample( in our case the 'yes_churn' (customers who churn) sample using the SMOTE algorithm. \n\n\n**Model**\n\nAfter testing several models, I fit a logistic regression model using all of the features.The AUC for the model was 0.84. I used recall (sensitivity) as my main metric.The sensitivity, which is a measure of the true positive rate (TP/(TP+FN)), was 79%.This tells us that the model has correctly identified 79% of customers that actually churned.\n\n**Cost Evaluation**\n\n I was interested in exploring the cost implications of implementing, vs. not implementing a predictive model.There were costs associated with the model erroneously assigning false positives and false negatives. It was important to look at similar costs associated with correct predictions of true positives and true negatives. Because the choice of the threshold affects all four of these statistics,it was important to consider the relative costs to the business for each of these four outcomes for each prediction.\n\nI  made the following cost assumptions to explore the cost implications of implementing the model.\n\n1. I assigned the true negatives the cost of USD 0. My model essentially correctly identified a happy customer in this case, and I don\u2019t need to do anything.\n\n2. False negatives were the most problematic, because they incorrectly predict that a churning customer will stay. I will lose the customer and will have to pay all the costs of acquiring a replacement customer, including foregone revenue, advertising costs, administrative costs, etc. I will assume USD 500. This is the cost of false negatives.\n\n3. Finally, for customers that my model identifies as churning, I will assume a retention incentive in the amount of USD 100.This is the cost of both true positive and false positive outcomes.In the case of false positives (the customer is happy,but the model mistakenly predicted churn),I will \u201cwaste\u201d the USD 100 concession.\n\nIt\u2019s clear that false negatives are substantially more costly than false positives. Instead of optimizing for total error,I selected a model that minimized a cost function ( basically minimized the number of false negatives)  that looks like this:\n\n**500\u2217FN(C)+0TN(C) + 100\u2217FP(C)+100TP(C)**\n\nC:Count\n\n\n**Conclusion**\n\nIn order to maintain their current customer base, using the current logistic regression model will save the company USD 135,800 in a month.So its worth investing in optimizing the model further to increase cost savings.I used a default threshold of 0.5 for my logistic regression model.if we lower the threshold further, the cost per customer decreases (from $148 to $90).This results in further cost savings.\n\n\n**Future Work**\n\nI want to optimize the sensitivity of the model (further decrease the number of false negatives).If I had more time, I would invest more in feature engineering and try and include more important features from other datasets.Fot future work, I also want identify the root causes as to why customers leave as it would help in fine tuning re-acquisition strategies.\n\n\n\n\n\n\n\n"}, {"repo": "FOHEART/U3D_Demo_MarketCustomer", "language": "C#", "readme_contents": "# FOHEART C1 Mocap Suit Unity 3D Demo -- MarketCustomer\n# Please refer Wiki pages of this project for more details\n![MainThumb](https://raw.githubusercontent.com/FOHEART/U3D_Demo_MarketCustomer/master/Assets/Thumbnail/MarketCustomer.png)\n\n======================EN===========================<br>\n\n##**[Version 1.3.0]**\nBeijing FOHEART Co.Ltd.<br>\n1. Unzip and load scene Assets\\MarketCustomer.unity.\n2. Put on [FOHEART C1](http://www.foheart.com/) mocap suit.\n3. Open MotionVenus and make sure all nodes are online.\n4. Do 2-step calibration.\n5. Done.\n\nCompatible with MotionVenus 1.3.0 or higher version<br>\nCompatible with Unity3D 5.6.1f1(64bit)<br>\nCompatible with Visual Studio Ultimate 2013<br>\n\n======================CN===========================<br>\n##**[Version 1.3.0]**\n\u5317\u4eac\u5b5a\u5fc3\u79d1\u6280\u6709\u9650\u516c\u53f8<br>\n1. \u4e0b\u8f7d\u5e76\u89e3\u538b\u4e0e\u60a8MotionVenus**\u7248\u672c\u53f7\u76f8\u540c**\u7684\u5206\u652f\u5de5\u7a0b\uff0c\u5e76\u6253\u5f00Assets\\MarketCustomer.unity\u573a\u666f\u3002\n2. \u7a7f\u6234\u597d[FOHEART C1](http://www.foheart.com/) \u52a8\u4f5c\u6355\u6349\u5957\u88c5\u3002\n3. \u6253\u5f00MotionVenus\u8f6f\u4ef6\u5e76\u786e\u8ba4\u6240\u6709\u52a8\u6355\u8282\u70b9\u5747\u5de5\u4f5c\u6b63\u5e38\u3002\n4. \u5b8c\u6210\u4e24\u4e2a\u52a8\u4f5c\u7684\u6821\u6b63\u6d41\u7a0b\u3002\n5. \u5b8c\u6210\u3002\n\n\u672c\u5de5\u7a0b\u4e0eMotionVenus v1.3.0\u53ca\u66f4\u9ad8\u7248\u672c\u517c\u5bb9\u3002<br>\n\u672c\u5de5\u7a0b\u4e0eUnity3D 5.6.1f1(64bit)\u517c\u5bb9\u3002<br>\n\u672c\u5de5\u7a0b\u4e0eVisual Studio Ultimate 2013\u517c\u5bb9\u3002<br>"}, {"repo": "latonaio/data-interface-for-salesforce-customer-bulk-get", "language": "Go", "readme_contents": "# data-interface-for-salesfore-customer-bulk-get  \ndata-interface-for-salesfore-customer-bulk-get \u306f\u3001salesforce \u306e\u9867\u5ba2\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u53d6\u5f97\u306b\u5fc5\u8981\u306a\u30c7\u30fc\u30bf\u306e\u6574\u5f62\u3001\u304a\u3088\u3073\u4f5c\u6210\u6642\u306b salesforce \u304b\u3089\u8fd4\u3063\u3066\u304d\u305f response \u306e MySQL \u3078\u306e\u683c\u7d0d\u3092\u884c\u3046\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u3067\u3059\u3002\n\n## \u52d5\u4f5c\u74b0\u5883  \ndata-interface-for-salesfore-customer-bulk-get\u306f\u3001aion-core\u306e\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u4e0a\u3067\u306e\u52d5\u4f5c\u3092\u524d\u63d0\u3068\u3057\u3066\u3044\u307e\u3059\u3002  \n\u4f7f\u7528\u3059\u308b\u969b\u306f\u3001\u4e8b\u524d\u306b\u4e0b\u8a18\u306e\u901a\u308aAION\u306e\u52d5\u4f5c\u74b0\u5883\u3092\u7528\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002  \n\n* OS: Linux OS   \n* CPU: ARM/AMD/Intel   \n* Kubernetes   \n* AION \u306e\u30ea\u30bd\u30fc\u30b9   \n\n## \u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n1. \u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3001docker image\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n```\n$ cd /path/to/data-interface-for-salesfore-customer-bulk-get\n$ make docker-build\n```\n\n2. \u672c\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u306f DB \u306b MySQL \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002MySQL \u306b\u95a2\u3059\u308b\u8a2d\u5b9a\u3092\u3001 `data-interface-for-salesfore-customer-bulk-get.yaml` \u306e\u74b0\u5883\u5909\u6570\u306b\u8a18\u8ff0\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n| env_name | description |\n| --- | --- |\n| MYSQL_HOST | \u30db\u30b9\u30c8\u540d |\n| MYSQL_PORT | \u30dd\u30fc\u30c8\u756a\u53f7 |\n| MYSQL_USER | \u30e6\u30fc\u30b6\u30fc\u540d |\n| MYSQL_PASSWORD | \u30d1\u30b9\u30ef\u30fc\u30c9 |\n| MYSQL_DBNAME | \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u540d |\n| MAX_OPEN_CONNECTION | \u6700\u5927\u30b3\u30cd\u30af\u30b7\u30e7\u30f3\u6570 |\n| MAX_IDLE_CONNECTION | \u30a2\u30a4\u30c9\u30eb\u72b6\u614b\u306e\u6700\u5927\u30b3\u30cd\u30af\u30b7\u30e7\u30f3\u6570 |\n| KANBANADDR: | kanban \u306e\u30a2\u30c9\u30ec\u30b9 |\n| TZ | \u30bf\u30a4\u30e0\u30be\u30fc\u30f3 |\n\n## \u8d77\u52d5\u65b9\u6cd5\n\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3001pod\u3092\u7acb\u3061\u4e0a\u3052\u3066\u304f\u3060\u3055\u3044\u3002\n```\n$ cd /path/to/data-interface-for-salesfore-customer-bulk-get\n$ kubectl apply -f data-interface-for-salesfore-customer-bulk-get.yaml\n```\n\n## kanban \u3068\u306e\u901a\u4fe1\n### kanban \u304b\u3089\u53d7\u4fe1\u3059\u308b\u30c7\u30fc\u30bf\nkanban \u304b\u3089\u53d7\u4fe1\u3059\u308b metadata \u306b\u4e0b\u8a18\u306e\u60c5\u5831\u3092\u542b\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n| key | value |\n| --- | --- |\n| connection_type | \u6587\u5b57\u5217 \"request\" |\n| method | \u6587\u5b57\u5217 \"get\" |\n| object | \u6587\u5b57\u5217 \"Account\" |\n| connection_type | request |\n\n\u5177\u4f53\u4f8b: \n```example\n# metadata (map[string]interface{}) \u306e\u4e2d\u8eab\n\n\"method\": \"get\"\n\"object\": \"Account\"\n\"connection_type\": \"request\"\n```\n\n### kanban \u306b\u9001\u4fe1\u3059\u308b\u30c7\u30fc\u30bf\nkanban \u306b\u9001\u4fe1\u3059\u308b metadata \u306f\u4e0b\u8a18\u306e\u60c5\u5831\u3092\u542b\u307f\u307e\u3059\u3002\n\n| key | type | description |\n| --- | --- | --- |\n| method | string | \u6587\u5b57\u5217 \"get\" \u3092\u6307\u5b9a |\n| object | string | \u6587\u5b57\u5217 \"AccountReratedList\" \u3092\u6307\u5b9a |\n\n\u5177\u4f53\u4f8b: \n```example\n# metadata (map[string]interface{}) \u306e\u4e2d\u8eab\n\n\"method\": \"get\"\n\"object\": \"AccountRelatedList\"\n\"connection_key\": \"customer_bulk_get\"\n```\n\n## kanban(salesforce-api-kube) \u304b\u3089\u53d7\u4fe1\u3059\u308b\u30c7\u30fc\u30bf\nkanban \u304b\u3089\u306e\u53d7\u4fe1\u53ef\u80fd\u30c7\u30fc\u30bf\u306f\u4e0b\u8a18\u306e\u5f62\u5f0f\u3067\u3059\n\n\n| key | value |\n| --- | --- |\n| key | \u6587\u5b57\u5217 \"AccountReratedList\" |\n| content | Accounr \u306e\u8a73\u7d30\u60c5\u5831\u3092\u542b\u3080 JSON \u914d\u5217|\n| connection_type | \u6587\u5b57\u5217 \"response\" |\n\n\u5177\u4f53\u4f8b:\n```example\n# metadata (map[string]interface{}) \u306e\u4e2d\u8eab\n\n\"key\": \"AccountRelatedList\"\n\"content\": \"[{xxxxxxxxxxxxxx}]\"\n\"connection_type\": \"response\"\n```\n\n\n"}, {"repo": "ozzael-codes/CustomerChurnAnalysis", "language": "Jupyter Notebook", "readme_contents": "# Customer Churn Analysis\n### This tutorial explains how to set up and run Jupyter Notebooks from within IBM\u00ae Watson\u2122 Studio. We start with a data set for customer churn that is available on Kaggle. \n\n1. Sign up/login into IBM Cloud: http://ibm.biz/JupyterML\n2. Link to follow the tutorial: https://developer.ibm.com/tutorials/watson-studio-using-jupyter-notebook/\n\nThe data set has a corresponding Customer Churn Analysis Jupyter Notebook (originally developed by Sandip Datta), which shows the archetypical steps in developing a machine learning model by going through the following essential steps:\n\n- Import the data set\n\n- Analyze the data by creating visualizations and inspecting basic statistic parameters (for example, mean or standard variation).\n\n- Prepare the data for machine model building (for example, by transforming categorical features into numeric features and by normalizing the data).\n\n- Split the data into training and test data to be used for model training and model validation.\n\n- Train the model by using various machine learning algorithms for binary classification.\n\n- Evaluate the various models for accuracy and precision using a confusion matrix.\n\n- Select the model that\u2019s the best fit for the given data set, and analyze which features have low and significant impact on the outcome of the prediction.\n\n\n"}, {"repo": "udacity/MLND_CN_P3_Customer_Segments", "language": "Jupyter Notebook", "readme_contents": "# \u9879\u76ee 3: \u975e\u76d1\u7763\u5b66\u4e60\n## \u521b\u5efa\u7528\u6237\u7ec6\u5206\n\n### \u5b89\u88c5\n\n\u8fd9\u4e2a\u9879\u76ee\u8981\u6c42\u4f7f\u7528 **Python 3** \u5e76\u4e14\u9700\u8981\u5b89\u88c5\u4e0b\u9762\u8fd9\u4e9bpython\u5305\uff1a\n\n- [NumPy](http\uff1a//www.numpy.org/)\n- [Pandas](http\uff1a//pandas.pydata.org)\n- [scikit-learn](http\uff1a//scikit-learn.org/stable/)\n\n\u4f60\u540c\u6837\u9700\u8981\u5b89\u88c5\u597d\u76f8\u5e94\u8f6f\u4ef6\u4f7f\u4e4b\u80fd\u591f\u8fd0\u884c[Jupyter Notebook](http://jupyter.org/)\u3002\n\n\u4f18\u8fbe\u5b66\u57ce\u63a8\u8350\u5b66\u751f\u5b89\u88c5 [Anaconda](https\uff1a//www.continuum.io/downloads), \u8fd9\u662f\u4e00\u4e2a\u5df2\u7ecf\u6253\u5305\u597d\u7684python\u53d1\u884c\u7248\uff0c\u5b83\u5305\u542b\u4e86\u6211\u4eec\u8fd9\u4e2a\u9879\u76ee\u9700\u8981\u7684\u6240\u6709\u7684\u5e93\u548c\u8f6f\u4ef6\u3002\n\n### \u4ee3\u7801\n\n\u521d\u59cb\u4ee3\u7801\u5305\u542b\u5728 `customer_segments.ipynb` \u8fd9\u4e2anotebook\u6587\u4ef6\u4e2d\u3002\u8fd9\u91cc\u9762\u6709\u4e00\u4e9b\u4ee3\u7801\u5df2\u7ecf\u5b9e\u73b0\u597d\u6765\u5e2e\u52a9\u4f60\u5f00\u59cb\u9879\u76ee\uff0c\u4f46\u662f\u4e3a\u4e86\u5b8c\u6210\u9879\u76ee\uff0c\u4f60\u8fd8\u9700\u8981\u5b9e\u73b0\u9644\u52a0\u7684\u529f\u80fd\u3002\n\n### \u8fd0\u884c\n\n\u5728\u547d\u4ee4\u884c\u4e2d\uff0c\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e3a `customer_segments.ipynb` \u6587\u4ef6\u5939\u7684\u6700\u9876\u5c42\uff08\u76ee\u5f55\u5305\u542b\u672c README \u6587\u4ef6\uff09\uff0c\u8fd0\u884c\u4e0b\u5217\u547d\u4ee4\uff1a\n\n```jupyter notebook customer_segments.ipynb```\n\n\u200b\u8fd9\u4f1a\u542f\u52a8 Jupyter Notebook \u5e76\u628a\u9879\u76ee\u6587\u4ef6\u6253\u5f00\u5728\u4f60\u7684\u6d4f\u89c8\u5668\u4e2d\u3002\n\n## \u6570\u636e\n\n\u200b\u8fd9\u4e2a\u9879\u76ee\u7684\u6570\u636e\u5305\u542b\u5728 `customers.csv` \u6587\u4ef6\u4e2d\u3002\u4f60\u80fd\u5728[UCI \u673a\u5668\u5b66\u4e60\u4fe1\u606f\u5e93](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers)\u9875\u9762\u4e2d\u627e\u5230\u66f4\u591a\u4fe1\u606f\u3002\n"}, {"repo": "jbossdemocentral/bpms-customer-evaluation-demo", "language": "Java", "readme_contents": "JBoss BPM Suite Customer Evaluation Demo\n========================================\nThe customer evaluation project demonstrates the BPM Suite process integration with rules.\nIt is a straight through process (STP).\n\nThere are three options available to you for using this demo; local, Openshift and containerized.\n\n\nOption 1 - Install on your machine\n----------------------------------\n1. [Download and unzip.](https://github.com/jbossdemocentral/bpms-customer-evaluation-demo/archive/master.zip)\n\n2. Add products to installs directory.\n\n3. Run 'init.sh' or 'init.bat' file. 'init.bat' must be run with Administrative privileges. \n\n4. Start JBoss BPMS Server by running 'standalone.sh' or 'standalone.bat' in the <path-to-project>/target/jboss-eap-6.4/bin directory.\n\n5. Login to http://localhost:8080/business-central  (u:erics / p:bpmsuite1!).\n\n6. Customer Evaluation demo pre-installed as project.\n\n7. Process and Task dashboard pre-filled with mock data optional now. For Windows installer, to add just uncomment install scripts\n\t (see inline script comments).\n\n8. Read the documentation found in the docs directory & enjoy JBoss BPM Suite!\n\n\nOption 2 - Install with one click in xPaaS (bpmPaaS)\n----------------------------------------------------\nAfter clicking button, ensure `Gear` size is set to `medium`:\n\n[![Click to install OpenShift](http://launch-shifter.rhcloud.com/launch/light/Install bpmPaaS.svg)](https://openshift.redhat.com/app/console/application_type/custom?&cartridges[]=https://raw.githubusercontent.com/jbossdemocentral/cartridge-bpmPaaS-customer-evaluation-demo/master/metadata/manifest.yml&name=customerevaluation&gear_profile=medium&initial_git_url=)\n\nOnce installed you can use the JBoss BPM Suite login: \n\n   * u:erics   p: bpmsuite  (admin)\n\n   * u: alan   p: bpmsuite  (analyst)\n\n   * u: daniel p: bpmsuite (developer)\n\n   * u: ursla  p: bpmsuite (user)\n\n   * u: mary   p: bpmsuite (manager)\n\nCurrent hosting of bpmPaaS is on JBoss BPM Suite 6.0.2 in OpenShift Online.\n\n\nOption 3 - Generate containerized installation\n----------------------------------------------\nThe following steps can be used to configure and run the demo in a container\n\n1. [Download and unzip.](https://github.com/jbossdemocentral/bpms-customer-evaluation-demo/archive/master.zip)\n\n2. Add product installer to installs directory.\n\n3. Copy contents of support/docker directory to the project root.\n\n4. Build demo image.\n\n\t```\n\tdocker build -t jbossdemocentral/bpms-customer-evaluation-demo .\n\t```\n5. Start demo container\n\n\t```\n\tdocker run -it -p 8080:8080 -p 9990:9990 jbossdemocentral/bpms-customer-evaluation-demo\n\t```\n6. Login to http://&lt;DOCKER_HOST&gt;:8080/business-central (u:erics / p:bpmsuite1!)\n\n7. Customer Evaluation demo pre-installed as project.\n\n8. Read the documentation found in the docs directory & enjoy JBoss BPM Suite!\n\nAdditional information can be found in the jbossdemocentral docker [developer repository](https://github.com/jbossdemocentral/docker-developer)\n\n\nNotes\n-----\nThis project is pre-loaded into the JBoss BPM Suite, after starting it you can login,\nexamine the rule, process, and data model from within the various product components.\nYou can then build and deploy the project, thereby generating the kjar maven artifact \nthat the developer team needs to begin working on any application using this projects\nknowledge artifacts.\n\nOnce you setup the project in JBoss Developer Studio (see the docs), you can use maven \nto pull in the kjar dependency, then examine the unit tests to discover how an application\ncan interact with a knowledge project (rules, processes, and model).\n\n\nSupporting Articles\n-------------------\n- [7 Steps to Your First Process with JBoss BPM Suite Starter\tKit](http://www.schabell.org/2015/08/7-steps-first-process-jboss-bpmsuite-starter-kit.html)\n\n- [3 shockingly easy ways into JBoss rules, events, planning & BPM](http://www.schabell.org/2015/01/3-shockingly-easy-ways-into-jboss-brms-bpmsuite.html)\n\n- [Jump Start Your Rules, Events, Planning and BPM Today](http://www.schabell.org/2014/12/jump-start-rules-events-planning-bpm-today.html)\n\n- [5 Handy Tips From JBoss BPM Suite For Release 6.0.3](http://www.schabell.org/2014/10/5-handy-tips-from-jboss-bpmsuite-release-603.html)\n\n- [Launching Into the Clouds With 2 New OpenShift Primer bpmPaaS Quickstarts](http://www.schabell.org/2014/10/launching-into-clouds-with-2-new-openshift-primer-bpmpaas-quickstarts.html)\n\n- [Red Hat JBoss BPM Suite - all product demos updated for version 6.0.2.GA release](http://www.schabell.org/2014/07/redhat-jboss-bpmsuite-product-demos-6.0.2-updated.html)\n\n\nReleased versions\n-----------------\nSee the tagged releases for the following versions of the product:\n\n- v2.0 - JBoss BPM Suite 6.2.0-BZ-1299002 on JBoss EAP 6.4.4 and customer evaluation demo installed.\n\n- v1.9 - JBoss BPM Suite 6.2.0, JBoss EAP 6.4.4 and OSE aligned containerization.\n\n- v1.8 - JBoss BPM Suite 6.2.0, JBoss EAP 6.4.4 with customer evaluation demo installed.\n\n- v1.7 - JBoss BPM Suite 6.1 with customer evaluation demo installed.\n\n- v1.6 - JBoss BPM Suite 6.0.3 installer with optional containerized installation.\n\n- v1.5 - moved to JBoss Demo Central, updated windows init.bat support and one click install button.\n\n- v1.4 - JBoss BPM Suite 6.0.3 installer with cutomer evalutation demo installed.\n\n- v1.3 - JBoss BPM Suite 6.0.2 installer used, with cutomer evalutation demo installed.\n\n- v1.2 - JBoss BPM Suite 6.0.2, JBoss EAP 6.1.1, and migrated JBDS project from BRMS 5.3.\n\n- v1.1 - JBoss BPM Suite 6.0.1, JBoss EAP 6.1.1, and migrated JBDS project from BRMS 5.3.\n\n- v1.0 - JBoss BPM Suite 6.0.0, JBoss EAP 6.1.1, and migrated JBDS project from BRMS 5.3.\n\n- v0.7 - JBoss BPM Suite 6.0.0.CR2, JBoss EAP 6.1.1, and migrated JBDS project from BRMS 5.3.\n\n- v0.6 - JBoss BPM Suite 6.0.0.CR1, JBoss EAP 6.1.1, and migrated JBDS project from BRMS 5.3.\n\n- v0.5 - JBoss BPM Suite 6.0.0.Beta, JBoss EAP 6.1.1, mock data populated in Process and Task dashboard, and migrated JBDS project from BRMS 5.3.\n\n- v0.4 - JBoss BPM Suite 6.0.0.Beta, JBoss EAP 6.1.1, migrated JBDS project from BRMS 5.3.\n\n- v0.3 - JBoss BPM Suite 6.0.0.ER5, JBoss EAP 6.1, migrated JBDS project from BRMS 5.3, and full documentation.\n\n- v0.2 - JBoss BPM Suite 6.0.0.ER5, JBoss EAP 6.1, and migrated JBDS project from BRMS 5.3.\n\n- v0.1 - JBoss BPM Suite 6.0.0.Beta1, JBoss EAP 6.1, and migrated JBDS project from BRMS 5.3.\n\n\n![Process](https://github.com/jbossdemocentral/bpms-customer-evaluation-demo/blob/master/docs/demo-images/process.png?raw=true)\n\n![Process & Task Dashboard](https://github.com/jbossdemocentral/bpms-customer-evaluation-demo/blob/master/docs/demo-images/mock-bpm-data.png?raw=true)\n\n"}, {"repo": "alfredsasko/Customer-Journey-Analytics", "language": "Jupyter Notebook", "readme_contents": "# Customer Journey Analytics\n\nDo you want to increase revenue and marketing ROI from your e-commerce platform? If yes, this is the project for you, so read further.\n\n### Table of Contents\n1. [Project Motivation](#motivation)\n2. [Results](#results)\n4. [Installation](#installation)\n3. [File Descriptions](#files)\n5. [Licensing, Authors, and Acknowledgements](#licensing)\n\n## Project Motivation<a name=\"motivation\"></a>\n\nCustomer journey mapping has become very popular in recent years. Making it right can upgrade marketing strategy, boost personalized branding and offerings and result in increased revenue and marketing ROI. To bring value to the business, it requires a healthy balance of qualitative knowledge of customer-facing functions about market and customers and quantitative insights, which can be gained from an e-commerce platform, CRM and other market related external sources. \n\nThe purpose of this project is to share with fellow sales, marketing professionals and data scientists how to approach quantitative part of customer journey mapping, namely answer questions:\n 1. How many buyer personas do we have?\n 2. What are their unique characteristics?\n 3. How accurately can we predict buyer persona from the first customer purchase transaction?\n 4. How can we adapt the marketing strategy concerning buyer personas to increase ROI?\n \n From a data science perspective it means:\n 1. How to use hierarchical and non-hierarchical clustering to identify buyer personas?\n 2. How to use ensemble and linear-based models to profile buyer personas characteristics?\n \n## Results<a name=\"results\"></a>\n\nThe main findings of the code can be found at the post available [here](https://medium.com/@alfred.sasko/customer-journey-analytics-will-make-you-more-money-ba7a11cda063).\n\n## Installation <a name=\"installation\"></a>\n\nThere are several necessary 3rd party libraries beyond the Anaconda distribution of Python which needs to be installed and imported to run code. These are:\n - [scikit_posthocs](https://scikit-posthocs.readthedocs.io/en/latest/) providing posthoc tests for multiple comparison\n - [google cloud SDK](https://anaconda.org/conda-forge/google-cloud-sdk) providing access to BigQuerry and [Google Analytics Sample Dataset](https://support.google.com/analytics/answer/7586738?hl=en)\n \n## File Descriptions <a name=\"files\"></a>\n\nThere is 1 notebook available here to showcase work related to the above questions.  Markdown cells were used to assist in walking through the thought process for individual steps.  \n\nThere are additional files:\n - `bigquery_.py` provides custom classes of BigqueryTable and BiqqueryDataset to query data to [Google Merchandise Store](https://www.googlemerchandisestore.com/) sample dataset.\n - `helper_py` provides custom functions for various analyses, to keep notebook manageable to read. \n - `custom_pca.py` holds adaptation of [scikit-learn PCA class](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) including _Varimax Rotation_ and _Latent Root Criterion_\n - `google_analytics_schema.xlsx` contains an analysis of variables in [Big Query Export Schema](https://support.google.com/analytics/answer/3437719?hl=en) used as a schema for Google Analytics Sample.\n - `product_categories.xlsx` ensures encoding of broken product category variables in the dataset\n - `temp.data.h5` stores codes/levels of each variable in the dataset\n \n## Licensing, Authors, Acknowledgements<a name=\"licensing\"></a>\n\nMust give credit to Google for the data.  @alexisbcook for a nice introduction to [Nested and Repeated Data](https://www.kaggle.com/alexisbcook/nested-and-repeated-data). Daqing Chen, Sai Laing Sain & Kun Guo for their technical article [Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining](https://link.springer.com/article/10.1057/dbm.2012.17).\n"}, {"repo": "naseemqota/CustomerOrderSearch", "language": "PHP", "readme_contents": "# CustomerOrderSearch\n\nMagento 2 - Customer can Search orders from his order history.\n\n\n## installation <br/>\ncomposer require qota/customer-order-search<br/>\nphp bin/magento setup:upgrade --keep-generated <br/>\nphp bin/magento setup:di:compile <br/>\nphp bin/magento cache:flush <br/>\n\nafter using above commands it will be automatically activated and will show to your customer-> My Order <br/>\nyou will be able to filter order no's,Product Sku's and created date as Date Range.\n\nlogin as customer \nGoto My orders.\n## Compatible\nMagento 2.x\n\n## Screenshot\n\n![image](https://user-images.githubusercontent.com/9654790/54287231-197de400-45c7-11e9-8eff-274fce33efb5.png)\n\n"}, {"repo": "FaisalAl-Tameemi/ml-customer-segments", "language": "HTML", "readme_contents": "\n# Machine Learning Engineer Nanodegree\n\n## Unsupervised Learning\n\n## Project 3: Creating Customer Segments\n\n## Getting Started\n\nIn this project, you will analyze a dataset containing data on various customers' annual spending amounts (reported in *monetary units*) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.\n\nThe dataset for this project can be found on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers). For the purposes of this project, the features `'Channel'` and `'Region'` will be excluded in the analysis \u2014 with focus instead on the six product categories recorded for customers.\n\nRun the code block below to load the wholesale customers dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported.\n\n\n```python\n# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nimport renders as rs\nimport seaborn as sns\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\n# Show matplotlib plots inline (nicely formatted in the notebook)\n%matplotlib inline\n\n# Load the wholesale customers dataset\ntry:\n    data = pd.read_csv(\"customers.csv\")\n    data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n    print \"Wholesale customers dataset has {} samples with {} features each.\".format(*data.shape)\nexcept:\n    print \"Dataset could not be loaded. Is the dataset missing?\"\n```\n\n    Wholesale customers dataset has 440 samples with 6 features each.\n\n\n## Data Exploration\nIn this section, you will begin exploring the data through visualizations and code to understand how each feature is related to the others. You will observe a statistical description of the dataset, consider the relevance of each feature, and select a few sample data points from the dataset which you will track through the course of this project.\n\nRun the code block below to observe a statistical description of the dataset. Note that the dataset is composed of six important product categories: **'Fresh'**, **'Milk'**, **'Grocery'**, **'Frozen'**, **'Detergents_Paper'**, and **'Delicatessen'**. Consider what each category represents in terms of products you could purchase.\n\n\n```python\n# Display a description of the dataset\ndisplay(data.describe())\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>440.000000</td>\n      <td>440.000000</td>\n      <td>440.000000</td>\n      <td>440.000000</td>\n      <td>440.000000</td>\n      <td>440.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>12000.297727</td>\n      <td>5796.265909</td>\n      <td>7951.277273</td>\n      <td>3071.931818</td>\n      <td>2881.493182</td>\n      <td>1524.870455</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>12647.328865</td>\n      <td>7380.377175</td>\n      <td>9503.162829</td>\n      <td>4854.673333</td>\n      <td>4767.854448</td>\n      <td>2820.105937</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.000000</td>\n      <td>55.000000</td>\n      <td>3.000000</td>\n      <td>25.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3127.750000</td>\n      <td>1533.000000</td>\n      <td>2153.000000</td>\n      <td>742.250000</td>\n      <td>256.750000</td>\n      <td>408.250000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>8504.000000</td>\n      <td>3627.000000</td>\n      <td>4755.500000</td>\n      <td>1526.000000</td>\n      <td>816.500000</td>\n      <td>965.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>16933.750000</td>\n      <td>7190.250000</td>\n      <td>10655.750000</td>\n      <td>3554.250000</td>\n      <td>3922.000000</td>\n      <td>1820.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>112151.000000</td>\n      <td>73498.000000</td>\n      <td>92780.000000</td>\n      <td>60869.000000</td>\n      <td>40827.000000</td>\n      <td>47943.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Implementation: Selecting Samples\nTo get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, add **three** indices of your choice to the `indices` list which will represent the customers to track. It is suggested to try different sets of samples until you obtain customers that vary significantly from one another.\n\n\n```python\n# Select three indices of your choice you wish to sample from the dataset\nindices = [71,221,323]\n\n# Create a DataFrame of the chosen samples\nsamples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)\nprint \"Chosen samples of wholesale customers dataset:\"\ndisplay(samples)\n\n# Generate a heatmap of the percentiles\npcts = rs.percentile_heatmap(indices, data)\n# Display the percentiles data\nprint \"Percentiles Data:\"\ndisplay(pcts)\n```\n\n    Chosen samples of wholesale customers dataset:\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18291</td>\n      <td>1266</td>\n      <td>21042</td>\n      <td>5373</td>\n      <td>4173</td>\n      <td>14472</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5396</td>\n      <td>7503</td>\n      <td>10646</td>\n      <td>91</td>\n      <td>4167</td>\n      <td>239</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13360</td>\n      <td>944</td>\n      <td>11593</td>\n      <td>915</td>\n      <td>1679</td>\n      <td>573</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Percentiles Data:\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>71</th>\n      <td>78.9</td>\n      <td>20.0</td>\n      <td>92.5</td>\n      <td>83.9</td>\n      <td>77.0</td>\n      <td>99.5</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>35.9</td>\n      <td>76.8</td>\n      <td>75.0</td>\n      <td>2.7</td>\n      <td>76.8</td>\n      <td>13.9</td>\n    </tr>\n    <tr>\n      <th>323</th>\n      <td>68.0</td>\n      <td>12.6</td>\n      <td>78.6</td>\n      <td>33.6</td>\n      <td>61.8</td>\n      <td>33.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n![png](static/output_7_4.png)\n\n\n### Question 1\nConsider the total purchase cost of each product category and the statistical description of the dataset above for your sample customers.  \n*What kind of establishment (customer) could each of the three samples you've chosen represent?*  \n**Hint:** Examples of establishments include places like markets, cafes, and retailers, among many others. Avoid using names for establishments, such as saying *\"McDonalds\"* when describing a sample customer as a restaurant.\n\n**Answer:**\n\n- Choice 1 (index = 71):  __Retailer or Market__\n    - Fresh: > mean, > 75%\n    - Milk: < mean, ~= 25%\n    - Grocery: > mean, > 75%\n    - Frozen: > mean, > 75%\n    - Detergents Paper: > mean, ~= 75%\n    - Delicatessen: > mean, ~= 75% (x3 std deviations)\n\n\n- Choice 2 (index = 221): __Cafe__\n    - _Fresh_: < mean, between 25% and 50%\n    - _Milk_: > mean, ~= 75%\n    - _Grocery_: > mean, ~= 75%\n    - _Frozen_: < mean, < 75%\n    - _Detergents Paper_: > mean, ~= 75%\n    - _Delicatessen_: < mean, ~= 25%\n\n\n- Choice 3 (index = 323): __Market__\n    - Fresh: > mean, ~= 65%\n    - Milk: < mean, ~= 15%\n    - Grocery: > mean, > 75%\n    - Frozen: < mean, ~= 25%\n    - Detergents Paper: < mean, < 75%, > 50%\n    - _Delicatessen_: < mean, > 25%, < 50%\n\n### Implementation: Feature Relevance\nOne interesting thought to consider is if one (or more) of the six product categories is actually relevant for understanding customer purchasing. That is to say, is it possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.\n\nIn the code block below, you will need to implement the following:\n - Assign `new_data` a copy of the data by removing a feature of your choice using the `DataFrame.drop` function.\n - Use `sklearn.cross_validation.train_test_split` to split the dataset into training and testing sets.\n   - Use the removed feature as your target label. Set a `test_size` of `0.25` and set a `random_state`.\n - Import a decision tree regressor, set a `random_state`, and fit the learner to the training data.\n - Report the prediction score of the testing set using the regressor's `score` function.\n\n\n```python\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\ncolumn_to_drop = 'Grocery'\n\n# Make a copy of the DataFrame, using the 'drop' function to drop the given feature\nnew_data = data.drop([column_to_drop], axis = 1)\n\n# Split the data into training and testing sets using the given feature as the target\nX_train, X_test, y_train, y_test = train_test_split(new_data, data[column_to_drop],\n                                                    test_size=0.25, random_state=42)\n\n# Create a decision tree regressor and fit it to the training set\nregressor = DecisionTreeRegressor(random_state=0)\nfit = regressor.fit(X_train, y_train)\n\n# Report the score of the prediction using the testing set\nscore = fit.score(X_test, y_test)\n\n\n# Check the feature importances & score\nprint \"The R^2 score when dropping the '{}' column is {}\".format(column_to_drop, score)\n\n# Visually inspect the correlations in the data\nrs.correlations_plot(data)\n```\n\n    The R^2 score when dropping the 'Grocery' column is 0.699248196675\n\n\n\n![png](static/output_11_1.png)\n\n\n### Question 2\n*Which feature did you attempt to predict? What was the reported prediction score? Is this feature is necessary for identifying customers' spending habits?*  \n**Hint:** The coefficient of determination, `R^2`, is scored between 0 and 1, with 1 being a perfect fit. A negative `R^2` implies the model fails to fit the data.\n\n**Answer:**\n\nThe selected feature is __Grocery__. This feature was selected because it has an __R^2 score of 0.67__ which indicates that we are closer to a perfect fit when predicting / trying to identify customers spending habits.\n\nA higher R^2 score means we can drop that feature and still be able to predict it from the other features.\n\nOther features, _such as 'Fresh' and 'Milk'_, were tested against and either had a low or negative score. Another feature that had a high fit score is 'Detergents_Paper'.\n\n### Visualize Feature Distributions\nTo get a better understanding of the dataset, we can construct a scatter matrix of each of the six product features present in the data. If you found that the feature you attempted to predict above is relevant for identifying a specific customer, then the scatter matrix below may not show any correlation between that feature and the others. Conversely, if you believe that feature is not relevant for identifying a specific customer, the scatter matrix might show a correlation between that feature and another feature in the data. Run the code block below to produce a scatter matrix.\n\n\n```python\n# Produce a scatter matrix for each pair of features in the data\n# (changed original codebase to use seaborn)\nmx_plot = sns.pairplot(data, diag_kind=\"kde\", size=1.6)\nmx_plot.set(xticklabels=[])\n```\n\n\n\n\n    <seaborn.axisgrid.PairGrid at 0x11bab9dd0>\n\n\n\n\n![png](static/output_15_1.png)\n\n\n### Question 3\n*Are there any pairs of features which exhibit some degree of correlation? Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict? How is the data for those features distributed?*  \n**Hint:** Is the data normally distributed? Where do most of the data points lie?\n\n**Answer:**\n\n__Detergents Paper and Grocery__ exhibit a higher degree of correlation that other figures shown in the scatter matrix above.\n\nThis confirms the previous answer about the relevance of 'Grocery' and 'Detergents_Paper' features. The data however doesn't seem to be normally distributed.\n\nOther interesting figures include __'Milk' and 'Detergents_Paper'__ or __'Grocery' and 'Milk'__ but the correlation degrees are not as strong.\n\n## Data Preprocessing\nIn this section, you will preprocess the data to create a better representation of customers by performing a scaling on the data and detecting (and optionally removing) outliers. Preprocessing data is often times a critical step in assuring that results you obtain from your analysis are significant and meaningful.\n\n### Implementation: Feature Scaling\nIf data is not normally distributed, especially if the mean and median vary significantly (indicating a large skew), it is most [often appropriate](http://econbrowser.com/archives/2014/02/use-of-logarithms-in-economics) to apply a non-linear scaling \u2014 particularly for financial data. One way to achieve this scaling is by using a [Box-Cox test](http://scipy.github.io/devdocs/generated/scipy.stats.boxcox.html), which calculates the best power transformation of the data that reduces skewness. A simpler approach which can work in most cases would be applying the natural logarithm.\n\nIn the code block below, you will need to implement the following:\n - Assign a copy of the data to `log_data` after applying a logarithm scaling. Use the `np.log` function for this.\n - Assign a copy of the sample data to `log_samples` after applying a logrithm scaling. Again, use `np.log`.\n\n\n```python\nfrom scipy import stats\n\n# Scale the data using the natural logarithm\nlog_data = np.log(data)\n\n# Scale the sample data using the natural logarithm\nlog_samples = np.log(samples)\n\n# Produce a scatter matrix for each pair of newly-transformed features\n# (changed original codebase to use seaborn)\nsns.pairplot(log_data, diag_kind=\"kde\", size=1.6)\n```\n\n\n\n\n    <seaborn.axisgrid.PairGrid at 0x123736e50>\n\n\n\n\n![png](static/output_20_1.png)\n\n\n### Observation\nAfter applying a natural logarithm scaling to the data, the distribution of each feature should appear much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before).\n\nRun the code below to see how the sample data has changed after having the natural logarithm applied to it.\n\n\n```python\n# Display the log-transformed sample data\ndisplay(log_samples)\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9.814164</td>\n      <td>7.143618</td>\n      <td>9.954276</td>\n      <td>8.589142</td>\n      <td>8.336390</td>\n      <td>9.579971</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.593413</td>\n      <td>8.923058</td>\n      <td>9.272940</td>\n      <td>4.510860</td>\n      <td>8.334952</td>\n      <td>5.476464</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9.500020</td>\n      <td>6.850126</td>\n      <td>9.358157</td>\n      <td>6.818924</td>\n      <td>7.425954</td>\n      <td>6.350886</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Implementation: Outlier Detection\nDetecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use [Tukey's Method for identfying outliers](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/): An *outlier step* is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal.\n\nIn the code block below, you will need to implement the following:\n - Assign the value of the 25th percentile for the given feature to `Q1`. Use `np.percentile` for this.\n - Assign the value of the 75th percentile for the given feature to `Q3`. Again, use `np.percentile`.\n - Assign the calculation of an outlier step for the given feature to `step`.\n - Optionally remove data points from the dataset by adding indices to the `outliers` list.\n\n**NOTE:** If you choose to remove any outliers, ensure that the sample data does not contain any of these points!  \nOnce you have performed this implementation, the dataset will be stored in the variable `good_data`.\n\n\n```python\nfrom collections import Counter\n\n# OPTIONAL: Select the indices for data points you wish to remove\noutliers  = []\n\n# For each feature find the data points with extreme high or low values\nfor feature in log_data.keys():\n\n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data[feature], 25)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(log_data[feature], 75)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = 1.5 * (Q3 - Q1)\n\n    # Display the outliers\n    print \"Data points considered outliers for the feature '{}':\".format(feature)\n    feature_outliers = log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))]\n    display(feature_outliers)\n\n    # Add the outliers indices to the list of outliers for removal later on\n    outliers.extend(list(feature_outliers.index.values))\n\n# Find the data points that where considered outliers for more than one feature\nmulti_feature_outliers = (Counter(outliers) - Counter(set(outliers))).keys()\n\n# Remove the outliers, if any were specified\ngood_data = log_data.drop(log_data.index[multi_feature_outliers]).reset_index(drop = True)\n```\n\n    Data points considered outliers for the feature 'Fresh':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>65</th>\n      <td>4.442651</td>\n      <td>9.950323</td>\n      <td>10.732651</td>\n      <td>3.583519</td>\n      <td>10.095388</td>\n      <td>7.260523</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>2.197225</td>\n      <td>7.335634</td>\n      <td>8.911530</td>\n      <td>5.164786</td>\n      <td>8.151333</td>\n      <td>3.295837</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>5.389072</td>\n      <td>9.163249</td>\n      <td>9.575192</td>\n      <td>5.645447</td>\n      <td>8.964184</td>\n      <td>5.049856</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>1.098612</td>\n      <td>7.979339</td>\n      <td>8.740657</td>\n      <td>6.086775</td>\n      <td>5.407172</td>\n      <td>6.563856</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>3.135494</td>\n      <td>7.869402</td>\n      <td>9.001839</td>\n      <td>4.976734</td>\n      <td>8.262043</td>\n      <td>5.379897</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>4.941642</td>\n      <td>9.087834</td>\n      <td>8.248791</td>\n      <td>4.955827</td>\n      <td>6.967909</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>5.298317</td>\n      <td>10.160530</td>\n      <td>9.894245</td>\n      <td>6.478510</td>\n      <td>9.079434</td>\n      <td>8.740337</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>5.192957</td>\n      <td>8.156223</td>\n      <td>9.917982</td>\n      <td>6.865891</td>\n      <td>8.633731</td>\n      <td>6.501290</td>\n    </tr>\n    <tr>\n      <th>218</th>\n      <td>2.890372</td>\n      <td>8.923191</td>\n      <td>9.629380</td>\n      <td>7.158514</td>\n      <td>8.475746</td>\n      <td>8.759669</td>\n    </tr>\n    <tr>\n      <th>304</th>\n      <td>5.081404</td>\n      <td>8.917311</td>\n      <td>10.117510</td>\n      <td>6.424869</td>\n      <td>9.374413</td>\n      <td>7.787382</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>5.493061</td>\n      <td>9.468001</td>\n      <td>9.088399</td>\n      <td>6.683361</td>\n      <td>8.271037</td>\n      <td>5.351858</td>\n    </tr>\n    <tr>\n      <th>338</th>\n      <td>1.098612</td>\n      <td>5.808142</td>\n      <td>8.856661</td>\n      <td>9.655090</td>\n      <td>2.708050</td>\n      <td>6.309918</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>4.762174</td>\n      <td>8.742574</td>\n      <td>9.961898</td>\n      <td>5.429346</td>\n      <td>9.069007</td>\n      <td>7.013016</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>5.247024</td>\n      <td>6.588926</td>\n      <td>7.606885</td>\n      <td>5.501258</td>\n      <td>5.214936</td>\n      <td>4.844187</td>\n    </tr>\n    <tr>\n      <th>357</th>\n      <td>3.610918</td>\n      <td>7.150701</td>\n      <td>10.011086</td>\n      <td>4.919981</td>\n      <td>8.816853</td>\n      <td>4.700480</td>\n    </tr>\n    <tr>\n      <th>412</th>\n      <td>4.574711</td>\n      <td>8.190077</td>\n      <td>9.425452</td>\n      <td>4.584967</td>\n      <td>7.996317</td>\n      <td>4.127134</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Milk':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>86</th>\n      <td>10.039983</td>\n      <td>11.205013</td>\n      <td>10.377047</td>\n      <td>6.894670</td>\n      <td>9.906981</td>\n      <td>6.805723</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>6.220590</td>\n      <td>4.718499</td>\n      <td>6.656727</td>\n      <td>6.796824</td>\n      <td>4.025352</td>\n      <td>4.882802</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>6.432940</td>\n      <td>4.007333</td>\n      <td>4.919981</td>\n      <td>4.317488</td>\n      <td>1.945910</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>356</th>\n      <td>10.029503</td>\n      <td>4.897840</td>\n      <td>5.384495</td>\n      <td>8.057377</td>\n      <td>2.197225</td>\n      <td>6.306275</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Grocery':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>75</th>\n      <td>9.923192</td>\n      <td>7.036148</td>\n      <td>1.098612</td>\n      <td>8.390949</td>\n      <td>1.098612</td>\n      <td>6.882437</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>6.432940</td>\n      <td>4.007333</td>\n      <td>4.919981</td>\n      <td>4.317488</td>\n      <td>1.945910</td>\n      <td>2.079442</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Frozen':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>38</th>\n      <td>8.431853</td>\n      <td>9.663261</td>\n      <td>9.723703</td>\n      <td>3.496508</td>\n      <td>8.847360</td>\n      <td>6.070738</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>8.597297</td>\n      <td>9.203618</td>\n      <td>9.257892</td>\n      <td>3.637586</td>\n      <td>8.932213</td>\n      <td>7.156177</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>4.442651</td>\n      <td>9.950323</td>\n      <td>10.732651</td>\n      <td>3.583519</td>\n      <td>10.095388</td>\n      <td>7.260523</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>10.000569</td>\n      <td>9.034080</td>\n      <td>10.457143</td>\n      <td>3.737670</td>\n      <td>9.440738</td>\n      <td>8.396155</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>7.759187</td>\n      <td>8.967632</td>\n      <td>9.382106</td>\n      <td>3.951244</td>\n      <td>8.341887</td>\n      <td>7.436617</td>\n    </tr>\n    <tr>\n      <th>264</th>\n      <td>6.978214</td>\n      <td>9.177714</td>\n      <td>9.645041</td>\n      <td>4.110874</td>\n      <td>8.696176</td>\n      <td>7.142827</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>10.395650</td>\n      <td>9.728181</td>\n      <td>9.519735</td>\n      <td>11.016479</td>\n      <td>7.148346</td>\n      <td>8.632128</td>\n    </tr>\n    <tr>\n      <th>420</th>\n      <td>8.402007</td>\n      <td>8.569026</td>\n      <td>9.490015</td>\n      <td>3.218876</td>\n      <td>8.827321</td>\n      <td>7.239215</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>9.060331</td>\n      <td>7.467371</td>\n      <td>8.183118</td>\n      <td>3.850148</td>\n      <td>4.430817</td>\n      <td>7.824446</td>\n    </tr>\n    <tr>\n      <th>439</th>\n      <td>7.932721</td>\n      <td>7.437206</td>\n      <td>7.828038</td>\n      <td>4.174387</td>\n      <td>6.167516</td>\n      <td>3.951244</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Detergents_Paper':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>75</th>\n      <td>9.923192</td>\n      <td>7.036148</td>\n      <td>1.098612</td>\n      <td>8.390949</td>\n      <td>1.098612</td>\n      <td>6.882437</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>9.428190</td>\n      <td>6.291569</td>\n      <td>5.645447</td>\n      <td>6.995766</td>\n      <td>1.098612</td>\n      <td>7.711101</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Data points considered outliers for the feature 'Delicatessen':\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>66</th>\n      <td>2.197225</td>\n      <td>7.335634</td>\n      <td>8.911530</td>\n      <td>5.164786</td>\n      <td>8.151333</td>\n      <td>3.295837</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>7.248504</td>\n      <td>9.724899</td>\n      <td>10.274568</td>\n      <td>6.511745</td>\n      <td>6.728629</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>4.941642</td>\n      <td>9.087834</td>\n      <td>8.248791</td>\n      <td>4.955827</td>\n      <td>6.967909</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>8.034955</td>\n      <td>8.997147</td>\n      <td>9.021840</td>\n      <td>6.493754</td>\n      <td>6.580639</td>\n      <td>3.583519</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>10.519646</td>\n      <td>8.875147</td>\n      <td>9.018332</td>\n      <td>8.004700</td>\n      <td>2.995732</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>6.432940</td>\n      <td>4.007333</td>\n      <td>4.919981</td>\n      <td>4.317488</td>\n      <td>1.945910</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>10.514529</td>\n      <td>10.690808</td>\n      <td>9.911952</td>\n      <td>10.505999</td>\n      <td>5.476464</td>\n      <td>10.777768</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>5.789960</td>\n      <td>6.822197</td>\n      <td>8.457443</td>\n      <td>4.304065</td>\n      <td>5.811141</td>\n      <td>2.397895</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>7.798933</td>\n      <td>8.987447</td>\n      <td>9.192075</td>\n      <td>8.743372</td>\n      <td>8.148735</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>6.368187</td>\n      <td>6.529419</td>\n      <td>7.703459</td>\n      <td>6.150603</td>\n      <td>6.860664</td>\n      <td>2.890372</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>6.871091</td>\n      <td>8.513988</td>\n      <td>8.106515</td>\n      <td>6.842683</td>\n      <td>6.013715</td>\n      <td>1.945910</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>10.602965</td>\n      <td>6.461468</td>\n      <td>8.188689</td>\n      <td>6.948897</td>\n      <td>6.077642</td>\n      <td>2.890372</td>\n    </tr>\n    <tr>\n      <th>289</th>\n      <td>10.663966</td>\n      <td>5.655992</td>\n      <td>6.154858</td>\n      <td>7.235619</td>\n      <td>3.465736</td>\n      <td>3.091042</td>\n    </tr>\n    <tr>\n      <th>343</th>\n      <td>7.431892</td>\n      <td>8.848509</td>\n      <td>10.177932</td>\n      <td>7.283448</td>\n      <td>9.646593</td>\n      <td>3.610918</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Question 4\n*Are there any data points considered outliers for more than one feature? Should these data points be removed from the dataset? If any data points were added to the `outliers` list to be removed, explain why.*\n\n**Answer:**\n\nYes, there are data points that were considered outliers for more than one feature. These points have the following indices: `[128, 65, 154, 75, 66]`.\n\nWe can remove data points based on being outliers for more than a single feature. Although, we could also argue that outliers for certain features such as __'Grocery' or 'Detergents_Paper'__ are the only ones we need to remove since these columns biggest correlation score when fitting.\n\nIn this submission, we chosen to remove the data points that are considered outliers for multiple features as implemented in the code above.\n\n## Feature Transformation\nIn this section you will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.\n\n### Implementation: PCA\n\nNow that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the `good_data` to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the *explained variance ratio* of each dimension \u2014 how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.\n\nIn the code block below, you will need to implement the following:\n - Import `sklearn.decomposition.PCA` and assign the results of fitting PCA in six dimensions with `good_data` to `pca`.\n - Apply a PCA transformation of the sample log-data `log_samples` using `pca.transform`, and assign the results to `pca_samples`.\n\n\n```python\nfrom sklearn.decomposition import PCA\n\n# Apply PCA by fitting the good data with the same number of dimensions as features\npca = PCA(n_components=len(good_data.keys())).fit(good_data)\n\n# Transform the sample log-data using the PCA fit above\npca_samples = pca.transform(log_samples)\n\n# Generate PCA results plot\npca_results = rs.pca_results(good_data, pca)\n\n# print \"The cumulative variance: \"\n# display(pca_results['Explained Variance'].cumsum())\n```\n\n\n![png](static/output_29_0.png)\n\n\n### Question 5\n*How much variance in the data is explained* ***in total*** *by the first and second principal component? What about the first four principal components? Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending.*  \n**Hint:** A positive increase in a specific dimension corresponds with an *increase* of the *positive-weighted* features and a *decrease* of the *negative-weighted* features. The rate of increase or decrease is based on the indivdual feature weights.\n\n**Answer:**\n\n\nExplained Variances of first and second principal components:\n\n\n| Principal Component | Explained Variance |\n|:--------------------|:------------------:|\n| First PC            | 0.4430             |\n| Second PC           | 0.2638             |\n\n\nThe cummulative variances are as follows:\n\n|    Dimension        | Cumulative Variance|\n|:--------------------|:------------------:|\n|    Dimension 1      |  0.4430            |\n|    Dimension 2      |  **0.7068**        |\n|    Dimension 3      |  0.8299            |\n|    Dimension 4      |  **0.9311**        |\n|    Dimension 5      |  0.9796            |\n|    Dimension 6      |  1.0000            |\n\n\n\n##### Discussion:\n\n- __First Principal Component__:\n    - This component has highly positive weights for `Detergents_Paper`, `Milk` and `Grocery`. It also shows negative weights for `Fresh` and `Frozen`.\n    - This patterns might represent spending on household products that are purchsed together.\n\n\n- __Second Principal Component__:\n    - This component shows a lot of variances in customers who purchase `Fresh`, `Frozen` and `Delicatessen`, which is the opposite of the first component.\n    - This component might represent spending habits of customers who aren't purchasing household items.\n\n\n- __Third Principal Component__:\n    - This component shows a lot of variances in customers who purchase `Frozen` and `Delicatessen` but __not__ `Fresh`.\n    - This component represents spending habits similar to those of component 2 (which is the opposite of the first component) however without a high variance in `Fresh` purchases.\n\n\n- __Fourth Principal Component__:\n    - This component has highly positive weights for `Frozen` and `Detergents_Paper` and negative weights for `Fresh` and `Delicatessen`.\n    - This component represents spending habits similar to those of component 2 and 3 but opposite to component 2 when it comes to purchasing `Fresh` and opposite to 2 and 3 for `Delicatessen`.\n\n### Observation\nRun the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions. Observe the numerical value for the first four dimensions of the sample points. Consider if this is consistent with your initial interpretation of the sample points.\n\n\n```python\n# Display sample log-data after having a PCA transformation applied\ndisplay(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension 1</th>\n      <th>Dimension 2</th>\n      <th>Dimension 3</th>\n      <th>Dimension 4</th>\n      <th>Dimension 5</th>\n      <th>Dimension 6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.4462</td>\n      <td>2.7836</td>\n      <td>1.1083</td>\n      <td>-0.3929</td>\n      <td>1.6836</td>\n      <td>-1.5910</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.1771</td>\n      <td>-1.8735</td>\n      <td>-1.8437</td>\n      <td>-1.1368</td>\n      <td>-0.3514</td>\n      <td>0.1295</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.2672</td>\n      <td>-0.0434</td>\n      <td>-1.0736</td>\n      <td>-0.1957</td>\n      <td>0.8635</td>\n      <td>-1.3171</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Implementation: Dimensionality Reduction\nWhen using principal component analysis, one of the main goals is to reduce the dimensionality of the data \u2014 in effect, reducing the complexity of the problem. Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the *cumulative explained variance ratio* is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards.\n\nIn the code block below, you will need to implement the following:\n - Assign the results of fitting PCA in two dimensions with `good_data` to `pca`.\n - Apply a PCA transformation of `good_data` using `pca.transform`, and assign the reuslts to `reduced_data`.\n - Apply a PCA transformation of the sample log-data `log_samples` using `pca.transform`, and assign the results to `pca_samples`.\n\n\n```python\n# Apply PCA by fitting the good data with only two dimensions\npca = PCA(n_components=2).fit(good_data)\n\n# Transform the good data using the PCA fit above\nreduced_data = pca.transform(good_data)\n\n# Transform the sample log-data using the PCA fit above\npca_samples = pca.transform(log_samples)\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n```\n\n### Observation\nRun the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions. Observe how the values for the first two dimensions remains unchanged when compared to a PCA transformation in six dimensions.\n\n\n```python\n# Display sample log-data after applying PCA transformation in two dimensions\ndisplay(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))\n```\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension 1</th>\n      <th>Dimension 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.4462</td>\n      <td>2.7836</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.1771</td>\n      <td>-1.8735</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.2672</td>\n      <td>-0.0434</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n## Clustering\n\nIn this section, you will choose to use either a K-Means clustering algorithm or a Gaussian Mixture Model clustering algorithm to identify the various customer segments hidden in the data. You will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale.\n\n### Question 6\n*What are the advantages to using a K-Means clustering algorithm? What are the advantages to using a Gaussian Mixture Model clustering algorithm? Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why?*\n\n**Answer:**\n\n\n__K-Means Clustering:__\n\n- Advantages\n    - With a large number of features, K-means can be computationally faster if K is small.\n    - K-Means could result in tighter clusters than hierarchical clustering.\n\n\n- Disadvantages\n    - Difficulty in comparing quality of the clusters produced.\n    - Since preset `K` value is required, it could be difficult to predict which `K` is best.\n    - Assumes data is globular with doesn't always occur in real-life data.\n\n\n__Gaussian Mixture:__\n\n- Advantages\n    - Capable of \"soft\" classification\n    - Works with different distributions of the data\n\n- Disadvantages\n    - Could fail if the dimensionality of the problem is too high\n\n\n----\n\n__Chosen Algorithim:__\n\n\nIn this submission, the chosen algorithim is __Gaussian Mixture Model__ because of its ability to apply \"soft\" classification and since we've reduced the dimensionality of the problem with PCA, GMM should be able to do the job.\n\nIf our dataset was considerably larger, we could reconsider and use K-means.\n\n----\n\n### Implementation: Creating Clusters\nDepending on the problem, the number of clusters that you expect to be in the data may already be known. When the number of clusters is not known *a priori*, there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data \u2014 if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's *silhouette coefficient*. The [silhouette coefficient](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the *mean* silhouette coefficient provides for a simple scoring method of a given clustering.\n\nIn the code block below, you will need to implement the following:\n - Fit a clustering algorithm to the `reduced_data` and assign it to `clusterer`.\n - Predict the cluster for each data point in `reduced_data` using `clusterer.predict` and assign them to `preds`.\n - Find the cluster centers using the algorithm's respective attribute and assign them to `centers`.\n - Predict the cluster for each sample data point in `pca_samples` and assign them `sample_preds`.\n - Import sklearn.metrics.silhouette_score and calculate the silhouette score of `reduced_data` against `preds`.\n   - Assign the silhouette score to `score` and print the result.\n\n\n```python\nfrom sklearn import mixture\nfrom sklearn.metrics import silhouette_score\n\n# Apply your clustering algorithm of choice to the reduced data\nclusterer = mixture.GMM(n_components=2)\ngmm = clusterer.fit(reduced_data)\n\n# Predict the cluster for each data point\npreds = gmm.predict(reduced_data)\n\n# Find the cluster centers\ncenters = gmm.means_\n\n# Predict the cluster for each transformed sample data point\nsample_preds = gmm.predict(pca_samples)\n\n# Calculate the mean silhouette coefficient for the number of clusters chosen\nscore = silhouette_score(reduced_data, preds, metric='euclidean')\n\n# Print the score result\nprint \"The silhouette_score is: {}\".format(score)\n```\n\n    The silhouette_score is: 0.411818864386\n\n\n### Question 7\n*Report the silhouette score for several cluster numbers you tried. Of these, which number of clusters has the best silhouette score?*\n\n**Answer:**\n\n\n| Number of Clusters | Silhouette Scoree |\n|:------------------:|:-----------------:|\n|      2             |      0.4118       |\n|      3             |      0.3735       |\n|      4             |      0.3333       |\n|      5             |      0.2870       |\n\n\nAs we can see in the table above, the __best number of clusters is 2__.\n\nAs we increase the number of clusters, the Silhouette score gets closer to 0 which indicate overlapping clusters. Where as when using 2 clusters, we get score values closer to 1 indicating dense and well seperated clusters.\n\n### Cluster Visualization\nOnce you've chosen the optimal number of clusters for your clustering algorithm using the scoring metric above, you can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters.\n\n\n```python\n# Display the results of the clustering from implementation\nrs.cluster_results(reduced_data, preds, centers, pca_samples)\n```\n\n\n![png](static/output_46_0.png)\n\n\n### Implementation: Data Recovery\nEach cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the *averages* of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to *the average customer of that segment*. Since the data is currently reduced in dimension and scaled by a logarithm, we can recover the representative customer spending from these data points by applying the inverse transformations.\n\nIn the code block below, you will need to implement the following:\n - Apply the inverse transform to `centers` using `pca.inverse_transform` and assign the new centers to `log_centers`.\n - Apply the inverse function of `np.log` to `log_centers` using `np.exp` and assign the true centers to `true_centers`.\n\n\n\n```python\n# Inverse transform the centers\nlog_centers = pca.inverse_transform(centers)\n\n# Exponentiate the centers\ntrue_centers = np.exp(log_centers)\n\n# Display the true centers\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\nprint \"True centers: \"\ndisplay(true_centers)\n\n# Compare the true centers to the mean and median of the data\nprint \"Compared to the mean: \"\ndisplay(true_centers - data.mean().round())\nprint \"Compared to the median: \"\ndisplay(true_centers - data.median().round())\n```\n\n    True centers:\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Segment 0</th>\n      <td>4316.0</td>\n      <td>6347.0</td>\n      <td>9555.0</td>\n      <td>1036.0</td>\n      <td>3046.0</td>\n      <td>945.0</td>\n    </tr>\n    <tr>\n      <th>Segment 1</th>\n      <td>8812.0</td>\n      <td>2052.0</td>\n      <td>2689.0</td>\n      <td>2058.0</td>\n      <td>337.0</td>\n      <td>712.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Compared to the mean:\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Segment 0</th>\n      <td>-7684.0</td>\n      <td>551.0</td>\n      <td>1604.0</td>\n      <td>-2036.0</td>\n      <td>165.0</td>\n      <td>-580.0</td>\n    </tr>\n    <tr>\n      <th>Segment 1</th>\n      <td>-3188.0</td>\n      <td>-3744.0</td>\n      <td>-5262.0</td>\n      <td>-1014.0</td>\n      <td>-2544.0</td>\n      <td>-813.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Compared to the median:\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fresh</th>\n      <th>Milk</th>\n      <th>Grocery</th>\n      <th>Frozen</th>\n      <th>Detergents_Paper</th>\n      <th>Delicatessen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Segment 0</th>\n      <td>-4188.0</td>\n      <td>2720.0</td>\n      <td>4799.0</td>\n      <td>-490.0</td>\n      <td>2230.0</td>\n      <td>-21.0</td>\n    </tr>\n    <tr>\n      <th>Segment 1</th>\n      <td>308.0</td>\n      <td>-1575.0</td>\n      <td>-2067.0</td>\n      <td>532.0</td>\n      <td>-479.0</td>\n      <td>-254.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n### Question 8\nConsider the total purchase cost of each product category for the representative data points above, and reference the statistical description of the dataset at the beginning of this project. *What set of establishments could each of the customer segments represent?*  \n**Hint:** A customer who is assigned to `'Cluster X'` should best identify with the establishments represented by the feature set of `'Segment X'`.\n\n**Answer:**\n\nThe average customer in __Segment 0__ has an above average spending on `Milk`, `Grocery` and `Detergents_Paper`. This could represent customers who purchase a lot of household items which possibly includes __Cafes or Grocery Stores__.\n\n__Segment 1__ shows the opposite spending habits by being far below the average in those categories mentioned above. This cluster could represent __Retails__.\n\n### Question 9\n*For each sample point, which customer segment from* ***Question 8*** *best represents it? Are the predictions for each sample point consistent with this?*\n\nRun the code block below to find which cluster each sample point is predicted to be.\n\n\n```python\n# Display the predictions\nfor i, (pred, sample) in enumerate(zip(sample_preds, data_samples.values)):\n    print \"Sample point\", i, \"predicted to be in Cluster\", pred\n    print \"Data for this sample: \", sample\n    print \"-----\"\n```\n\n    Sample point 0 predicted to be in Cluster 0\n    Data for this sample:  [ 5909 23527 13699 10155   830  3636]\n    -----\n    Sample point 1 predicted to be in Cluster 0\n    Data for this sample:  [10766  1175  2067  2096   301   167]\n    -----\n    Sample point 2 predicted to be in Cluster 1\n    Data for this sample:  [27380  7184 12311  2809  4621  1022]\n    -----\n\n\n**Answer:**\n\nFor __sample point 0__, the values for 'Grocery', 'Milk' are above average and mimic the __Segment 0 center__ in those categories \u2014 so, the predicted cluster seems to be consistent with the sample.\n\nThe same behaviour is seem when inspecting `Fresh` and `Milk` purchases in __sample point 2__ (3rd point from above) which is also consistent with the initial findings.\n\n## Conclusion\n\n### Question 10\n*Companies often run [A/B tests](https://en.wikipedia.org/wiki/A/B_testing) when making small changes to their products or services. If the wholesale distributor wanted to change its delivery service from 5 days a week to 3 days a week, how would you use the structure of the data to help them decide on a group of customers to test?*  \n**Hint:** Would such a change in the delivery service affect all customers equally? How could the distributor identify who it affects the most?\n\n**Answer:**\n\nThe assumption is that Cafes and Diners which means they are likely to be effected the delivery service change.\n\nSince retails make larger order and possibly have more space to stock items, they are likely to make their orders once or twice a week.\n\nThe distributor can identify which customer segment would be most affected by the delivery service change can be found by running an A/B test on the patterns of the orders for each segment.\n\nBy inspecting the number of times each of those customer segments makes an order (per week); we can assume that customers who need more than 3 deliveries per week to be affected by the service change. We can then count the number of customers affected from each segment and make a decision on whether or not to go ahead with the change.\n\n### Question 11\n*Assume the wholesale distributor wanted to predict a new feature for each customer based on the purchasing information available. How could the wholesale distributor use the structure of the clustering data you've found to assist a supervised learning analysis?*  \n**Hint:** What other input feature could the supervised learner use besides the six product features to help make a prediction?\n\n**Answer:**\n\nThe segment classification (or cluster) itself could be another feature used to help with making predictions in a supervised learning model.\n\nFor example, let's consider than we fed back the cluster of each customer into the original dataset. We can then run a supervised learning model to predict if a certain customer would be interested in opting in for a new service the distributor is thinking about offering.\n\n### Visualizing Underlying Distributions\n\nAt the beginning of this project, it was discussed that the `'Channel'` and `'Region'` features would be excluded from the dataset so that the customer product categories were emphasized in the analysis. By reintroducing the `'Channel'` feature to the dataset, an interesting structure emerges when considering the same PCA dimensionality reduction applied earlier on to the original dataset.\n\nRun the code block below to see how each data point is labeled either `'HoReCa'` (Hotel/Restaurant/Cafe) or `'Retail'` the reduced space. In addition, you will find the sample points are circled in the plot, which will identify their labeling.\n\n\n```python\n# Display the clustering results based on 'Channel' data\nrs.channel_results(reduced_data, multi_feature_outliers, pca_samples[:5])\n\n# Display the clustering results WITHOUT removing the outliers\nrs.channel_results(reduced_data, [], pca_samples[:5])\n```\n\n\n![png](static/output_60_0.png)\n\n\n\n![png](static/output_60_1.png)\n\n\n### Question 12\n*How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers? Are there customer segments that would be classified as purely 'Retailers' or 'Hotels/Restaurants/Cafes' by this distribution? Would you consider these classifications as consistent with your previous definition of the customer segments?*\n\n**Answer:**\n\nThe clustering algorithim chosen has correctly identified 4 out of the 5 first samples. Also the initial intuition of these 2 customer segments has turned out to the close to accurate in terms of the kinds of customers they might be.\n\nSince we have used the GMM learning model which uses probability instead of \"hard\" classification. This means that there are technically no points which are entirely segment 0 or segment 1, rather a probability of being each of those clusters.\n\nI would consider the majority of these classifications to be inline with the real results.\n"}, {"repo": "archd3sai/Customer-Survival-Analysis-and-Churn-Prediction", "language": "Jupyter Notebook", "readme_contents": "# Customer Survival Analysis and Churn Prediction\n\nApp: https://churn-prediction-app.herokuapp.com/\n\nCustomer attrition, also known as customer churn, customer turnover, or customer defection, is the loss of clients or customers.\n\nTelephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer attrition analysis and customer attrition rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.\n\nPredictive analytics use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn.\n\nIn this project I aim to perform customer survival analysis and build a model which can predict customer churn. I also aim to build an app which can be used to understand why a specific customer would stop the service and to know his/her expected lifetime value.  \n\n## Final Customer Churn Prediction App\n<img src=https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/app-pic.png>\n\n## Project Organization\n```\n.\n\u251c\u2500\u2500 Images/                             : contains images\n\u251c\u2500\u2500 static/                             : plots to show gauge chart, hazard and survival curve, shap values in Flask App \n\u2502   \u2514\u2500\u2500 images/\n\u2502       \u251c\u2500\u2500 hazard.png\n\u2502       \u251c\u2500\u2500 surv.png\n\u2502       \u251c\u2500\u2500 shap.png\n\u2502       \u2514\u2500\u2500 new_plot.png\n\u251c\u2500\u2500 templates/                          : contains html template for flask app\n\u2502   \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 Customer Survival Analysis.ipynb    : Survival Analysis kaplan-Meier curve, log-rank test and Cox-proportional Hazard model\n\u251c\u2500\u2500 Exploratory Data Analysis.ipynb     : Data Analysis to understand customer data\n\u251c\u2500\u2500 Churn Prediction Model.ipynb        : Random Forest model to predict customer churn\n\u251c\u2500\u2500 app.py                              : Flask App\n\u251c\u2500\u2500 app-pic.png                         : Final App image  \n\u251c\u2500\u2500 explainer.bz2                       : Shap Explainer\n\u251c\u2500\u2500 model.pkl                           : Random Forest model\n\u251c\u2500\u2500 survivemodel.pkl                    : Cox-proportional Hazard model\n\u251c\u2500\u2500 requirements.txt                    : requirements to run this model\n\u251c\u2500\u2500 Procfile                            : procfile for app deployment\n\u251c\u2500\u2500 LICENSE.md                          : MIT License\n\u2514\u2500\u2500 README.md                           : Report\n```\n\n## Customer Survival Analysis\n\n**Survival Analysis:** \nSurvival analysis is generally defined as a set of methods for analyzing data where the outcome variable is the time until the occurrence of an event of interest. The event can be death, occurrence of a disease, marriage, divorce, etc. The time to event or survival time can be measured in days, weeks, years, etc.\n\nFor example, if the event of interest is heart attack, then the survival time can be the time in years until a person develops a heart attack.\n\n**Objective:**\nThe objective of this analysis is to utilize non-parametric and semi-parametric methods of survival analysis to answer the following questions.\n- How the likelihood of the customer churn changes over time?\n- How we can model the relationship between customer churn, time, and other customer characteristics?\n- What are the significant factors that drive customer churn?\n- What is the survival and Hazard curve of a specific customer?\n- What is the expected lifetime value of a customer?\n\n**Kaplan-Meier Survival Curve:**\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/SurvivalCurve.png\" width=\"400\" height=\"300\">\n</p>\n\nFrom above graph, we can say that\n- AS expected, for telcom, churn is relatively low. The company was able to retain more than 60% of its customers even after 72 months.\n- There is a constant decrease in survival probability probability between 3-60 months.\n- After 60 months or 5 years, survival probability decreases with a higher rate. \n\n**Log-Rank Test:** \n\nLog-rank test is carried out to analyze churning probabilities group wise and to find if there is statistical significance between groups. The plots show survival curve group wise.\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/gender.png\" width=\"250\" height=\"200\"/> \n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/Senior%20Citizen.png\" width=\"250\" height=\"200\"/>\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/partner_1.png\" width=\"250\" height=\"200\"/> \n</p>\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/dependents.png\" width=\"250\" height=\"200\"/> \n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/phoneservice.png\" width=\"250\" height=\"200\"/>\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/MultipleLines.png\" width=\"250\" height=\"200\"/> \n</p>\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/InternetService.png\" width=\"250\" height=\"200\"/> \n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/OnlineSecurity.png\" width=\"250\" height=\"200\"/> \n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/OnlineBackup.png\" width=\"250\" height=\"200\"/> \n</p>\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/DeviceProtection.png\" width=\"250\" height=\"200\"/> \n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/TechSupport.png\" width=\"250\" height=\"200\"/>\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/Contract.png\" width=\"250\" height=\"200\"/> \n</p>\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/StreamingMovies.png\" width=\"250\" height=\"200\"/>\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/paymentmethod.png\" width=\"250\" height=\"200\"/> \n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/PaperlessBilling.png\" width=\"250\" height=\"200\"/>\n</p>\n\nFrom above graphs we can conclude following:\n- Customer's Gender and the phone service type are not indictive features and their p value of log rank test is above threshold value 0.05.\n- If customer is young and has a family, he or she is less likely to churn. The reason might be the busy life, more money or another factors.\n- If customer is not enrolled in services like online backup, online security, device protection, tech support, streaming Tv and streaming movies even though having active internet service, the survival probability is less.\n- The company should traget customers who opt for internet service as their survival probability constantly descreases. Also, Fiber Optilc type of Internet Service is costly and fast compared to DSL and this might be the reason of higher customer churning. \n- More offers should be given to customers who opt for month-to-month contract and company should target customers to subscribe for long-term service. \n- If customer's paying method is automatic, he or she is less likely to churn. The reason is in the case of electronic check and mailed check, a customer has to make an effort to pay and it takes time.\n\n**Survival Regression:**\nI use cox-proportional hazard model to perform survival regression analysis on customer data. This model is used to relate several risk factors or exposures simultaneously to survival time. In a Cox proportional hazards regression model, the measure of effect is the hazard rate, which is the risk or probability of suffering the event of interest given that the participant has survived up to a specific time. The model fits the data well and the coefficients are shown below.\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/Survival-analysis.png\" width=\"750\" height=\"500\"/>\n</p>\n\nUsing this model we can calculate the survival curve and hazard curve of any customer as shown below. These plots are useful to know the remaining life of a customer. \n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/survival.png\" width=\"400\" height=\"300\"/>\n<img src=\"https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/hazard.png\" width=\"400\" height=\"300\"/>\n</p>\n\n**Customer Lifetime Value:**\n\nTo calculate customer lifetime value, I would multiply the Monthly charges the customer is paying to Telcom and the expected life time of the customer.\n\nI utilize the survival function of a customer to calculate its expected life time. I would like to be little bit conservative and consider the customer is churned when the survival probability of him is 10%.\n\n## Customer Churn Prediction\nI aim to implement a machine learning model to accurately predict if the customer will churn or not.\n\n### Analysis\n\n**Churn and Tenure Relationship:**\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/tenure-churn.png\" width=\"600\" height=\"300\"/>\n</p>\n\n- As we can see the higher the tenure, the lesser the churn rate. This tells us that the customer becomes loyal with the tenure.\n\n<br />\n\n**Tenure Distrbution by Various Services:**\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/tenure-dist.png\" width=\"340\" height=\"250\"/>\n</p>\n\n- When the customers are new they do not opt for various services and their churning rate is very high. This can be seen in above plot for Streaming Movies and this holds true for all various services.\n\n<br />\n\n**Internet Service By Contract Type:**\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/internetservice-contract.png\" width=\"360\" height=\"250\"/>\n</p>\n\n- Many of the people of who opt for month-to-month Contract choose Fiber optic as Internet service and this is the reason for higher churn rate for fiber optic Internet service type.\n\n<br />\n\n**Payment method By Contract Type:**\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/payment-contract.png\" width=\"500\" height=\"250\"/>\n</p>\n\n- People having month-to-month contract prefer paying by Electronic Check mostly or mailed check. The reason might be short subscription cancellation process compared to automatic payment.\n\n<br />\n\n**Monthly Charges:**\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/monthlycharges.png\" width=\"300\" height=\"220\"/>\n</p>\n\n- As we can see the customers paying high monthly fees churn more.\n\n<br />\n\n### Modelling\n\nFor the modelling, I will use tress based Ensemble method as we do not have linearity in this classification problem. Also, we have a class imbalance of 1:3 and to combat it I will assign class weightage of 1:3 which means false negatives are 3 times costlier than false positives. I built a model on 80% of data and validated model on remaining 20% of data keeping in mind that I do not have data leakage. The random forest model has many hyperparameters and I tuned them using Grid Search Cross Validation while making sure that I do not overfit.\n\nThe final model resulted in 0.62 F1 score and 0.85 ROC-AUC. The resulting plots can be seen below.\n\n<p align=\"center\">\n<img src=\"https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/model_1.png\" width=\"600\" height=\"300\"/>\n<img src=\"https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/model_feat_imp.png\" width=\"600\" height=\"400\"/>\n\n</p>\n\nFrom the feature importance plot, we can see which features govern the customer churn.\n\n### Explainability\n\nWe can explain and understand the Random forest model using explainable AI modules such as Permutation Importance, Partial Dependence plots and Shap values.\n\n1. Permutation Importance shows feature importance by randomly shuffling feature values and measuring how much it degrades our performance.\n\n<p align=\"center\">\n<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/eli51.png height=250 width=200>\n<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/eli52.png height=130 width=200> \n</p>\n\n2. Partial dependence plot is used to see how churning probability changes across the range of particular feature. For example, in below graph of tenure group, the churn probability decreases at a higher rate if a person is in tenure group 2 compared to 1.\n\n<p align=\"center\">\n<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/pdp_tenure.png height=250 width=400>\n<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/pdp_contract.png height=250 width=400> \n</p>\n\n<p align=\"center\">\n<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/pdp_monthly_charges.png height=250 width=400>\n<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/pdp_total_charges.png height=250 width=400> \n</p>\n\n3. Shap values (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. In below plot we can see that why a particual customer's churning probability is less than baseline value and which features are causing them.\n\n![](https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/shap.png)\n\n## Flask App\n\nI saved the final tuned Random Forest model and deployed it using Flask web app. Flask is a micro web framework written in Python.  It is designed to make getting started quick and easy, with the ability to scale up to complex applications. I saved the shap value explainer tuned using random forest model to show shap plots in app. I have also utilized the cox-proportional hazard model to show survival curve and hazard curve, and to calculate expected customer lifetime value. \n\nThe final app shows churning probability, gauge chart of how severe a customer is and shap values based on customer's data. The final app layout can be seen above.  \n\n\n\n\n"}, {"repo": "Arjun-Mota/customer-churn-prediction", "language": "Jupyter Notebook", "readme_contents": "# customer-churn-prediction\n* Goal of this project is to implement machine learning model to predict customer churn of telecom company.\n\n* Out of 29 features present in dataset, after normalizing and cleaning data, I've selected 15 features using RandomForestClassifier with ensemble learning.\n\n* Used 80/20 training and testing data for model development.\n\n* Logistic regression prediction gave me around 80% accuracy for customer churn prediction and that has still scope for better prediction after optimization.\n\n* As a part of customer retention program, this model can help management to make correct decisions on their future plans.\n\n\n### Installation\n\n* Install Jupyter Notebook and Python 3\n* Install Python libraries mentioned in requirements.txt for this  project\n\n### Working with Jupyter Notebook\n\n* Open notebook (.ipynb) from this project with Jupyter Notebook"}, {"repo": "Roc-J/loss_customers", "language": "Jupyter Notebook", "readme_contents": "# Introduction\n\n\u4f7f\u7528Python\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53d1\u73b0\u6d41\u5931\u7684\u5ba2\u6237\n\n\u4f7f\u7528python3\u548ctensorflow \u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u7684\u5b66\u4e60\u5206\u7c7b"}, {"repo": "collindching/Olist-customer-churn", "language": "Jupyter Notebook", "readme_contents": "![Banner photo](https://github.com/collindching/Olist-Customer-Churn/blob/master/ecommerce%20banner.jpg)\n\n# Olist-Customer-Churn\n\nCustomer churn problems can be parsed in to contractual or non-contractual. When business is contractual, you have a supervised binary classification problem. In the non-contractual setting, you have an unobserved probabilistic problem. \n\nIn this project, I modeled non-contractual customer churn in Python, using the Lifetimes implementation of BG/NBD. \n\n## View the notebook\n\nClick [here](https://nbviewer.jupyter.org/github/collindching/Olist-Customer-Churn/blob/master/Olist%20churn%20risk%20model.ipynb) to view the notebook through nbviewer.\n\n## Follow along\n\nTo follow along, download the notebook in this repo. Then download the data from [here](https://www.kaggle.com/olistbr/brazilian-ecommerce), and move into the same folder as the notebook. Everything should run smoothly from there. \n\nFeel free to let me know if you have any issues, and thanks for checking out this repo!\n\n\n\n"}, {"repo": "SohelRaja/Customer-Churn-Analysis", "language": "Jupyter Notebook", "readme_contents": "\n# Customer-Churn-Analysis\n\nImplementation of Decision Tree Classifier, Esemble Learning, Association Rule Mining and Clustering models(Kmodes & Kprototypes) for Customer attrition analysis of telecommunication company to identify the cause and conditions of the churn.\n\n### Contributers: [Arghadip Chakraborty](https://github.com/arghac14), [Sohel Raja Molla](https://github.com/SohelRaja), [Disha Sinha](https://github.com/disha2sinha), [Shankhadeep Giri](https://github.com/shankha2018). \n\n## Project Summary:\nFinding the best features of the dataset by comparing the rules obtained from Decision Tree Classifier, Clustering Models(Kmodes and Kprototypes) and Assocication Rule Mining(Apriori algorithm) and then implementing Voting Classifiers(Ensemble Learning) with other classifier models, taking the best features with other classifier models to see if there is a boost in accuracy of prediction or not.\n\n## Workflow:\n![](https://github.com/SohelRaja/Customer-Churn-Analysis/blob/master/Snapshots/workflow.jpg)\n\n### Collecting Data:\nWe used this telecom service customer churn dataset for this particular project- [WA_Fn-UseC_-Telco-Customer-Churn.csv](https://github.com/arghac14/Customer-Churn-Analysis/blob/master/Decision%20Tree/WA_Fn-UseC_-Telco-Customer-Churn.csv)\n\n### Data preprocessing:\nWe cleaned the dataset and took dummy datas in the form of categorical datas for our classification purpose.\nHere is the new dataset- [new_telco.csv](https://github.com/SohelRaja/Customer-Churn-Analysis/blob/master/Decision%20Tree/new_telco.csv)\n\n### Initial classification using Decison Tree Classifier:\nFirst of all, we did an initial classification by Implementing Decision Tree classifier using all the features of our dataset.\nWe got an **accuracy** of **79.83%** at depth=5 for Decision Tree Entropy technique.\n[See the notebook](https://github.com/SohelRaja/Customer-Churn-Analysis/blob/master/Decision%20Tree/Decision_Tree_Customer_Churn_v3.ipynb)\n![](https://github.com/SohelRaja/Customer-Churn-Analysis/blob/master/Snapshots/ID3graph.JPG)\n\n### Implementation of Clustering models:\nWe implemented Kmodes and Kprototype clustering to get the clusters and centroids for each feature of our dataset. It will be used to find the best features of our dataset.\n[See the notebook](https://github.com/arghac14/Customer-Churn-Analysis/tree/master/Clustering)\n\n### Associtaion Rule Mining:\nWe implemented apriori algorithm(association rule miining) to get the rules of features, depending on which we will find the best features of our dataset.\n[See the notebooks](https://github.com/arghac14/Customer-Churn-Analysis/tree/master/Association%20Rule%20Mining)\n\n##### Most important frqeuent items when Churn=0:\n![](https://github.com/arghac14/Customer-Churn-Analysis/blob/master/Snapshots/apriori1.JPG)\n\n##### Most important frqeuent items when Churn=1:\n![](https://github.com/arghac14/Customer-Churn-Analysis/blob/master/Snapshots/apriori2.JPG)\n\n### Finding the best features:\nComparing the results of Decisiton Tree classifier, Clustering and Association Rule mining, we get the following best features-\n**'tenure'**,**'InternetService'**,**'PhoneService'**.\n\n### Implementing other classifiers:\nWe implemented other classifiers like K-nearest Neighbors, Logistic Regression, Support Vector Machine, Random Forest and Naive Bayes Classifiers taking all the feauters and then taking the best features of the dataset. Then we compared the accuracy of the different models.\n[See the notebooks](https://github.com/SohelRaja/Customer-Churn-Analysis/tree/master/Ensemble%20Learning/Other%20Classifiers)\n* [Decision Tree](https://github.com/SohelRaja/Customer-Churn-Analysis/tree/master/Ensemble%20Learning/Other%20Classifiers/Decision%20Tree)\n* [KNN](https://github.com/disha2sinha/Customer-Churn-Analysis/tree/master/Ensemble%20Learning/Other%20Classifiers/KNN)\n* [Logistic Regression](https://github.com/shankha2018/Customer-Churn-Analysis/tree/master/Ensemble%20Learning/Other%20Classifiers/Logistic%20Regression)\n* [Naive Bayes](https://github.com/shankha2018/Customer-Churn-Analysis/tree/master/Ensemble%20Learning/Other%20Classifiers/Naive%20Bayes)\n* [Random Forest](https://github.com/SohelRaja/Customer-Churn-Analysis/tree/master/Ensemble%20Learning/Other%20Classifiers/Random%20Forest)\n* [SVM](https://github.com/disha2sinha/Customer-Churn-Analysis/tree/master/Ensemble%20Learning/Other%20Classifiers/SVC)\n#### Accuracy report-\nWith all features:\n\n![](https://github.com/disha2sinha/Customer-Churn-Analysis/blob/master/Snapshots/otherClassifiers1.png)\n\nWith best features:\n\n![](https://github.com/SohelRaja/Customer-Churn-Analysis/blob/master/Snapshots/other%20classifiers.JPG)\n\n### Implementing Voting Classifier (Ensemble Leaning):\nWe implemeted Voting Classifier that combines several classifier models in order to produce one optimal predictive model and improves the model performance.\n[See the notebook](https://github.com/arghac14/Customer-Churn-Analysis/tree/master/Ensemble%20Learning/Voting%20Classifier)\n\n## Final Result:\n![](https://github.com/SohelRaja/Customer-Churn-Analysis/blob/master/Snapshots/voting.JPG)\n\n![](https://github.com/SohelRaja/Customer-Churn-Analysis/blob/master/Snapshots/accc.JPG)\n\n\n   \n\n\n"}, {"repo": "jalajthanaki/Customer_churn_analysis", "language": "Jupyter Notebook", "readme_contents": "work under progress"}, {"repo": "bradtraversy/aurelia_customer_manager", "language": "JavaScript", "readme_contents": "# Aurelia Customer Manager\n\nSimple customer manager app built on the Aurelia JS framework that implements the localStorage API to store customers\n\n### Version\n1.0.0\n\n## Usage\n\n### Installation\n\nDownload or clone project\n\n```sh\n$ npm install -g http-server\n```\n\n### Serve\nTo serve in the browser\n\n```sh\n$ http-server\n```\n\n## App Info\n\n### Author\n\nBrad Traversy\n[Traversy Media](http://www.traversymedia.com)\n\n### Version\n\n1.0.0\n\n### License\n\nThis project is licensed under the MIT License"}, {"repo": "rakeshmagento/magento2-create-order-for-guest-customer-rest-api", "language": null, "readme_contents": "# magento2 create order for guest customer using rest api\n\n### 1. Get the admin token :-\nEndPoint : http://magento.host/index.php/rest/V1/integration/admin/token\n\nMethod : POST\n\nHeaders:\n\nContent-Type : application/json\n \nBody json:-\n\n{\"username\":\"API_USER\",\"password\":\"API_PASSWORD\"}\n\nResponse:-\n\n16fsjy4242fssdsfsxipegpr9gpxf7p2424bx91sbbje0mg\n\nThis token will be used for the further steps while creating order through api.\n\n### 2. Create empty cart for the guest customer :-\n\nEndPoint : http://magento.host/index.php/rest/V1/guest-carts/\n\nMethod : POST\n\nHeaders:-\n\nContent-Type  : application/json\n\nAuthorization : Bearer {TOKEN}\n\nIt will return a alphanumeric string, which is the cart id, this will be used to add/update/delete items form the cart.\n\n### 3. Add/Update items in cart :-\n\nEndpoint : http://magento.host/index.php/rest/V1/guest-carts/{cartId}/items\n \ni.e. http://magento.host/index.php/rest/V1/guest-carts/4522ebc7d219rre415a8ad1eewr22f73b55/items\n \nMethod : POST\n\nHeaders:\n\nContent-Type  : application/json\n\nAuthorization : Bearer {TOKEN}\n\nBody json:\n\n{\n\t\"cartItem\":\n\t{\n\t  \"quote_id\": \"4522ebc7d219rre415a8ad1eewr22f73b55\",\n\t  \"sku\": \"Item SKU\",\n\t  \"qty\": 1\n\t}\n}\n\n#### Note :- It will return the item_id which can be used later to update/delete item from the cart.\n\nTo update shopping item use the following request in body json on the same endpoint as above :-\n\n{\n  \"cartItem\":\n  {\n    \"quote_id\": \"4522ebc7d219rre415a8ad1eewr22f73b55\",\n     \"item_id\": 1139,\n     \"sku\": \"Item SKU\",\n     \"qty\": 2\n  }\n}\n\n### 4. Remove item from cart :-\n\nEndPoint : http://magento.host/index.php/rest/V1/guest-carts/{cartId}/items/{itemId} \n\nAs seen above the cart Id and ItemId, it should be looks like the following\n \nhttp://magento.host/index.php/rest/V1/guest-carts/4522ebc7d219rre415a8ad1eewr22f73b55/items/1348\n \nMethod : DELETE\n\nHeaders:\n\nContent-Type  : application/json\n\nAuthorization : Bearer {TOKEN}\n\nResponse : true\n\n### 5. Get the cart details :-\n\nEndPoint : http://magento.host/index.php/rest/V1/guest-carts/{CartId}\n\nMethod : GET\n\nHeaders:\n\nContent-Type  : application/json\n\nAuthorization : Bearer {TOKEN}\n\n### 6. Add shipping information to the cart :-\n\nEndPoint : http://magento.host/index.php/rest/V1/guest-carts/{cartId}/shipping-information\n\nMethod : POST\n\nHeaders:\n\nContent-Type  : application/json\n\nAuthorization : Bearer {TOKEN}\n\nBody json :-\n\n{\n    \"addressInformation\": {\n        \"shippingAddress\": {\n            \"country_id\": \"US\",\n            \"street\": [\n                \"Street Address\"\n            ],\n            \"company\": \"Company\",\n            \"telephone\": \"2313131312\",\n            \"postcode\": \"\",\n            \"city\": \"California\",\n            \"firstname\": \"john\",\n            \"lastname\": \"harrison\",\n            \"email\": \"guestuser@gmail.com\",\n            \"sameAsBilling\": 1\n        },\n        \"billingAddress\": {\n            \"country_id\": \"US\",\n            \"street\": [\n                \"Street Address\"\n            ],\n            \"company\":\"Company\",\n            \"telephone\": \"2313131312\",\n            \"postcode\": \"\",\n            \"city\": \"California\",\n            \"firstname\": \"john\",\n            \"lastname\": \"harrison\",\n            \"email\": \"guestuser@gmail.com\"\n        },\n        \"shipping_method_code\": \"flatrate\",\n        \"shipping_carrier_code\": \"flatrate\"\n   }\n}\n\n#### Note :- Here flatrate is the shipping method code, you can change whatever shipping method you are using for your store.\n\nIt will return the payment methods available with entire cart information.\n\nResponse :-\n{\n    \"payment_methods\": [\n        {\n            \"code\": \"checkmo\",\n            \"title\": \"Check / Money Order\"\n        }\n    ],\n    \"totals\": {\n        \"grand_total\": 40.5,\n        \"base_grand_total\": 40.5,\n        \"subtotal\": 40,\n        \"base_subtotal\": 40,\n        \"discount_amount\": 0,\n        \"base_discount_amount\": 0,\n        \"subtotal_with_discount\": 40,\n        \"base_subtotal_with_discount\": 40,\n        \"shipping_amount\": 0.5,\n        \"base_shipping_amount\": 0.5,\n        \"shipping_discount_amount\": 0,\n        \"base_shipping_discount_amount\": 0,\n        \"tax_amount\": 0,\n        \"base_tax_amount\": 0,\n        \"weee_tax_applied_amount\": null,\n        \"shipping_tax_amount\": 0,\n        \"base_shipping_tax_amount\": 0,\n        \"subtotal_incl_tax\": 40,\n        \"shipping_incl_tax\": 0.5,\n        \"base_shipping_incl_tax\": 0.5,\n        \"base_currency_code\": \"USD\",\n        \"quote_currency_code\": \"USD\",\n        \"items_qty\": 2,\n        \"items\": [\n            {\n                \"item_id\": 14437,\n                \"price\": 10,\n                \"base_price\": 10,\n                \"qty\": 2,\n                \"row_total\": 40,\n                \"base_row_total\": 40,\n                \"row_total_with_discount\": 0,\n                \"tax_amount\": 0,\n                \"base_tax_amount\": 0,\n                \"tax_percent\": 0,\n                \"discount_amount\": 0,\n                \"base_discount_amount\": 0,\n                \"discount_percent\": 0,\n                \"price_incl_tax\": 10,\n                \"base_price_incl_tax\": 10,\n                \"row_total_incl_tax\": 40,\n                \"base_row_total_incl_tax\": 40,\n                \"options\": \"[]\",\n                \"weee_tax_applied_amount\": null,\n                \"weee_tax_applied\": null,\n                \"name\": \"Baby Bag\"\n            }\n        ],\n        \"total_segments\": [\n            {\n                \"code\": \"subtotal\",\n                \"title\": \"Subtotal\",\n                \"value\": 40\n            },\n            {\n                \"code\": \"flatrate\",\n                \"title\": \"Flat Rate\",\n                \"value\": 0.5\n            },\n            {\n                \"code\": \"tax\",\n                \"title\": \"Tax\",\n                \"value\": 0,\n                \"extension_attributes\": {\n                    \"tax_grandtotal_details\": []\n                }\n            },\n            {\n                \"code\": \"grand_total\",\n                \"title\": \"Grand Total\",\n                \"value\": 40.5,\n                \"area\": \"footer\"\n            }\n        ]\n    }\n}\n\n### 7. Add payment information and place the order :-\n\nEndPoint : http://magento.host/index.php/rest/V1/guest-carts/{cartId}/order\n\nMethod : PUT\n\nHeaders:\n\nContent-Type  : application/json\n\nAuthorization : Bearer {TOKEN}\n\nBody json:-\n\n{\n    \"paymentMethod\": {\n       \"method\": \"checkmo\"\n    }\n}\n\nIt will return the order id some thing like 227.\n\nNow you can check the order has been created in the admin.\n\n\n\n\n\n"}, {"repo": "ankitsingh1240/CUSTOMER-CHURN-PREDICTION", "language": "Jupyter Notebook", "readme_contents": "# CUSTOMER-CHURN-PREDICTION\nINSAIDINSTRUCTIONS:You are required to come up with the solution of the given business case.Business Context:This case requires trainees to develop a model for predicting customer churn at a fictitious wireless telecom company and use insights from the model to develop an incentive plan for enticing would-be churners to remain with company.Data for the case are available in csv format. The data are a scaled down version of the full database generously donated by an anonymous wireless telephone company. There are still 7043 customers in the database, and 20 potential predictors. Candidates can use whatever method they wish to develop their machine learning model. The data are available inone data file with 7043 rows that combines the calibration and validation customers. \u201ccalibration\u201d database consisting of 4000customers and a \u201cvalidation\u201d database consisting of 3043customers. Each database contained (1) a \u201cchurn\u201d variable signifying whether the customer had left the company two months after observation, and (2) a set of 20 potential predictor variables that could be used in a predictive churn model. Following usual model development procedures, the model would be estimated on the calibration data and tested on the validation data. \n"}, {"repo": "prakhardogra921/Clustering-Analysis-on-customers-of-a-wholesale-distributor", "language": "Jupyter Notebook", "readme_contents": "# Content: Unsupervised Learning\n## Project: Creating Customer Segments\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute a [Jupyter Notebook](http://ipython.org/notebook.html)\n\nIf you do not have Python installed yet, it is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. \n\n### Code\n\nTemplate code is provided in the `customer_segments.ipynb` notebook file. You will also be required to use the included `visuals.py` Python file and the `customers.csv` dataset file to complete your work. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project. Note that the code included in `visuals.py` is meant to be used out-of-the-box and not intended for students to manipulate. If you are interested in how the visualizations are created in the notebook, please feel free to explore this Python file.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `customer_segments/` (that contains this README) and run one of the following commands:\n\n```bash\nipython notebook customer_segments.ipynb\n```  \nor\n```bash\njupyter notebook customer_segments.ipynb\n```\n\nThis will open the Jupyter Notebook software and project file in your browser.\n\n## Data\n\nThe customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal. More information can be found on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers).\n\nNote (m.u.) is shorthand for *monetary units*.\n\n**Features**\n1) `Fresh`: annual spending (m.u.) on fresh products (Continuous); \n2) `Milk`: annual spending (m.u.) on milk products (Continuous); \n3) `Grocery`: annual spending (m.u.) on grocery products (Continuous); \n4) `Frozen`: annual spending (m.u.) on frozen products (Continuous);\n5) `Detergents_Paper`: annual spending (m.u.) on detergents and paper products (Continuous);\n6) `Delicatessen`: annual spending (m.u.) on and delicatessen products (Continuous); \n7) `Channel`: {Hotel/Restaurant/Cafe - 1, Retail - 2} (Nominal)\n8) `Region`: {Lisbon - 1, Oporto - 2, or Other - 3} (Nominal) "}, {"repo": "Nexmo/ruby-sms-customer-engagement", "language": "Ruby", "readme_contents": "# Two-way SMS for customer engagement\n\nA Sinatra (Ruby) app showing how to implement two-way customer engagement over SMS using the Nexmo SMS API.\n\n## Prerequisites\n\n* [A Nexmo account](https://dashboard.nexmo.com/sign-up)\n* [Ruby 2.1+](https://www.ruby-lang.org/) and [Bundler](http://bundler.io/)\n\n## Getting started\n\n```sh\n# clone this repository\ngit clone git@github.com:Nexmo/ruby-customer-engagement.git\n# change to folder\ncd ruby-sms-customer-engagement\n# install dependencies\nbundle install\n# create a .env\ncp .env.example .env\n```\n\nNext you will need to sign up for a Nexmo account, get your API credentials from the API dashboard, register a SMS number, and put all of these details in your `.env` file.\n\n\nThen you can start the server.\n\n```sh\nruby app.rb\n```\n\nFinally make sure to link the callback URL for your number to your app. For this you will need a public IP or a tool like [ngrok](https://ngrok.com/). We highly recommend using the [Nexmo CLI](https://github.com/Nexmo/nexmo-cli) to link your number up to the callback URL:\n\n```sh\n> nexmo link:sms [your_nexmo_number] http://[your.domain.com]/inbound\nNumber updated\n```\n\n## Usage\n\nOnce your server is started:\n\n* Visit [localhost:4567](http://localhost:4567/)\n* Fill in your phone number without any leading 0's or +'s (e.g. `445555666777`)\n* You will receive a message in seconds, respond to it by typing \"1\", \"2\" or \"3\"\n* Your app will receive the incoming message in and respond back to your phone with a confirmation in seconds.\n\nAs this is a very simple starter app this app does not verify or store your response.\n\n## License\n\nThis project is licensed under the [MIT license](LICENSE).\n"}, {"repo": "Rajneesh-Tiwari/Kaggle_santander-customer-transaction-prediction", "language": "Jupyter Notebook", "readme_contents": "# Kaggle_santander-customer-transaction-prediction\nCode for 118th position in Kaggle santander-customer-transaction-prediction challenge\n"}, {"repo": "Shubha23/Exploratory-Data-Analysis-Customer-Churn-Prediction", "language": "Jupyter Notebook", "readme_contents": "# Will existing customers churn?\n**NOTE - Please see the Jupyter notebook(.ipynb file) for complete explanation and in-depth Exploratory Data Analysis **\n\nProject goals -\n1. Data cleansing and preprocessing.\n2. Data visualization and Exploratory Data Analysis\n3. Statistical analysis of the data.\n4. Model generation for prediction of customer churn behavior.\n5. Application of Logistic Regression, SVM-Linear, SVM-RBF and Random Forest algorithms on data and performance comparison.\n\n******************************************\nProject Description -\n******************************************\nData source - Kaggle & IBM sample dataset community.\nDataset - Prediction of user behavior to retain customers. The dependent variable have binary value, 1 - churned and 0 - not or true/false. The data set includes information about:\n\nCustomers who left within the last month \u2013 the column is called Churn\nServices that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\nCustomer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\nDemographic info about customers \u2013 gender, age range, and if they have partners and dependents.\n\nTo execute - python churn_rate.py\n\n*Data was in a Csv file format. For other formats use other read function of pandas.\n*Update the file path to local directory before running the file.\n\n\n**************************************************** END OF FILE ******************************************************\n"}, {"repo": "devinsays/woocommerce-customer-source", "language": "PHP", "readme_contents": "=== WooCommerce Customer Source ===\n\nContributors: @downstairsdev\nTags: woocommerce, checkout\nRequires at least: 4.7.0\nTested up to: 4.7.0\nStable tag: 1.0.0\nLicense: GPLv2 or later\nLicense URI: http://www.gnu.org/licenses/gpl-2.0.html\nWC requires at least: 3.2.1\nWC tested up to: 3.2.1\n\n== Description ==\n\nWork in progress.\n\nLearn where your customers are coming from. Adds a select field to the WooCommerce checkout screen that asks how new customers found out about the store.\n\n== Additional Notes ==\n\n== Compatibility ==\n\n* Requires WooCommerce 3.2.1 or later.\n\n== Changelog ==\n\n= Development =\n\n= 1.0.0 =\n\n* Initial release.\n"}, {"repo": "iateadonut/magento_copy_customer", "language": "PHP", "readme_contents": "\nCopy Magento Customers\n==============\n\nThis is a script that copies a customer from one magento store to another.  The two stores must have the same products.\n\nContact dan at edenwired dot com with questions/requests, or contact me through github.\n\nUsage\n==============\n\nFirst, configure the source and target databases in config_db.php.\n\nThen, set the id of the customer you want to transfer in magento_alter.php.\n\nLastly, just run\n\nphp ./magento_alter.php\n\n\nCaveats\n==============\n\nThis software is distributed with ABSOLUTELY NO WARRANTY!  It may screw up your Magento store - that's your responsibility.\n\n*BACKUP!*\n\n*TEST THIS FIRST WITH A TEST INSTALLATION OF THE TARGET DATABASE*\n\nI don't know why the sales_flat_order table has two unique keys, but if you look for these lines:\n\t\t\t\tunset( $row['entity_id'] );\n\t\t\t\tunset( $row['increment_id'] );\nyou'll see what I'm talking about.\n\nFor now, I'm grabbing the max(increment_id)+1 from the target database's sales_flat_order table and using that.  I'm guessing magento has a more sophisticated way to \n"}]